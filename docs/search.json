[
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Work Experience",
    "section": "",
    "text": "Currently leading the data science team at the Research department in ECOM Trading, one of the world‚Äôs largest traders of coffee, cotton and cocoa. I started at ECOM Trading in 2019 as a Senior Data Scientist, I was promoted to the role of Lead Data Scientist in November 2022.\nMy career at ECOM has been marked by pivotal contributions to our core product offerings. These contributions span the realms of data infrastructure and data centralisation as well as the creation, deployment and maintenance of machine learning models, which I describe in further detail below.\nI‚Äôve played a significant role in the development and maintenance of advanced trading signal models that leverage Supply & Demand dynamics, weather data, and historical price trends. This unique blend of information allows us to predict market behaviour with high precision, creating a strategic advantage in our trading operations.\nRecognizing the critical importance of sustainability in today‚Äôs market, I‚Äôve been instrumental in developing a new framework for measuring deforestation on global farms. This initiative highlights our commitment to responsible trading and provides invaluable insights into environmental impact.\nTo further streamline our processes and ensure data integrity, I‚Äôve implemented an automated alert system that identifies potential anomalies in cotton, coffee and cocoa price data. This system proactively sends email alerts when irregularities are detected, allowing us to swiftly mitigate potential risks and maintain market-leading accuracy in our data-driven decisions.\n\nSenior Data Scientist | ECOM Trading | Dec 2019 - Nov 2022\n\nLed the development of a comprehensive suite of data products, encompassing dashboards, automatic reports, and web applications in the research department as a senior data scientist.\nDeveloped an array of machine learning models to tackle key industry questions, from predicting the supply and demand of commodities like coffee, cotton, and cocoa to estimating crop yields using satellite imagery.\nUtilized my predictive models to forecast a range of pivotal factors, including the productivity of West African cocoa farms, the medium-term climate change effects on global plantations, and future price dynamics based on climatic variables.\nConceived an innovative web application that fused historical deforestation data with georeferenced farm locations, forming a unique deforestation risk model for global farms.\n\nProduct Developer - Data Science | Decoded | Oct 2018 - Nov 2019\n\nPioneered and executed over 40% of the curriculum for Decoded‚Äôs flagship offering, the Data Academy, fostering data science integration among blue-chip clients, including Societe General, UBS, Unilever, Nike, and M&S.\nCrafted comprehensive course material covering fundamental coding in Python and R, advanced statistics, and a wide array of machine learning techniques.\nAuthored specialized R and Python modules for an extensive range of topics such as regression analysis, classification methods, neural networks, Big Data handling, time series analysis, and SQL.\nRegularly hosted workshops to impart these modules to client employees, both in-person and via webinars, and trained new facilitators to carry the torch forward.\n\nResearch Associate - Data Science | Institute for Risk and Disaster Reduction, University College London | Jan 2018 - Oct 2018\n\nDeveloped a machine learning model capable of real-time prediction of potential mosquito breeding points across four cities in Brazil, employing Python, TensorFlow, and Keras to create recurrent neural networks that geospatially modelled virus and mosquito occurrences.\nThe results of my work were manifested in a user-friendly bokeh web application, which I built and was actively used by health professionals in Brazil.\nComplimenting this, I also created a mobile application to streamline data collection from health professionals on the ground."
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": "Contact me",
    "section": "",
    "text": "üíª Website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr Juan D. Beltran",
    "section": "",
    "text": "Here you can find my cv and more information about my data science journey! As well as some tutorials and notes that I share regularly with my team!\nI am a lead data scientist passionate about developing useful, effective and practical data science products to solve commercial problems in different industries. I have a strong academic background and I build my data solutions following rigorous and pragmatic data science principles. I love analysing and solving complex problems that involve data analysis. I am always eager to learn new techniques and knowledge to incorporate them into my data science toolbox.\nI have been coding in R for the last 15 years and in Python for the last 9 years. I have designed, developed and maintained a wide range of data products which combine machine learning models and GIS analysis in the shape of stand alone models, dashboards, advanced visualisation, automatic reports, and web apps. I have extensive knowledge in statistical modelling, forecasting time series and geospatial analysis.\nWhen I am not coding I really enjoy going to the gym, hiking outdoors or drinking cocktails!"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications and Awards",
    "section": "",
    "text": "2020: Beltr√°n JD and Puddifoot R: ECOM research into global warming and its effect on suitability for cocoa production. ‚ÄôAGEFI Commodities, special edition, Disruption in a Disrupted World. April: 22\n2017: Yang X, Hu R, Yin H, Jenkins J, Shu S, Tang H, Liu D, Weighill DA, Ha J, Heyduk K, Goodstein DM, Guo H, Moseley RC, Fitzek E, Jawdy S, Zhang Z, Xie M, Hartwell J, Grimwood J, Abraham PE, Mewalal R, Yim WC, Beltr√°n JD, Boxall SF, Dever LV, Palla KJ, Albion R, Garcia T, Mayer J, Lim SD, Wai CM, Van Buren R, De Paoli HC, Borland AM, Guo H, Chen J, Muchero W, Yin Y, Jacobson DA, Tschaplinski TJ, Hettich RL, Ming R , Winter K , Leebens-Mack JH, Smith JAC, Cushman J, Schmutz J, Tuskan GA.: Kalancho√´ genome reveals convergent evolution of crassulacean acid metabolism. Nature Communications 8:¬†1899\n2015: Yang X, Cushman JC, Borland AM, Edwards EJ, Wullschleger SD, Tuskan GA, Owen NA, Griffiths H, Smith JAC, De Paoli HC, Weston DJ, Cottingham R, Hartwell J, Davis SC, Silvera K, Ming R, Schlauch K, Abraham P, Stewart JR, Guo H, Albion R, Ha J, Lim SD, Wone BWM, Yim WC, Garcia T, Mayer JA, Petereit J, Nair SS, Casey E, Hettich RL, Ceusters J, Ranjan P, Palla KP, Yin H, Reyes-Garc√≠a C, Andrade JL, Freschi L, Beltr√°n JD, Dever LV, Boxall SF,Waller J, Davies J, Bupphada P, Kadu N, Winter K, Sage RF, Aguilar CN, Schmutz J, Jenkins J, Holtum JAM. A roadmap for research on crassulacean acid metabolism (CAM) to enhance sustainable food and bioenergy production in a hotter, drier world. New Phytologist 207:491‚Äì504.\n2014: Poster presentation, ‚ÄúEvolutionary origins and ecophysiology of CAM photosynthesis in the montane genus Puya (Bromeliaceae)‚Äù, 34 th New Phytologist Symposium ‚ÄúPlant Systems Biology and Ecology of CAM plants‚Äù, Lake Tahoe, Tahoe City, CA, USA, 15‚Äì18 July 2014.\n2013: Beltr√°n JD., Lasso E., Madri√±√°n S., Virgo A., Winter K.: Juvenile tank-bromeliads lacking tanks: Do they engage in CAM photosynthesis?, Photosynthetica 51: 55‚Äì62.\n2012: M.Sc. Thesis: CAM or not CAM: A study on juveniles of Guzmania lingulata, Guzmania monostachia and Werauhia sanguinolenta (Bromeliaceae), Universidad de los Andes.\n2010: B.Sc. Thesis: Evolution of xerophyte habit in the tribe Tillandsieae (Bromeliaceae): a phylogenetic approach, Universidad de los Andes."
  },
  {
    "objectID": "publications.html#awards",
    "href": "publications.html#awards",
    "title": "Publications and Awards",
    "section": "Awards",
    "text": "Awards\n‚Ä¢ Weidenfeld Scholarship and Leadership Program for D.Phil. Funding, University of Oxford (2013-2016) ‚Ä¢ Scholarship from Administrative Department of Science, Technology and Innovation (Colombia) for D.Phil. Funding, University of Oxford (2013-2017) ‚Ä¢ Winner of New Phytologist Poster Prize (1st place), 34th New Phytologist Symposium ‚ÄúPlant Systems Biology and Ecology of CAM plants‚Äù, California, U.S.A. (2014)"
  },
  {
    "objectID": "juan.html",
    "href": "juan.html",
    "title": "Brief Description",
    "section": "",
    "text": "My project management skills are honed through practical experience with cross-functional teams, Agile methodologies, and continuous integration/delivery principles on the lookout for new ways to create business value.\nIn addition, my leadership qualities are widely acknowledged, demonstrated by my commitment to mentorship and fostering data-centric approaches across diverse teams. My approach to problem-solving fuses intense curiosity with unwavering rigour, enabling me to address complex issues effectively. Furthermore, I excel in distilling intricate concepts into accessible narratives, ensuring clear and empathetic communication with stakeholders. This blend of leadership, problem-solving, and communication skills has been a cornerstone of my data science journey."
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "Education",
    "section": "",
    "text": "Degree\nInstitution\nDuration\nDetails\n\n\n\n\nPhD (DPhil), Plant Sciences\nUniversity of Oxford\n2013 - 2017\nApplied machine learning, computational modelling, and advanced statistics to analyse plant behaviour under drought stress and the impact of climate change on plant species distribution. Used various techniques including Neural Networks, XGBoost, Random Forest, Clustering analysis, and A/B tests using bootstrapping and Bayesian statistics.\n\n\nMSc, Biology (1st place)\nUniversidad de los Andes, Colombia\n2012\nUtilized R to explore hypotheses about biochemical changes in juvenile plants under drought conditions.\n\n\nBSc, Biology\nUniversidad de los Andes, Colombia\n2010\nGained proficiency in R during biostatistics coursework and applied it extensively in my dissertation and subsequent work."
  },
  {
    "objectID": "Notes/DokervsKubernets.html",
    "href": "Notes/DokervsKubernets.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nDocker and Kubernetes are two prominent tools used in deploying and managing applications in a modern development pipeline. Although they are often discussed together, they serve different, albeit complementary, roles. It‚Äôs crucial to understand the unique functions of Docker and Kubernetes and how they can work together.\n\nPart I: Overview of Docker and Kubernetes\n1.1: What is Docker?\nDocker is an open-source platform that automates the deployment, scaling, and management of applications by encapsulating them into containers. A Docker container packages an application along with all of its dependencies, ensuring that it will run uniformly regardless of the host environment.\n1.2: What is Kubernetes?\nKubernetes (also known as K8s) is an open-source platform for automating the deployment, scaling, and managing of containerized applications. It groups containers into ‚Äúpods‚Äù (the smallest deployable unit in Kubernetes) and manages these pods across multiple host machines.\n\nPart II: Comparing Docker and Kubernetes\n2.1: Scope\nDocker focuses on automating application deployment within containers, ensuring consistency across environments. Kubernetes, on the other hand, manages clusters of containers across multiple machines, handling tasks like load balancing, network traffic distribution, and service discovery.\n2.2: Scalability\nWhile Docker can scale applications, its capacity is limited. Kubernetes excels at managing and scaling large clusters of containers, making it more suitable for large-scale applications.\n2.3: Networking\nIn Docker, each container has its unique IP, with linking allowing containers to communicate. Kubernetes uses a flat network model, allowing all pods to communicate with each other without needing to map ports.\n2.4: Data Storage\nDocker volumes can be used for persistent data storage. Kubernetes, while also supporting Docker volumes, provides its volume abstraction layer, making it compatible with numerous cloud storage options.\n2.5: Load Balancing\nDocker uses a simple DNS-based round-robin method for load balancing. Kubernetes, however, provides more advanced load balancing features, including the distribution of network traffic to maintain stable application performance.\n\nPart III: Docker and Kubernetes Working Together\nDocker and Kubernetes are not mutually exclusive; instead, they complement each other. Docker provides the containerization technology, and Kubernetes provides orchestration capabilities for managing large clusters of Docker containers.\n\nPart IV: When to Use Docker or Kubernetes\n4.1: Use Docker When:\n\nYou are containerizing applications and ensuring consistent behavior across environments.\nYour application is relatively simple and does not require advanced orchestration or scaling features.\nYou want a simpler learning curve. Docker is generally easier to learn and implement.\n\n4.2: Use Kubernetes When:\n\nYou need to manage and scale large clusters of containers across multiple machines.\nYour application requires advanced orchestration features, such as load balancing, service discovery, and automated rollouts and rollbacks.\nYou are deploying microservices architecture at a large scale.\n\n\nConclusion:\nUnderstanding Docker and Kubernetes and their respective strengths can help you make informed decisions when deploying and managing your applications. While Docker provides an excellent platform for containerizing applications, Kubernetes offers powerful tools for managing and scaling those containers. Choosing the right tool depends on your specific use case, the complexity and scale of your application, and your infrastructure needs."
  },
  {
    "objectID": "Notes/commenting.html",
    "href": "Notes/commenting.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nWriting clean, readable, and maintainable code is just as important as writing code that works. Good commenting and maintenance practices make your code more understandable to others (and your future self). Let‚Äôs dive into why and how we should comment our code and some best practices for code maintenance.\n\nPart I: Understanding Code Commenting\n1.1: What is Code Commenting?\nCode commenting involves adding descriptive lines to your code to explain what certain parts of the code are intended to do. These lines are ignored by the compiler or interpreter.\n1.2: Why is Commenting Important?\nComments improve the readability of your code by providing context. They can explain the purpose of a particular code block, provide additional information about a complex algorithm, or clarify a tricky section of code.\n\nPart II: Best Practices for Commenting Code\n2.1: Comment on the ‚ÄúWhy‚Äù, not the ‚ÄúWhat‚Äù\nYour comments should explain why the code does something, not what it does. If the code‚Äôs purpose is not immediately clear, consider refactoring it to make it more readable.\n2.2: Keep Comments Concise\nComments should be brief and to the point. If you need a lengthy explanation to understand a piece of code, it may be a sign that you need to simplify or refactor your code.\n2.3: Avoid Redundant Comments\nDon‚Äôt write comments for code that is self-explanatory. For example, there‚Äôs no need to comment on a line of code that just increments a counter.\n2.4: Update Comments as Code Changes\nMake sure your comments always reflect the current state of your code. Outdated comments can lead to confusion and misinformation.\n\nPart III: Importance of Code Maintenance\nMaintaining code is about ensuring your code stays clean, understandable, and easy to debug over time. This includes commenting, but also involves practices like refactoring, using version control, and regularly reviewing code.\n\nPart IV: Best Practices for Code Maintenance\n4.1: Regular Refactoring\nRefactoring involves changing the code‚Äôs structure without changing its functionality. Regular refactoring makes your code more efficient, readable, and maintainable.\n4.2: Use Version Control Systems\nVersion control systems like Git help you track changes, understand how your codebase evolves over time, and make it easier to revert changes or branch out for testing new features.\n4.3: Conduct Regular Code Reviews\nRegularly reviewing your code and your team‚Äôs code can help spot potential issues before they become problems. Code reviews also promote knowledge sharing among team members.\n4.4: Follow a Coding Standard\nAdhere to a coding standard or style guide to keep your code consistent. This will make it easier for others (and yourself) to read and understand your code.\n4.5: Write Unit Tests\nWriting unit tests can help you catch bugs early, ensure that your code works as expected, and prevent future changes from breaking functionality.\n\nConclusion:\nCommenting your code and maintaining it properly is crucial for long-term project success. Good practices help your future self and others understand your code better, make it easier to debug, and create a more collaborative environment. Remember that writing code is not just about getting a machine to perform tasks, but also about communicating your thought process effectively to other developers."
  },
  {
    "objectID": "Notes/MLOPS0.html",
    "href": "Notes/MLOPS0.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "In today‚Äôs blog, I will discuss a pivotal concept in modern machine learning architectures: MLOps, short for Machine Learning Operations.\n\n\nMLOps is a set of best practices combining Machine Learning (ML), Data Engineering, and DevOps. The goal of MLOps is to streamline and standardize the machine learning lifecycle, from development and deployment to maintenance.\nMLOps aims to address various challenges in operationalizing ML models, such as managing data dependencies, maintaining code quality, ensuring model reproducibility, and monitoring model performance over time.\n\n\n\nLet‚Äôs dive deeper into the various components that constitute MLOps:\n\n\nVersion control is essential for tracking changes to code, data, parameters, and environment configuration. Git is often used for code, but tools like DVC (Data Version Control) can help manage data and model versions.\n\n\n\nTesting ensures the stability and reliability of the code, including model code and infrastructure code. Types of tests include unit tests, integration tests, and end-to-end tests. Additionally, data validation tests are crucial to ensure the quality of the data feeding into the models.\n\n\n\nCI is a practice where developers frequently merge their code changes into a central repository. After each merge, automated builds and tests are run. In the context of MLOps, this includes integration tests of ML code, infrastructure, and data pipelines.\n\n\n\nContinuous Deployment is the practice of automatically deploying the code to production after passing the build and test stages. For ML, this also includes model training, validation, and deployment.\n\n\n\nModel performance needs to be monitored over time to ensure it doesn‚Äôt degrade. Monitoring systems can track model performance metrics and provide alerts for any significant changes. Logging systems keep detailed records of data inputs and outputs, model predictions, and any errors or exceptions.\n\n\n\nEnsuring the same results can be achieved if a past version of the model is re-run with the same data and parameters is key. This includes versioning data, code, and environment, as Ill as tracking all experiments.\n\n\n\n\nSeveral tools and platforms help facilitate MLOps, including:\n\nMLflow: An open-source platform for managing the end-to-end ML lifecycle, including experimentation, reproducibility, and deployment.\nTFX (TensorFlow Extended): A Google-production-scale ML platform that provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor ML systems.\nKubeflow: An open-source project dedicated to making deployments of machine learning workflows on Kubernetes simple, portable, and scalable.\nSeldon: An open-source platform that enables businesses to deploy, scale, and optimize machine learning models in production.\n\n\n\n\nMLOps brings several benefits, including:\n\nEfficiency: By automating many steps in the ML lifecycle.\nReproducibility: By tracking every experiment with its data, code, and parameters.\nCollaboration: By enabling data scientists, data engineers, and DevOps to work together effectively.\nMonitoring: By continuously monitoring model performance and data quality.\nGovernance and regulatory compliance: By maintaining detailed logs of data, model predictions, and model changes.\n\nThe ultimate goal of MLOps is to accelerate the ML lifecycle and make ML models more valuable to the organization. Having a firm grasp of MLOps principles has been invaluable in my professional journey."
  },
  {
    "objectID": "Notes/MLOPS1.html",
    "href": "Notes/MLOPS1.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Following our introductory session on MLOps, today we will delve deeper into each component of the MLOps lifecycle and understand their practical applications.\n\n\nThe quality and consistency of data are critical for the performance of ML models. As such, versioning data is an important practice in MLOps. Tools like DVC can be used for this purpose. They keep track of changes in datasets and models, enabling you to reproduce any version of your experiment.\nAlso, don‚Äôt forget about data validation: tools like TensorFlow Data Validation (TFDV) can help you analyze and validate the consistency of your data over time.\n\n\n\nDuring this stage, data scientists build and train various machine learning models to solve the given problem. Key aspects of this stage are tracking experiments and ensuring reproducibility. An experiment involves a specific version of the code, a set of parameters, a dataset, and produces a model and its metrics. MLflow is a popular tool used to manage these experiments.\nTo ensure reproducibility, you must have version control in place for not just your code (using Git), but also your data and model (using tools like DVC or MLflow). Containerization technologies (like Docker) can also be used to maintain the consistency of the computing environment across different stages of the ML lifecycle.\n\n\n\nIn MLOps, testing isn‚Äôt limited to just the code; it also involves data testing and model validation.\n\nData Tests: These tests ensure that the data is in the correct format, within the expected range, and not corrupted.\nModel Tests: These tests confirm that the model is performing as expected and that its predictions make sense. Model validation techniques, like cross-validation or train/validation/test split, are used to assess the performance of the model.\n\n\n\n\nCI/CD are key DevOps practices that have been adapted for MLOps.\n\nContinuous Integration (CI): This involves regularly merging code changes into a central repository, after which automated builds and tests are run. CI ensures that the code remains in a deployable state and helps to catch bugs early.\nContinuous Deployment (CD): In CD, code changes are automatically deployed to production once they pass the necessary automated tests. In the context of MLOps, CD often involves deploying ML models to a serving infrastructure, which could be a server, a serverless platform, or an edge device.\n\nTools like Jenkins, Travis CI, or GitLab CI/CD are often used for implementing CI/CD pipelines.\n\n\n\nOnce the model is deployed, it‚Äôs important to continuously monitor its performance to ensure that it‚Äôs still providing valuable predictions as new data comes in. This can be achieved using monitoring tools like Prometheus or Grafana. Logging systems like Elasticsearch or Fluentd can help store detailed records of data inputs and outputs, model predictions, and any errors or exceptions.\n\n\n\nIn many industries, there are regulations requiring that models be explainable, fair, and unbiased. Thus, MLOps also involves applying techniques for model interpretability (like SHAP or LIME), and ensuring data privacy (like differential privacy or federated learning). Furthermore, maintaining detailed logs and implementing proper access control mechanisms can help meet regulatory compliance requirements.\nWe‚Äôll continue exploring these components in our hands-on session where we‚Äôll walk through the lifecycle of an ML project using popular MLOps tools."
  },
  {
    "objectID": "Notes/MLOPS3.html",
    "href": "Notes/MLOPS3.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Sure, while a full end-to-end example with code would be quite lengthy for this format, I can provide simplified examples for each of the key steps we discussed in the previous tutorial. These examples will use Python and some popular libraries for each task."
  },
  {
    "objectID": "Notes/MLOPS3.html#setting-up-the-environment",
    "href": "Notes/MLOPS3.html#setting-up-the-environment",
    "title": "Dr Juan Beltran",
    "section": "1. Setting up the Environment",
    "text": "1. Setting up the Environment\nYou can initialize a new Git repository and DVC project using the command line:\n# Initialize a Git repository\ngit init\n\n# Initialize a DVC project\ndvc init"
  },
  {
    "objectID": "Notes/MLOPS3.html#data-preparation",
    "href": "Notes/MLOPS3.html#data-preparation",
    "title": "Dr Juan Beltran",
    "section": "2. Data Preparation",
    "text": "2. Data Preparation\nAssume you have a CSV file as your raw data. We‚Äôll use pandas to clean and preprocess the data:\nimport pandas as pd\n\n# Load raw data\ndf = pd.read_csv('raw_data.csv')\n\n# Preprocess the data\ndf_clean = df.dropna()  # Drop missing values\ndf_clean.to_csv('clean_data.csv', index=False)\n\n# Add to DVC\n!dvc add clean_data.csv"
  },
  {
    "objectID": "Notes/MLOPS3.html#experimentation-and-model-building",
    "href": "Notes/MLOPS3.html#experimentation-and-model-building",
    "title": "Dr Juan Beltran",
    "section": "3. Experimentation and Model Building",
    "text": "3. Experimentation and Model Building\nLet‚Äôs use sklearn for building a simple logistic regression model and mlflow for tracking the experiment:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport mlflow\n\n# Load clean data\ndf_clean = pd.read_csv('clean_data.csv')\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df_clean.drop('target', axis=1), df_clean['target'], test_size=0.2)\n\n# Start an MLflow experiment\nwith mlflow.start_run():\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n    \n    # Log metrics\n    mlflow.log_metric(\"accuracy\", model.score(X_test, y_test))"
  },
  {
    "objectID": "Notes/MLOPS3.html#testing",
    "href": "Notes/MLOPS3.html#testing",
    "title": "Dr Juan Beltran",
    "section": "4. Testing",
    "text": "4. Testing\nYou can use pytest to create tests for your data and model:\nimport pytest\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\ndef test_data():\n    df = pd.read_csv('clean_data.csv')\n    assert not df.isnull().any().any(), \"Data contains null values.\"\n\ndef test_model():\n    df = pd.read_csv('clean_data.csv')\n    model = LogisticRegression()\n    model.fit(df.drop('target', axis=1), df['target'])\n    assert model.score(df.drop('target', axis=1), df['target']) > 0.8, \"Model accuracy is too low.\""
  },
  {
    "objectID": "Notes/MLOPS3.html#model-deployment",
    "href": "Notes/MLOPS3.html#model-deployment",
    "title": "Dr Juan Beltran",
    "section": "5. Model Deployment",
    "text": "5. Model Deployment\nFor model deployment, you can save your model to a file and then load it in your serving code:\n# Save model to a file\nimport joblib\njoblib.dump(model, 'model.pkl')\n\n# Load model in serving code\nmodel = joblib.load('model.pkl')\nYou would then include this model file and the serving code in a Docker image."
  },
  {
    "objectID": "Notes/MLOPS3.html#monitoring",
    "href": "Notes/MLOPS3.html#monitoring",
    "title": "Dr Juan Beltran",
    "section": "6. Monitoring",
    "text": "6. Monitoring\nPrometheus and Grafana are often used for monitoring, but they are typically used outside of your Python code, so we‚Äôll skip the code example for this step."
  },
  {
    "objectID": "Notes/MLOPS3.html#maintenance-and-iteration",
    "href": "Notes/MLOPS3.html#maintenance-and-iteration",
    "title": "Dr Juan Beltran",
    "section": "7. Maintenance and Iteration",
    "text": "7. Maintenance and Iteration\nIf the model performance drops, you would go back to the experimentation stage, adjust your model or data, and then rerun your code.\nRemember, these are simplified examples and real-world scenarios would involve more complex data preprocessing, model training, testing, and deployment steps."
  },
  {
    "objectID": "Notes/MLOPS2.html",
    "href": "Notes/MLOPS2.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Following our detailed discussion on MLOps components, it‚Äôs now time to put theory into practice. Today‚Äôs session will focus on implementing an end-to-end machine learning project using the MLOps principles we‚Äôve learned.\nWe will use a mix of tools for different stages of the project, including DVC for data versioning, MLflow for experiment tracking, Jenkins for CI/CD, and Prometheus and Grafana for monitoring.\n\n\nFirstly, we‚Äôll set up a collaborative environment with Git for version control. We will also set up DVC to track changes in our data and model files. All project members will clone the Git repository and set up DVC remotes.\n\n\n\nIn this step, we will prepare and explore the dataset. We‚Äôll create scripts for data cleaning and preprocessing, ensuring we annotate them with proper comments for clarity. Once we have a processed dataset ready for ML modeling, we‚Äôll use DVC to track the data file.\n\n\n\nNext, we will start building and training our models. It‚Äôs important to track the hyperparameters, metrics, and models for each experiment. We‚Äôll use MLflow for this purpose. MLflow will allow us to compare different experiments and choose the best model for our problem.\n\n\n\nNow, we will create unit tests for our code, data validation tests for our dataset, and model validation tests for our model. For each code commit, these tests will run automatically in our Jenkins CI/CD pipeline.\n\n\n\nOnce we have a model that performs well and passes all tests, it‚Äôs time to deploy it. We‚Äôll create a Docker image that includes our model and the necessary serving code, and we‚Äôll configure Jenkins to deploy this image automatically to our serving infrastructure.\n\n\n\nAfter deployment, we need to monitor the model‚Äôs performance in real-time. We will set up Prometheus to collect metrics from our model server, and we‚Äôll use Grafana to create a dashboard for viewing these metrics.\n\n\n\nMLOps is not a one-time setup; it‚Äôs a continuous process. We‚Äôll set up alerts to inform us when our model‚Äôs performance drops below a certain threshold. If that happens, or if we receive new data, we‚Äôll iterate on our models: we‚Äôll go back to the experimentation stage, make improvements, and push the changes through our CI/CD pipeline.\nRemember, MLOps is about managing the lifecycle of machine learning projects in a way that promotes collaboration, efficiency, and reliability. The tools and techniques we‚Äôre learning in this course will serve you well in your future data science endeavors.\nIn the next session, we‚Äôll get hands-on with these tools, and you‚Äôll have the opportunity to set up your own MLOps pipelines."
  },
  {
    "objectID": "Notes/Newenvs.html",
    "href": "Notes/Newenvs.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nWhen working on multiple Python projects, it‚Äôs essential to create separate environments to keep dependencies, libraries, and even Python versions isolated from one another. This practice ensures that our projects remain clean, manageable, and free from package conflict. It‚Äôs an industry-standard best practice that all Python developers, including data scientists, should follow.\n\nPart I: Understanding Python Environments\n1.1: What is a Python Environment?\nA Python environment is a context in which you run Python code and includes everything that Python interacts with - the Python interpreter, libraries, and global settings.\n1.2: Why Do We Need Python Environments?\nCreating separate environments helps prevent conflicts between packages and Python versions when working on different projects. Each project can have its own dependencies, regardless of what dependencies other projects have.\n\nPart II: Importance of Creating Python Environments\n2.1: Isolation of Project Dependencies\nBy isolating your project environments, you avoid issues such as package version conflicts and the subsequent ‚Äúdependency hell.‚Äù It ensures that upgrading a package for one project doesn‚Äôt break another.\n2.2: Reproducibility\nEnvironments help ensure that your code runs consistently across different platforms. By specifying the versions of packages used in an environment, you can ensure that your code will run the same way on any machine.\n2.3: Ease of Sharing and Collaboration\nWhen sharing your code with others, it is easier if you also provide the environment used to run it. This way, collaborators can replicate your environment and run your code without having to resolve any dependency issues.\n\nPart III: Tools for Python Environment Management\n3.1: Virtualenv\nVirtualenv is a popular tool that creates isolated Python environments. It allows you to create an environment, install the necessary packages, and then activate and deactivate the environment as needed.\n3.2: Conda\nConda is a package manager that also manages environments. It‚Äôs particularly popular in the data science world because it makes it easy to install packages that are hard to compile from source code, like NumPy or SciPy.\n3.3: Pipenv\nPipenv combines the capabilities of pip and virtualenv into one tool, providing both package management and virtual environment support. It introduces a ‚Äúlock file‚Äù to lock the environment‚Äôs exact dependencies, improving reproducibility.\n3.4: Docker\nDocker isn‚Äôt a Python-specific tool, but it‚Äôs worth mentioning. It encapsulates your application and its environment into a container, ensuring consistency across multiple development and release cycles.\n\nPart IV: Best Practices for Creating Python Environments\n4.1: One Project, One Environment\nCreate a new environment for each project to isolate its dependencies. This practice is essential to avoid conflicts between different project dependencies.\n4.2: Document Dependencies\nAlways keep a record of your project‚Äôs dependencies. Tools like pip can generate a requirements.txt file, and conda can create an environment.yml file.\n4.3: Use Version Control\nVersion control systems like git can help keep track of changes, not only in your code but also in your environment setup. They can help you trace back what changes in your environment might have caused your code to break.\n4.4: Clean Up Regularly\nOld, unused environments can take up space and create clutter. It‚Äôs good practice to remove any environments you‚Äôre no longer using.\n\nConclusion:\nCreating and managing Python environments might seem like an extra step in the development process, but it is crucial for creating stable, reproducible, and conflict-free Python projects. By understanding the importance of Python environments and following the best practices,\nyou can improve your Python development workflow and make your life as a Python developer or data scientist significantly easier."
  },
  {
    "objectID": "Notes/PythonvsR.html",
    "href": "Notes/PythonvsR.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nOne of the most common discussions in the data science world revolves around the choice between Python and R. Both languages have unique strengths and they are powerful tools when used effectively. A proficient data scientist can and often does, use both in their toolkit. These languages are not mutually exclusive, and each language can be the perfect tool for specific tasks. It‚Äôs all about understanding their unique strengths and leveraging them accordingly.\n\nPart I: Understanding Python and R\n1.1: Overview of Python\nPython is a general-purpose language noted for its simplicity and readability. This makes it a fantastic choice for beginners in programming and data science. Python‚Äôs robustness comes from its extensive libraries and packages, which cover almost every aspect of data science.\n1.2: Overview of R\nR is a statistical programming language that was specifically designed for data analysis, making it a go-to language for statisticians and researchers. It‚Äôs well-respected for its comprehensive statistical and graphical capabilities. R also has a wealth of packages for specialized scientific computation tasks.\n\nPart II: Strengths of Python in Data Science\n2.1: Machine Learning\nPython‚Äôs major strength lies in machine learning. Libraries like scikit-learn, TensorFlow, and PyTorch offer tools for predictive modeling, neural networks, natural language processing, and more.\n2.2: General Programming & Scripting\nPython shines in general-purpose programming tasks. This makes it perfect for building data pipelines, web scraping, automation, web development, and more.\n2.3: Community & Learning Resources\nPython boasts a larger user community than R, leading to more resources for learning and troubleshooting. Websites like Stack Overflow have a massive amount of content related to Python, making it easier for new data scientists to find help.\n\nPart III: Strengths of R in Data Science\n3.1: Statistical Analysis\nR is unparalleled in its statistical analysis capabilities. It has a wide range of in-built functions for testing statistical hypotheses and conducting complex data analyses.\n3.2: Data Visualization\nAlthough Python has Matplotlib, Seaborn, and Plotly, R‚Äôs ggplot2 package is considered one of the most sophisticated data visualization tools. It has a high level of flexibility and enables detailed layering and thematic customization.\n3.3: Reporting and Reproducible Research\nWith tools like R Markdown, Shiny, and Knitr, R excels at creating reports and interactive web applications, allowing others to reproduce your analysis with the original data and code.\n\nPart IV: Python vs R: A Comparative Summary\n\nEase of Learning: Python‚Äôs syntax is straightforward, making it easier for beginners to learn. However, R has a steeper learning curve but provides more statistical power.\nData Handling Capabilities: Python is preferred for large datasets and big data analysis due to its speed and efficiency, while R is better suited for dataset manipulation and statistical modeling.\nVisualization: Python has several good visualization libraries, but many data scientists agree that R‚Äôs ggplot2 offers superior control and complexity.\nMachine Learning: Python has a better machine learning ecosystem, which includes libraries like TensorFlow and PyTorch. R also has machine learning libraries like caret and mlr, but they are less developed compared to Python‚Äôs ecosystem.\nCommunity Support: Python has a wider community, resulting in faster package development and troubleshooting assistance. However, R has strong support in academia and research-oriented industries.\nJob Market: Python is generally more in demand in the industry. However, R is favored in specific sectors like biostatistics, bioinformatics, and academic research.\n\n\nPart V: The Convergence - Python and R in Data Science\nOne shouldn‚Äôt have to choose between Python and R; instead, the focus should be on learning to use both effectively. Many professionals use both languages in their work - Python for data manipulation and machine learning, and R for data analysis and visualization.\n5.1: Tools for Interoperability\nTools like Jupyter notebooks, Rpy2, and reticulate make it possible to use both languages interchangeably in the same project.\n5.2: Building a Polyglot Data Science Toolkit\nData scientists can and should develop a toolkit that takes advantage of the strengths of both languages. For example, you might use Python‚Äôs scikit-learn for machine learning, R‚Äôs ggplot2 for advanced visualizations, and Python‚Äôs pandas for data manipulation.\n\nConclusion:\nThe ‚ÄúPython vs.¬†R‚Äù debate is less about choosing one over the other and more about understanding the strengths of each language and using them to your advantage. Both languages have a significant role to play in the data science landscape and knowing when to use each one is a skill every data scientist should cultivate.\nRemember, the best tool for the job often depends on the specific task, the industry you‚Äôre in, and your team‚Äôs capabilities and preferences. Always choose the right tool for the task and keep learning and adapting. After all, data science is a field that‚Äôs always evolving."
  },
  {
    "objectID": "Notes/timeseries.html",
    "href": "Notes/timeseries.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nTime series analysis is a statistical approach to analyze time-ordered data. It allows us to extract meaningful statistics and identify patterns to forecast future data points. We‚Äôll start with traditional methods and move towards advanced techniques like neural networks, providing examples in Python and R.\n\nPart I: Traditional Time Series Analysis Methods\n1.1: Autoregressive (AR) Models\nAutoregressive models forecast future values based on past values. It assumes that past values have a linear influence on future values.\n# Python example using statsmodels\nfrom statsmodels.tsa.ar_model import AutoReg\nmodel = AutoReg(data, lags=1)\nmodel_fit = model.fit()\n# R example using forecast package\nlibrary(forecast)\nfit <- auto.arima(data)\n1.2: Moving Average (MA) Models\nMA models use past forecast errors in a regression-like model. The dependence between an observation and an error in a previous time step is accounted for by this model.\n# Python example using statsmodels\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(data, order=(0, 0, 1))\nmodel_fit = model.fit()\n# R example using forecast package\nlibrary(forecast)\nfit <- auto.arima(data)\n1.3: Autoregressive Integrated Moving Average (ARIMA) Models\nARIMA combines autoregressive, differencing, and moving average elements. It‚Äôs a versatile model that can model a range of time series patterns.\n# Python example using statsmodels\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(data, order=(1, 1, 1))\nmodel_fit = model.fit()\n# R example using forecast package\nlibrary(forecast)\nfit <- auto.arima(data)\n\nPart II: Advanced Time Series Analysis Methods\n2.1: Recurrent Neural Networks (RNN)\nRNNs can use their internal state (memory) to process sequences of inputs, which makes them ideal for time series forecasting.\n# Python example using Keras\nfrom keras.models import Sequential\nfrom keras.layers import SimpleRNN\n\nmodel = Sequential()\nmodel.add(SimpleRNN(units=50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n2.2: Long Short-Term Memory (LSTM)\nLSTM is a type of RNN that can learn long-term dependencies. They‚Äôre widely used for sequence prediction problems and have proven to be particularly effective.\n# Python example using Keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nPart III: Evaluating Time Series Models\nRegardless of the type of model, it‚Äôs critical to evaluate its performance. Two common metrics for time series models are the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE).\n# Python example using sklearn\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\ny_pred = model.predict(test_data)\nMAE = mean_absolute_error(test_data, y_pred)\nRMSE = sqrt(mean_squared_error(test_data, y_pred))\n\n# R example using Metrics package\nlibrary(Metrics)\ny_pred <- forecast(fit, h=length(test_data))\nMAE = mae(test_data, y_pred$mean)\nRMSE = rmse(test_data, y_pred$mean)\n\nConclusion:\nTime series analysis is a complex field with a variety of methods ranging from traditional statistical models to advanced neural networks. The choice of method depends on the nature of the problem, the availability of data, and the level of accuracy required. Understanding the underlying principles of these methods is crucial in selecting the appropriate model and accurately interpreting its results."
  },
  {
    "objectID": "Notes/NeuralNetworks.html",
    "href": "Notes/NeuralNetworks.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nNeural networks and deep learning have revolutionized numerous fields, including image recognition, natural language processing, and autonomous vehicles. This series of tutorial notes will provide a foundational understanding of these topics.\n\nPart I: Introduction to Neural Networks\n1.1: What is a Neural Network?\nA neural network is a computing system inspired by the human brain‚Äôs neural network. It‚Äôs composed of interconnected processing nodes, called neurons or nodes. These nodes are organized in layers: an input layer, one or more hidden layers, and an output layer.\n1.2: The Neuron\nThe basic computational unit of a neural network is the neuron. It receives input from other neurons, processes it, and sends its output to other neurons in the network. Each input is weighted, and these weights get adjusted during the learning process.\n1.3: Activation Functions\nAn activation function defines the output of a neuron given a set of inputs. It transforms the weighted sum of the inputs into the output of the neuron. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh.\n\nPart II: The Learning Process in Neural Networks\n2.1: Forward Propagation\nDuring forward propagation, data flows from the input layer through the hidden layers to the output layer. Each neuron performs a weighted sum of inputs and passes the result through an activation function.\n2.2: Backward Propagation (Backpropagation)\nBackpropagation is a method to adjust the weights of the neurons based on the error of the network‚Äôs output. The network calculates the gradient of the loss function concerning each weight and adjusts the weights to minimize the loss.\n2.3: Gradient Descent and Optimizers\nGradient descent is an optimization algorithm that iteratively moves in the direction of steepest descent to minimize a function, in our case, the loss function. Optimizers like Stochastic Gradient Descent (SGD), Adam, RMSprop use different techniques to speed up this process.\n\nPart III: Introduction to Deep Learning\n3.1: What is Deep Learning?\nDeep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain‚Äôs neural networks. It uses multiple layers of neural networks to model complex patterns in data.\n3.2: Convolutional Neural Networks (CNNs)\nCNNs are a type of deep learning model particularly effective for image recognition tasks. They employ a mathematical operation called convolution to process data from the image pixels.\n3.3: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)\nRNNs are a class of neural networks for processing sequential data. LSTM is a type of RNN that can learn and remember over long sequences and is less likely to suffer from the vanishing gradient problem.\n\nPart IV: Regularization, Overfitting, and Underfitting\n4.1: Understanding Overfitting and Underfitting\nOverfitting occurs when a model learns the training data too well, including noise and outliers, leading to poor generalization to new data. Underfitting is when the model does not learn enough from the training data, resulting in poor performance on both training and test data.\n4.2: Regularization Techniques\nRegularization helps prevent overfitting by adding a penalty to the loss function based on the complexity of the model. Techniques include L1 and L2 regularization, dropout, and early stopping.\n\nPart V: Evaluation Metrics and Model Tuning\n5.1: Loss Functions\nLoss functions quantify how well the model‚Äôs predictions match the true values. Common loss functions include mean\nsquared error for regression tasks and cross-entropy for classification tasks.\n5.2: Evaluation Metrics\nEvaluation metrics help assess model performance. For classification tasks, these include accuracy, precision, recall, and F1-score. For regression tasks, common metrics include mean absolute error and root mean squared error.\n5.3: Hyperparameter Tuning\nHyperparameters are the parameters of the learning process, not learned from the data. Tuning them can significantly improve model performance. Techniques include grid search, random search, and Bayesian optimization.\n\nConclusion:\nNeural networks and deep learning represent a vast and rapidly evolving field. Understanding the foundational theory and principles equips you with the ability to effectively apply these tools, continue learning about new developments, and even contribute to pushing the boundaries of what‚Äôs possible.\nFor extra material go to the tutorial section"
  },
  {
    "objectID": "products.html#selected-projects-over-the-last-2-years",
    "href": "products.html#selected-projects-over-the-last-2-years",
    "title": "Projects",
    "section": "Selected projects over the last 2 years",
    "text": "Selected projects over the last 2 years\n\nPredicting Market Direction for Cocoa Futures\nBased on supply and demand data as well as key weather events I built time series models in order to predict the direction of price changes in futures contracts for cocoa in London and New York. I used prophet library and Keras, using recurrent neural networks. This model is used by traders in ECOM on a daily basis.\n\n\nPredicting Cocoa Production\nIn this project I used climatic variables to predict the yield of two crop seasons for cocoa in C√¥te d‚ÄôIvore. The model was a series of ensemble models that weighted the anomalies for each week/month and adjusted the forecast. The predictions have been tested over the last 2 years (4 seasons) with accurate results, even with models with early information.\n\n\nPredicting loss of suitability of coffee and cocoa due to climate change\nClimate change is having a major impact on the production of cocoa and coffee around the world. I used several models (Random Forest, Neural Networks and XGBoost) to model the current suitability of cocoa and coffee using several climatic variables. Once the model was calibrated on current conditions, the predictions were applied to future conditions to understand either the expansion or contraction of the areas of production that are suitable today. The results of these models has helped to develop the priorities to mitigate dramatic changes in the tropics.\n\n\nAmex Default Prediction\nI used this data to train junior peers to show them how to use different classification methods to extract value of large datasets. The data came from a Kaggle competition. I used gradient boosting decision trees. Specifically, I trained and evaluated the LGBMClassifier, XGBClassifier and the CatBoostClassifier models with gold standard results in the context of the competition to successfully identify whether a customer would default on their repayments of credit."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning that focuses on how an agent ought to take actions in an environment to maximize some notion of cumulative reward. In this tutorial, we will go over the fundamental principles of RL and how to implement them in Python and R.\n\n\nRL revolves around these main concepts:\n\nAgent: The RL algorithm that learns from trial and error.\nEnvironment: The context where the agent operates.\nState: The current situation of the agent.\nAction: All possible moves the agent can make.\nReward: Feedback from the environment.\n\nThe agent learns by interacting with its environment, it takes actions, the environment returns the new state and gives a reward. The goal of the agent is to maximize the sum of rewards.\n\n\n\nOne of the simplest ways to understand reinforcement learning is through Q-Learning. The objective of Q-Learning is to find a policy that is optimal in the sense that the expected value of the total reward over all successive steps is maximized.\nThe Q-Learning algorithm uses a table (Q-table) where we have a row for each state (s) and a column for each action (a). The cell at the intersection of state (s) and action (a) represents the expected future reward that the agent will get if it takes action (a) while in state (s).\nLet‚Äôs start with the Python implementation of Q-learning.\n\n\n\nFor our Python implementation, we will use the OpenAI Gym environment, which provides different game scenarios for training reinforcement learning agents. We‚Äôll work with the FrozenLake-v0 game.\nimport numpy as np\nimport gym\nimport random\nimport time\nfrom IPython.display import clear_output\n\n# Create the environment\nenv = gym.make(\"FrozenLake-v0\")\n\n# Initialize Q-table with zero\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n# Hyperparameters\nnum_episodes = 10000\nmax_steps_per_episode = 100\n\nlearning_rate = 0.1\ndiscount_rate = 0.99\n\nexploration_rate = 1\nmax_exploration_rate = 1\nmin_exploration_rate = 0.01\nexploration_decay_rate = 0.001\n\nrewards_all_episodes = []\n\n# Q-learning algorithm\nfor episode in range(num_episodes):\n    state = env.reset()\n\n    done = False\n    rewards_current_episode = 0\n\n    for step in range(max_steps_per_episode): \n        exploration_rate_threshold = random.uniform(0, 1)\n        if exploration_rate_threshold > exploration_rate:\n            action = np.argmax(q_table[state,:]) \n        else:\n            action = env.action_space.sample()\n\n        new_state, reward, done, info = env.step(action)\n\n        # Update Q-table for Q(s,a)\n        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n\n        state = new_state\n        rewards_current_episode += reward \n\n        if done == True: \n            break\n\n    # Exploration rate decay\n    exploration_rate = min_exploration_rate + \\\n        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n\n    rewards_all_episodes.append(rewards_current_episode)\nThis code creates an agent that learns to play\nthe FrozenLake-v0 game. The agent uses Q-Learning to learn how to play.\n\n\n\nNow let‚Äôs implement the same Q-Learning in R. We‚Äôre going to make a simple grid world with four cells, where the agent needs to find the terminal state to get a reward.\nlibrary(MDPtoolbox)\n\n# Define transition and reward matrices\nS <- 4\nA <- 2\nT <- array(0, c(S, A, S))\nR <- matrix(0, S, A)\n\n# Define transitions\nT[1, 1, 2] <- 1\nT[2, 1, 3] <- 1\nT[3, 1, 4] <- 1\nT[4, 1, 4] <- 1\n\nT[1, 2, 1] <- 1\nT[2, 2, 1] <- 1\nT[3, 2, 2] <- 1\nT[4, 2, 3] <- 1\n\n# Define rewards\nR[3, 1] <- 1\nR[4, 1] <- 1\n\n# Run Q-Learning\nresult <- mdp_example_qlearning(T, R, 0.9, 10000)\nThe code above creates a simple environment using the MDPtoolbox library. The environment is a simple four-state system where the agent learns to navigate to the terminal state using Q-Learning.\n\n\n\nReinforcement learning is a powerful approach for tasks that involve sequential decision-making. This tutorial presented the fundamental concepts of RL and walked through an example of how to implement Q-Learning, a simple but powerful RL algorithm, in Python and R. Remember that the RL field is vast and complex, this is just the tip of the iceberg!\nThis tutorial is intended as a starting point. I encourage you to continue exploring more complex environments, policies, and algorithms as you continue your journey in reinforcement learning. Happy learning!"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Full CV",
    "section": "",
    "text": "pdf copy here"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "Programming Python\nAdvanced proficiency in Pandas, GeoPandas, SciKit-Learn, TensorFlow, Pytorch, keras, Matplotlib, NumPy, SciPy, SQLAlchemy, Seaborn, Bokeh, SciKit-Image\n\n\nProgramming R\nAdvanced proficiency in RStudio, Shiny, tidyverse, Parsnip, bayestestR, sparklyr, keras, dygraph, forecast\n\n\nModelling\nAdvanced experience in statistical modelling, time series analysis, supervised and unsupervised machine learning algorithms, currently implementing solutions in reinforcement learning (intermediate)\n\n\nGeospatial Analysis\nAdvanced skills in geospatial machine learning, professional proficiency in ArcGIS, QGIS and OpenGeos (Python)\n\n\nBig Data & Databases\nProficient in SQL, MongoDB (NoSQL), Spark (pyspark and sparklyr)\n\n\nExperimental Design\nStrong background in designing and implementing A/B tests, and Bayesian statistics\n\n\nLanguages\nFluent in English and Spanish (native)"
  },
  {
    "objectID": "Tutorials/RF.html",
    "href": "Tutorials/RF.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nRandom Forests are a popular machine learning method that are highly flexible and capable of performing both regression and classification tasks. In this tutorial, we will explore Random Forests in depth, including when they are a good fit, their parameters and hyperparameters, and how to implement and optimize them in Python.\n\nPart I: Understanding Random Forests\n1.1: What is a Random Forest?\nA Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mean/ mode of the individual trees for regression/ classification tasks, respectively.\n1.2: When to Use Random Forests\nRandom Forests are particularly effective in situations where you have a large dataset with many features and complex relationships. They can handle both categorical and numerical data and are robust to outliers and missing data. They also prevent overfitting by averaging the output of many trees.\n\nPart II: Parameters and Hyperparameters of Random Forests\n2.1: Main Parameters\n\nn_estimators: The number of trees in the forest.\nmax_features: The maximum number of features considered when splitting a node.\nmax_depth: The maximum depth of the tree.\n\n2.2: Hyperparameters\nThese are parameters that cannot be learned from the data and are set prior to training. In Random Forests, these include all parameters like n_estimators, max_features, and max_depth.\n\nPart III: Implementing Random Forests in Python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instantiate the random forest model\nrf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\nPart IV: Optimizing Random Forests\nOptimizing a Random Forest involves tuning its hyperparameters to find the best combination for your specific dataset. This can be achieved using methods such as Grid Search and Randomized Search, as we discussed in the previous tutorial.\n4.1: Grid Search Example\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8]\n}\n\n# Create a based model\nrf = RandomForestClassifier()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\nConclusion:\nRandom Forests are a powerful and flexible machine learning algorithm that can provide excellent results on a wide range of problems. Understanding the underlying mechanics, knowing how to implement them in Python, and being able to optimize their hyperparameters are all vital skills for any machine learning practitioner. Remember, as with all machine learning algorithms, it‚Äôs important to thoroughly understand your data and the problem at hand to make informed decisions on algorithm selection and tuning."
  },
  {
    "objectID": "Tutorials/SQL.html",
    "href": "Tutorials/SQL.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nSQL (Structured Query Language) is a standard language for managing and manipulating relational databases. This tutorial will introduce you to SQL, starting with the basics and advancing to more complex topics.\n\nPart I: SQL Basics\n1.1: Creating a Database\nCreating a database is the first step in SQL. The CREATE DATABASE statement is used to do this.\nCREATE DATABASE myDatabase;\n1.2: Creating a Table\nTables are created within a database to store data. The CREATE TABLE statement is used, followed by the columns‚Äô names and data types.\nCREATE TABLE Employees (\n    ID INT PRIMARY KEY,\n    FirstName VARCHAR(100),\n    LastName VARCHAR(100),\n    Email VARCHAR(50),\n    Age INT\n);\n1.3: Inserting Data\nThe INSERT INTO statement is used to insert data into a table.\nINSERT INTO Employees (ID, FirstName, LastName, Email, Age)\nVALUES (1, 'John', 'Doe', 'john.doe@example.com', 30);\n1.4: Querying Data\nThe SELECT statement is used to select data from a database.\nSELECT * FROM Employees;\n\nPart II: SQL Intermediate\n2.1: WHERE Clause\nThe WHERE clause is used to filter records.\nSELECT * FROM Employees WHERE Age > 30;\n2.2: UPDATE Statement\nThe UPDATE statement is used to modify existing records in a table.\nUPDATE Employees SET Email = 'j.doe@example.com' WHERE ID = 1;\n2.3: DELETE Statement\nThe DELETE statement is used to delete existing records in a table.\nDELETE FROM Employees WHERE ID = 1;\n\nPart III: SQL Advanced\n3.1: JOIN\nA JOIN clause is used to combine rows from two or more tables, based on a related column.\nSELECT Orders.OrderID, Customers.CustomerName\nFROM Orders\nINNER JOIN Customers\nON Orders.CustomerID = Customers.CustomerID;\n3.2: GROUP BY\nThe GROUP BY statement groups rows that have the same values in specified columns into aggregated data.\nSELECT COUNT(ID), Age\nFROM Employees\nGROUP BY Age;\n3.3: Subqueries\nA subquery is a query nested inside another query and can be used in SELECT, INSERT, UPDATE, or DELETE statements.\nSELECT FirstName, LastName\nFROM Employees\nWHERE Age > (SELECT AVG(Age) FROM Employees);\n3.4: Views\nA view is a virtual table based on the result-set of an SQL statement. A view contains rows and columns, just like a real table.\nCREATE VIEW [Old Employees] AS\nSELECT FirstName, LastName\nFROM Employees\nWHERE Age > 60;\n\nConclusion:\nSQL is a powerful language that‚Äôs essential for any data professional. Understanding SQL from basics to advanced topics will enable you to work efficiently with databases, an essential skill in today‚Äôs data-driven world. Always remember to practice regularly to become proficient."
  },
  {
    "objectID": "Tutorials/ImageClassification.html",
    "href": "Tutorials/ImageClassification.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nImage Classification is a key application of Deep Learning, with uses ranging from medical imaging to autonomous driving. In this tutorial, we‚Äôll explore Convolutional Neural Networks (CNNs), a class of models that has proven to be highly effective for image classification tasks. We will also discuss how to implement them in Python and explore ways to optimize them.\n\nPart I: Understanding Image Classification with Deep Learning\n1.1: What is Image Classification?\nImage Classification is a process of assigning a class label to an image from a predefined set. This is often achieved by training a model on a large number of images with known class labels, then applying this model to classify new images.\n1.2: Convolutional Neural Networks (CNNs)\nCNNs are specifically designed to process grid-like data, where there‚Äôs a spatial relationship between the pixels. This makes them ideal for image processing tasks.\n\nPart II: Implementing CNNs in Python for Image Classification\nWe will use the Keras library to build a simple CNN for image classification. Here‚Äôs an example of a simple CNN architecture:\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Initialize the CNN\nmodel = Sequential()\n\n# Add a Convolutional layer\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n\n# Add a Pooling layer\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten\nmodel.add(Flatten())\n\n# Full connection\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32)\nCode Explanation:\n\nWe first import the necessary libraries and initialize a Sequential model.\nWe then add a Conv2D layer. This is the convolutional layer that will extract features from the input images by sliding a convolution filter over the input to produce a feature map. Here we choose 32 filters and each filter is of size 3 by 3.\nNext, we add a MaxPooling2D layer. This is a pooling layer with a 2x2 filter that reduces the dimensions of the feature maps and helps to make the model invariant to small translations.\nWe then flatten the 2D arrays into a 1D array to feed into a Dense layer (also called a fully connected layer).\nTwo Dense layers are added. The first has 128 units, and the second has 1 unit, which is the output layer (as this is a binary classification problem).\nThe model is then compiled with the ‚Äòadam‚Äô optimizer, ‚Äòbinary_crossentropy‚Äô as the loss function, and ‚Äòaccuracy‚Äô as the evaluation metric.\nFinally, we fit the model to our training data.\n\n\nPart III: Optimizing CNNs\nOptimizing a CNN can involve adjusting the architecture (number and types of layers, number of nodes in layers), tuning the hyperparameters, or using techniques such as data augmentation or transfer learning.\n3.1: Adjusting the Architecture\nFor example, we can add more convolutional layers or fully connected layers, change the size of the filters, or add Dropout layers to prevent overfitting.\n3.2: Hyperparameter Tuning\nThis could involve adjusting the learning rate, batch size, number of epochs, or parameters related to specific layers (like the number\nof filters in a Conv2D layer).\n3.3: Data Augmentation and Transfer Learning\nData Augmentation involves creating new training examples by applying random jitters and transformations to the images, which can improve performance and reduce overfitting.\nTransfer Learning involves taking a pre-trained model (usually trained on a large dataset like ImageNet) and fine-tuning it on your specific task. This can often yield excellent results with less data and compute resources.\n\nConclusion:\nImage Classification with CNNs is a key aspect of Deep Learning, with a wide range of applications. Understanding how these models work, how to implement them in Python, and how to optimize them are crucial skills for anyone working with image data. Always consider the complexity of your specific problem and the resources at your disposal when selecting a model and optimization strategy."
  },
  {
    "objectID": "Tutorials/xgboost.html",
    "href": "Tutorials/xgboost.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nXGBoost, short for eXtreme Gradient Boosting, is an optimized and scalable machine learning system for tree boosting. In this tutorial, we‚Äôll explore the workings of XGBoost, when it‚Äôs a good choice, its parameters and hyperparameters, and how to implement and optimize it in Python.\n\nPart I: Understanding XGBoost\n1.1: What is XGBoost?\nXGBoost is an ensemble learning method that applies the principle of gradient boosting to decision trees. It‚Äôs recognized for its execution speed and model performance, which have made it a favorite tool among Kaggle competition winners.\n1.2: When to Use XGBoost\nXGBoost is a great choice when dealing with numerical or mixed data types, and when prediction accuracy is a major concern. It can handle both regression and classification problems, but it might be overkill for simple problems where a simpler model could work better.\n\nPart II: Parameters and Hyperparameters of XGBoost\n2.1: Main Parameters\n\nn_estimators: The number of gradient boosted trees. Equivalent to number of boosting rounds.\nmax_depth: Maximum tree depth for base learners.\nlearning_rate: Boosting learning rate (also known as eta).\ngamma: Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\n2.2: Hyperparameters\nJust like with Random Forests, the parameters listed above are hyperparameters, meaning they are set before the learning process begins.\n\nPart III: Implementing XGBoost in Python\nimport xgboost as xgb\n\n# Instantiate the XGBClassifier\nxgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n\n# Fit the classifier to the training set\nxgb_clf.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = xgb_clf.predict(X_test)\n\nPart IV: Optimizing XGBoost\nOptimizing XGBoost involves tuning its hyperparameters to get the best performance for a specific dataset. We can again use techniques like Grid Search and Randomized Search.\n4.1: Grid Search Example\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300, 500],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth' : [4,5,6,7,8],\n    'gamma': [0, 0.1, 0.2]\n}\n\n# Create a based model\nxgb_clf = xgb.XGBClassifier()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = xgb_clf, param_grid = param_grid, cv = 3)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\nConclusion:\nXGBoost is a powerful machine learning algorithm that excels in many contexts, particularly where accuracy is paramount. Understanding the ins and outs of XGBoost, knowing how to implement it in Python, and being able to optimize its hyperparameters are essential skills for any data scientist or machine learning engineer."
  },
  {
    "objectID": "Tutorials/Regression2.html",
    "href": "Tutorials/Regression2.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nIn the previous tutorial, we discussed Lasso, Ridge, and Elastic Net regressions. In this tutorial, we will focus on how to optimize these models through hyperparameter tuning.\n\nPart I: Understanding Hyperparameters\n1.1: What are Hyperparameters?\nHyperparameters are parameters that are not learned from data. They are set prior to the commencement of the learning process and help to control the behavior of the model. For instance, the ‚Äòalpha‚Äô in our previous tutorial is a hyperparameter.\n\nPart II: Methods for Hyperparameter Tuning\n2.1: Grid Search\nGrid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on the specific parameter values of a model.\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the grid of hyperparameters 'params'\nparams = {'alpha': [0.02, 0.024, 0.025, 0.026, 0.03]}\n\n# Instantiate a 10-fold CV grid search object 'grid'\ngrid = GridSearchCV(estimator=lasso, param_grid=params, cv=10)\n\n# Fit the grid search\ngrid.fit(X_train, y_train)\n2.2: Random Search\nRandom search differs from grid search in that it does not try every parameter value. Instead, it randomly selects values within the ranges specified.\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Define the grid of hyperparameters 'params'\nparams = {'alpha': [0.02, 0.024, 0.025, 0.026, 0.03]}\n\n# Instantiate a 10-fold CV randomized search object 'random'\nrandom = RandomizedSearchCV(estimator=ridge, param_distributions=params, cv=10)\n\n# Fit the random search\nrandom.fit(X_train, y_train)\n\nPart III: Hyperparameter Tuning for Regression Models\n3.1: Tuning Lasso Regression\nWhen tuning Lasso Regression, the main hyperparameter to adjust is ‚Äòalpha‚Äô, which controls the amount of shrinkage: the larger the value of alpha, the greater the amount of shrinkage.\n3.2: Tuning Ridge Regression\nIn Ridge Regression, ‚Äòalpha‚Äô is also the primary hyperparameter. However, the impact is slightly different: alpha reduces the complexity of the model by coefficient shrinkage.\n3.3: Tuning Elastic Net Regression\nElastic Net has two key hyperparameters: ‚Äòalpha‚Äô and ‚Äòl1_ratio‚Äô. ‚ÄòAlpha‚Äô is the penalty term and ‚Äòl1_ratio‚Äô is the mix between L1 and L2 regularization.\n\nConclusion:\nHyperparameter tuning is crucial in building optimal machine learning models. Although it can be a time-consuming process, Python, with libraries such as Scikit-Learn, provides efficient tools for carrying out this process effectively. Tuning your models can lead to significant improvements in model performance."
  },
  {
    "objectID": "Tutorials/GoogleEarth.html",
    "href": "Tutorials/GoogleEarth.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nGoogle Earth Engine is a cloud-based platform for planetary-scale environmental data analysis. It combines a multi-petabyte catalog of satellite imagery and geospatial datasets with computational capabilities to analyze and extract insights. In this blog, we will explore how to use Google Earth Engine, access its data, what types of analysis are possible, and how to produce composite images and measure deforestation.\n\nPart I: Getting Started with Google Earth Engine\n1.1: Setting Up\nTo begin, you will need to sign up for a Google Earth Engine account. Once approved, you can access the Code Editor at https://code.earthengine.google.com/.\n1.2: Navigating the Interface\nThe Google Earth Engine Code Editor is divided into several panels:\n\nScripts panel: where you can create, save, and manage your scripts.\nCode Editor panel: where you enter JavaScript code.\nMap panel: where results are displayed.\nInspector panel: to query map results.\n\n\nPart II: Accessing Data\n2.1: Image and ImageCollection\nIn Google Earth Engine, raster data are represented as Image objects. A collection of images is an ImageCollection. You can access satellite imagery and other raster datasets from the Earth Engine Data Catalog.\n2.2: Example - Accessing Landsat Imagery\nvar image = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20140318');\nMap.addLayer(image, {bands: ['B4', 'B3', 'B2'], max: 0.3}, 'true color image');\n\nPart III: Types of Analysis\n3.1: Basic Operations\nYou can perform arithmetic operations on images, compute spectral indices, and extract statistics.\n3.2: Example - NDVI Calculation\nvar ndvi = image.normalizedDifference(['B5', 'B4']);\nMap.addLayer(ndvi, {min: 0, max: 1, palette: ['white', 'green']}, 'NDVI');\n\nPart IV: Creating Composite Images\nCreating a composite image involves combining several images into one. In the context of Google Earth Engine, you can create a composite image from an ImageCollection.\n4.1: Example - Median Composite\nvar collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_TOA')\n    .filterDate('2019-01-01', '2019-12-31');\n\nvar composite = collection.median();\n\nMap.addLayer(composite, {bands: ['B4', 'B3', 'B2'], max: 0.3}, 'composite image');\n\nPart V: Measuring Deforestation with Google Earth Engine\nGoogle Earth Engine can be used to monitor and measure deforestation. You can compute the difference between two images or track changes over time.\n5.1: Example - Difference in NDVI\nvar image1 = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20140101');\nvar image2 = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20141231');\n\nvar ndvi1 = image1.normalizedDifference(['B5', 'B4']);\nvar ndvi2 = image2.normalizedDifference(['B5', 'B4']);\n\nvar difference = ndvi2.subtract(ndvi1);\nMap.addLayer(difference, {min: -1, max: 1, palette: ['blue', 'white', 'green\n\n']}, 'NDVI difference');\n\nConclusion:\nGoogle Earth Engine is a powerful platform for large-scale geospatial data analysis. It provides an accessible interface for both accessing vast amounts of data and applying complex analyses to generate meaningful insights. As we‚Äôve seen in the blog, it can be used to monitor and measure environmental phenomena such as deforestation. While we‚Äôve only scratched the surface, I encourage you to explore the platform further, as the range of applications is vast."
  },
  {
    "objectID": "Tutorials/Forecasting.html",
    "href": "Tutorials/Forecasting.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "When it comes to predicting future trends, events, or quantities‚Äîknown as forecasting‚Äîtime series methods are some of the most commonly used approaches. By definition, a time series is a sequence of data points recorded at successive, equally spaced intervals. Given the unique traits of time series data, specialized analysis techniques have been developed over time, and these methods can be used to forecast future data points.\nIn this blog post, we will look at how to apply time series methods to forecast data using two popular programming languages: R and Python.\n\n\nBefore delving into the coding aspect, it‚Äôs important to understand the key components of a time series:\n\nTrend: The underlying pattern of growth or decline in a time series.\nSeasonality: Regular and predictable changes that recur every calendar year.\nCyclical components: Fluctuations happening due to economic cycles.\nIrregular (or residual) component: The random variation in the series.\n\n\n\n\nR provides robust support for time series analysis via several specialized packages like forecast, tseries, and tsibble.\nIn this example, we will use the forecast package to analyze the AirPassengers dataset.\n\n# Loading required library\n\nlibrary(forecast)\n\n# Load the dataset\n\ndata(AirPassengers)\n\n# Plotting the data\n\nplot(AirPassengers)\n\n# Decomposing the time series\n\ndecomposed_ap <- decompose(AirPassengers)\n\n# Plotting the decomposed time series\n\nplot(decomposed_ap)\n\n# Applying the ARIMA model\n\narima_model <- auto.arima(AirPassengers)\n\n# Forecasting for next 24 months\n\nforecast_arima <- forecast(arima_model, h=24)\n\n# Plotting the forecast\n\nplot(forecast_arima)\nIn this example, we first decomposed the time series to understand its underlying components. We then fitted an ARIMA model using the auto.arima() function, and used it to forecast the next 24 months.\n\n\n\nPython also has strong support for time series analysis, particularly through the pandas and statsmodels libraries.\nLet‚Äôs analyze the same AirPassengers dataset in Python.\n\n# Importing required libraries\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nimport matplotlib.pyplot as plt\n\n# Load the dataset\n\nair_passengers = pd.read_csv('AirPassengers.csv', parse_dates=['Month'], index_col='Month')\n\n# Plotting the data\n\nair_passengers.plot()\n\n# Decomposing the time series\n\ndecomposition = seasonal_decompose(air_passengers)\n\n# Plotting the decomposed time series\n\ndecomposition.plot()\n\nplt.show()\n\n# Applying the ARIMA model\n\nmodel = ARIMA(air_passengers, order=(5,1,0))\n\nmodel_fit = model.fit(disp=0)\n\n# Forecasting for next 24 months\n\nforecast, stderr, conf_int = model_fit.forecast(steps=24)\n\n# Plotting the forecast\n\nplt.plot(forecast)\n\nplt.fill_between(range(len(forecast)), forecast - stderr, forecast + stderr, color='r', alpha=.1)\n\nplt.show()\nHere, we used the seasonal_decompose() function from statsmodels to decompose the time series, fitted an ARIMA model, and made a forecast for the next 24 months.\n\n\n\nIn summary, time series forecasting is a powerful tool for\npredicting future trends and events. Both R and Python provide excellent support for time series analysis and can be chosen based on the specific requirements of your project and your familiarity with the language. Happy forecasting!"
  },
  {
    "objectID": "Tutorials/Regression.html",
    "href": "Tutorials/Regression.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nRegression analysis is a statistical technique that models the relationship between a dependent variable and one or more independent variables. In this blog, we will explore three types of regression analysis: Lasso, Ridge, and Elastic Net.\n\nPart I: Understanding Regression\n1.1: Linear Regression\nLinear regression is a simple and commonly used regression method. It assumes a linear relationship between the dependent and independent variables.\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression object\nreg = LinearRegression()\n\n# Train the model using the training sets\nreg.fit(X_train, y_train)\n\nPart II: Regularization in Regression\n2.1: Understanding Regularization\nRegularization adds a penalty term to the objective function and controls the model complexity using that penalty term. Regularization techniques used in linear regression are Lasso, Ridge, and Elastic Net.\n\nPart III: Lasso Regression\n3.1: Understanding Lasso Regression\nLasso regression (Least Absolute Shrinkage and Selection Operator) performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients.\n3.2: Implementing Lasso Regression in Python\nfrom sklearn.linear_model import Lasso\n\n# Create lasso regression object\nlasso = Lasso(alpha=0.1)\n\n# Train the model using the training sets\nlasso.fit(X_train, y_train)\n\nPart IV: Ridge Regression\n4.1: Understanding Ridge Regression\nRidge regression performs L2 regularization, which adds a penalty equal to the square of the magnitude of coefficients.\n4.2: Implementing Ridge Regression in Python\nfrom sklearn.linear_model import Ridge\n\n# Create ridge regression object\nridge = Ridge(alpha=0.1)\n\n# Train the model using the training sets\nridge.fit(X_train, y_train)\n\nPart V: Elastic Net Regression\n5.1: Understanding Elastic Net Regression\nElastic Net regression is a combination of both L1 and L2 regularization. It has a hyperparameter to control the balance between them.\n5.2: Implementing Elastic Net Regression in Python\nfrom sklearn.linear_model import ElasticNet\n\n# Create elastic net regression object\nenet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n\n# Train the model using the training sets\nenet.fit(X_train, y_train)\n\nConclusion:\nRegularization is a powerful technique to prevent overfitting in regression models. Lasso, Ridge, and Elastic Net are key regularization techniques that each have their strengths and use-cases. Python, with its powerful libraries such as Scikit-Learn, provides straightforward methods to implement these techniques."
  },
  {
    "objectID": "Tutorials/RL.html",
    "href": "Tutorials/RL.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning that focuses on how an agent ought to take actions in an environment to maximize some notion of cumulative reward. In this tutorial, we will go over the fundamental principles of RL and how to implement them in Python and R.\n\n\nRL revolves around these main concepts:\n\nAgent: The RL algorithm that learns from trial and error.\nEnvironment: The context where the agent operates.\nState: The current situation of the agent.\nAction: All possible moves the agent can make.\nReward: Feedback from the environment.\n\nThe agent learns by interacting with its environment, it takes actions, the environment returns the new state and gives a reward. The goal of the agent is to maximize the sum of rewards.\n\n\n\nOne of the simplest ways to understand reinforcement learning is through Q-Learning. The objective of Q-Learning is to find a policy that is optimal in the sense that the expected value of the total reward over all successive steps is maximized.\nThe Q-Learning algorithm uses a table (Q-table) where we have a row for each state (s) and a column for each action (a). The cell at the intersection of state (s) and action (a) represents the expected future reward that the agent will get if it takes action (a) while in state (s).\nLet‚Äôs start with the Python implementation of Q-learning.\n\n\n\nFor our Python implementation, we will use the OpenAI Gym environment, which provides different game scenarios for training reinforcement learning agents. We‚Äôll work with the FrozenLake-v0 game.\nimport numpy as np\nimport gym\nimport random\nimport time\nfrom IPython.display import clear_output\n\n# Create the environment\nenv = gym.make(\"FrozenLake-v0\")\n\n# Initialize Q-table with zero\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n# Hyperparameters\nnum_episodes = 10000\nmax_steps_per_episode = 100\n\nlearning_rate = 0.1\ndiscount_rate = 0.99\n\nexploration_rate = 1\nmax_exploration_rate = 1\nmin_exploration_rate = 0.01\nexploration_decay_rate = 0.001\n\nrewards_all_episodes = []\n\n# Q-learning algorithm\nfor episode in range(num_episodes):\n    state = env.reset()\n\n    done = False\n    rewards_current_episode = 0\n\n    for step in range(max_steps_per_episode): \n        exploration_rate_threshold = random.uniform(0, 1)\n        if exploration_rate_threshold > exploration_rate:\n            action = np.argmax(q_table[state,:]) \n        else:\n            action = env.action_space.sample()\n\n        new_state, reward, done, info = env.step(action)\n\n        # Update Q-table for Q(s,a)\n        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n\n        state = new_state\n        rewards_current_episode += reward \n\n        if done == True: \n            break\n\n    # Exploration rate decay\n    exploration_rate = min_exploration_rate + \\\n        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n\n    rewards_all_episodes.append(rewards_current_episode)\nThis code creates an agent that learns to play\nthe FrozenLake-v0 game. The agent uses Q-Learning to learn how to play.\n\n\n\nNow let‚Äôs implement the same Q-Learning in R. We‚Äôre going to make a simple grid world with four cells, where the agent needs to find the terminal state to get a reward.\nlibrary(MDPtoolbox)\n\n# Define transition and reward matrices\nS <- 4\nA <- 2\nT <- array(0, c(S, A, S))\nR <- matrix(0, S, A)\n\n# Define transitions\nT[1, 1, 2] <- 1\nT[2, 1, 3] <- 1\nT[3, 1, 4] <- 1\nT[4, 1, 4] <- 1\n\nT[1, 2, 1] <- 1\nT[2, 2, 1] <- 1\nT[3, 2, 2] <- 1\nT[4, 2, 3] <- 1\n\n# Define rewards\nR[3, 1] <- 1\nR[4, 1] <- 1\n\n# Run Q-Learning\nresult <- mdp_example_qlearning(T, R, 0.9, 10000)\nThe code above creates a simple environment using the MDPtoolbox library. The environment is a simple four-state system where the agent learns to navigate to the terminal state using Q-Learning.\n\n\n\nReinforcement learning is a powerful approach for tasks that involve sequential decision-making. This tutorial presented the fundamental concepts of RL and walked through an example of how to implement Q-Learning, a simple but powerful RL algorithm, in Python and R. Remember that the RL field is vast and complex, this is just the tip of the iceberg!\nThis tutorial is intended as a starting point. I encourage you to continue exploring more complex environments, policies, and algorithms as you continue your journey in reinforcement learning. Happy learning!"
  },
  {
    "objectID": "Tutorials/advSQL.html",
    "href": "Tutorials/advSQL.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nBuilding on our SQL basics and intermediate knowledge, this tutorial will delve deeper into advanced SQL techniques, which can help in managing and manipulating complex relational databases.\n\nPart I: Advanced Querying\n1.1: Self Joins\nSelf join is a regular join, but the table is joined with itself.\nSELECT A.FirstName, B.FirstName, A.Age\nFROM Employees A, Employees B\nWHERE A.Age = B.Age\n1.2: Union and Union All\nUNION is used to combine the result-set of two or more SELECT statements. UNION removes duplicate records, UNION ALL includes duplicates.\nSELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n1.3: INTERSECT and EXCEPT\nINTERSECT returns the common records in both SELECT statement results. EXCEPT returns the difference between the first and second SELECT statement.\nSELECT column_name(s) FROM table1\nINTERSECT\nSELECT column_name(s) FROM table2;\n\nPart II: Stored Procedures and Functions\n2.1: Stored Procedures\nStored procedures are a prepared SQL code that can be saved and reused. It can take parameters, and can also have complex logic with control statements like IF, WHILE, etc.\nCREATE PROCEDURE SelectAllEmployees\nAS\nSELECT * FROM Employees;\nGO\n2.2: Functions\nFunctions are compiled SQL statements that can return a value. They can take parameters, and are used to encapsulate reusable computations.\nCREATE FUNCTION GetTotalEmployees()\nRETURNS INT AS\nBEGIN\n  RETURN (SELECT COUNT(*) FROM Employees);\nEND;\n\nPart III: Transactions and Indexing\n3.1: Transactions\nA transaction is a unit of work that is performed against a database. COMMIT saves the transaction; ROLLBACK ends the transaction and undoes any changes.\nBEGIN TRANSACTION;\nUPDATE Employees SET Age = Age + 1;\nCOMMIT;\n3.2: Indexing\nIndexes are used to retrieve data from the database more quickly. They are particularly useful in large databases.\nCREATE INDEX idx_Employees\nON Employees (LastName);\n\nPart IV: Triggers\nTriggers are automatic actions that are performed when insert, update or delete commands are executed on a database table.\nCREATE TRIGGER trg_AfterInsert\nAFTER INSERT ON Employees\nFOR EACH ROW\n   BEGIN\n   INSERT INTO AuditTable SET action = 'insert', employeeName = NEW.FirstName;\n   END;\n\nConclusion:\nAdvanced SQL provides you with the tools to handle complex data manipulation and management tasks. Understanding these concepts is essential for anyone working extensively with relational databases. Remember, practice is key in mastering SQL."
  },
  {
    "objectID": "Tutorials/NeuralNetworks.html",
    "href": "Tutorials/NeuralNetworks.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nDeep Learning provides powerful tools for time series analysis. Models like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Convolutional Neural Networks (CNNs) have proven to be effective for tasks such as time series forecasting. In this tutorial, we‚Äôll explore these models, how they can be implemented in Python, and some ways to optimize them.\n\nPart I: Understanding Time Series Analysis with Deep Learning\n1.1: What is Time Series Analysis?\nTime Series Analysis involves studying ordered data points collected over time to identify patterns, trends, cycles, and forecast future data points.\n1.2: Deep Learning for Time Series Analysis\nDeep Learning models have shown significant success in Time Series Analysis due to their ability to learn complex temporal dependencies, handle large amounts of data, and automatically extract features.\n\nPart II: Deep Learning Models for Time Series Analysis\n2.1: Recurrent Neural Networks (RNNs)\nRNNs are designed to use their internal state (memory) to process sequences of inputs, which makes them ideal for time series forecasting.\n2.2: Long Short-Term Memory (LSTM)\nLSTMs are an extension of RNNs and were designed to remember long-term dependencies in sequence data.\n2.3: Convolutional Neural Networks (CNNs)\nThough traditionally used for image processing, CNNs can also be used for time series forecasting. They can learn to automatically extract spatial and temporal features from raw input data.\n\nPart III: Implementing Deep Learning Models in Python\nUsing the Keras library, we can easily implement these models. Here‚Äôs an example of an LSTM:\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\n# Define model architecture\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Fit the model\nmodel.fit(X, y, epochs=200, verbose=0)\n\nPart IV: Optimizing Deep Learning Models\nOptimization of Deep Learning models for time series analysis is typically done via hyperparameter tuning and architecture design.\n4.1: Hyperparameter Tuning\nJust like in previous algorithms we have discussed, we can adjust parameters such as the learning rate, number of hidden units, number of layers, and many others to optimize the model‚Äôs performance.\n4.2: Architecture Design\nThe design of the model‚Äôs architecture can also have a significant impact on performance. For instance, one could decide to stack multiple LSTM layers or use a combination of CNN and LSTM layers.\n\nConclusion:\nDeep Learning provides a suite of powerful tools for time series analysis. Understanding these models, their application to time series data, and how to implement and optimize them in Python are key skills in the modern data scientist‚Äôs toolbox. Always remember, though, that these are complex models, and simpler methods may often provide equally effective solutions. Always consider the complexity and requirements of your specific problem when selecting a model."
  }
]