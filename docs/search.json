[
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Full CV",
    "section": "",
    "text": "pdf copy here"
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "Education",
    "section": "",
    "text": "Degree\nInstitution\nDuration\nDetails\n\n\n\n\nPhD (DPhil), Plant Sciences\nUniversity of Oxford\n2013 - 2017\nApplied machine learning, computational modelling, and advanced statistics to analyse plant behaviour under drought stress and the impact of climate change on plant species distribution. Used various techniques including Neural Networks, XGBoost, Random Forest, Clustering analysis, and A/B tests using bootstrapping and Bayesian statistics.\n\n\nMSc, Biology (1st place)\nUniversidad de los Andes, Colombia\n2012\nUtilized R to explore hypotheses about biochemical changes in juvenile plants under drought conditions.\n\n\nBSc, Biology\nUniversidad de los Andes, Colombia\n2010\nGained proficiency in R during biostatistics coursework and applied it extensively in my dissertation and subsequent work."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr Juan D. Beltran",
    "section": "",
    "text": "Hi and welcome to my website!\nHere you can find my cv and more information about my data science journey! As well as some tutorials and notes that I share regularly with my team!\nI am a lead data scientist passionate about developing useful, effective and practical data science products to solve commercial problems in different industries. I have a strong academic background and I build my data solutions following rigorous and pragmatic data science principles. I love analysing and solving complex problems that involve data analysis. I am always eager to learn new techniques and knowledge to incorporate them into my data science toolbox.\nI have been coding in R for the last 15 years and in Python for the last 9 years. I have designed, developed and maintained a wide range of data products which combine machine learning models and GIS analysis in the shape of stand alone models, dashboards, advanced visualisation, automatic reports, and web apps. I have extensive knowledge in statistical modelling, forecasting time series and geospatial analysis.\nWhen I am not coding I really enjoy going to the gym, hiking outdoors or drinking cocktails!"
  },
  {
    "objectID": "juan.html",
    "href": "juan.html",
    "title": "Brief Description",
    "section": "",
    "text": "As a seasoned data scientist with more than 7 years of experience across various sectors, including the global soft commodities trading industry, I have developed an advanced understanding of machine learning, time series analysis, and GIS. These skills have been instrumental in enabling me to discern complex data patterns and provide actionable insights within the trading ecosystem. My technical expertise is underscored by my proficiency in Python and R programming languages, tools I‚Äôve leveraged extensively to address data-driven challenges in the commodities market.\nMy project management skills are honed through practical experience with cross-functional teams, Agile methodologies, and continuous integration/delivery principles on the lookout for new ways to create business value.\nIn addition, my leadership qualities are widely acknowledged, demonstrated by my commitment to mentorship and fostering data-centric approaches across diverse teams. My approach to problem-solving fuses intense curiosity with unwavering rigour, enabling me to address complex issues effectively. Furthermore, I excel in distilling intricate concepts into accessible narratives, ensuring clear and empathetic communication with stakeholders. This blend of leadership, problem-solving, and communication skills has been a cornerstone of my data science journey."
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": "Contact me",
    "section": "",
    "text": "üì± +44 07922857788\nüíª Website"
  },
  {
    "objectID": "Notes/commenting.html",
    "href": "Notes/commenting.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Commenting Code and Best Practices for Code Maintenance\nIntroduction:\nWriting clean, readable, and maintainable code is just as important as writing code that works. Good commenting and maintenance practices make your code more understandable to others (and your future self). Let‚Äôs dive into why and how we should comment our code and some best practices for code maintenance.\n\nPart I: Understanding Code Commenting\n1.1: What is Code Commenting?\nCode commenting involves adding descriptive lines to your code to explain what certain parts of the code are intended to do. These lines are ignored by the compiler or interpreter.\n1.2: Why is Commenting Important?\nComments improve the readability of your code by providing context. They can explain the purpose of a particular code block, provide additional information about a complex algorithm, or clarify a tricky section of code.\n\nPart II: Best Practices for Commenting Code\n2.1: Comment on the ‚ÄúWhy‚Äù, not the ‚ÄúWhat‚Äù\nYour comments should explain why the code does something, not what it does. If the code‚Äôs purpose is not immediately clear, consider refactoring it to make it more readable.\n2.2: Keep Comments Concise\nComments should be brief and to the point. If you need a lengthy explanation to understand a piece of code, it may be a sign that you need to simplify or refactor your code.\n2.3: Avoid Redundant Comments\nDon‚Äôt write comments for code that is self-explanatory. For example, there‚Äôs no need to comment on a line of code that just increments a counter.\n2.4: Update Comments as Code Changes\nMake sure your comments always reflect the current state of your code. Outdated comments can lead to confusion and misinformation.\n\nPart III: Importance of Code Maintenance\nMaintaining code is about ensuring your code stays clean, understandable, and easy to debug over time. This includes commenting, but also involves practices like refactoring, using version control, and regularly reviewing code.\n\nPart IV: Best Practices for Code Maintenance\n4.1: Regular Refactoring\nRefactoring involves changing the code‚Äôs structure without changing its functionality. Regular refactoring makes your code more efficient, readable, and maintainable.\n4.2: Use Version Control Systems\nVersion control systems like Git help you track changes, understand how your codebase evolves over time, and make it easier to revert changes or branch out for testing new features.\n4.3: Conduct Regular Code Reviews\nRegularly reviewing your code and your team‚Äôs code can help spot potential issues before they become problems. Code reviews also promote knowledge sharing among team members.\n4.4: Follow a Coding Standard\nAdhere to a coding standard or style guide to keep your code consistent. This will make it easier for others (and yourself) to read and understand your code.\n4.5: Write Unit Tests\nWriting unit tests can help you catch bugs early, ensure that your code works as expected, and prevent future changes from breaking functionality.\n\nConclusion:\nCommenting your code and maintaining it properly is crucial for long-term project success. Good practices help your future self and others understand your code better, make it easier to debug, and create a more collaborative environment. Remember that writing code is not just about getting a machine to perform tasks, but also about communicating your thought process effectively to other developers."
  },
  {
    "objectID": "Notes/DokervsKubernets.html",
    "href": "Notes/DokervsKubernets.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction:\nDocker and Kubernetes are two prominent tools used in deploying and managing applications in a modern development pipeline. Although they are often discussed together, they serve different, albeit complementary, roles. It‚Äôs crucial to understand the unique functions of Docker and Kubernetes and how they can work together.\n\nPart I: Overview of Docker and Kubernetes\n1.1: What is Docker?\nDocker is an open-source platform that automates the deployment, scaling, and management of applications by encapsulating them into containers. A Docker container packages an application along with all of its dependencies, ensuring that it will run uniformly regardless of the host environment.\n1.2: What is Kubernetes?\nKubernetes (also known as K8s) is an open-source platform for automating the deployment, scaling, and managing of containerized applications. It groups containers into ‚Äúpods‚Äù (the smallest deployable unit in Kubernetes) and manages these pods across multiple host machines.\n\nPart II: Comparing Docker and Kubernetes\n2.1: Scope\nDocker focuses on automating application deployment within containers, ensuring consistency across environments. Kubernetes, on the other hand, manages clusters of containers across multiple machines, handling tasks like load balancing, network traffic distribution, and service discovery.\n2.2: Scalability\nWhile Docker can scale applications, its capacity is limited. Kubernetes excels at managing and scaling large clusters of containers, making it more suitable for large-scale applications.\n2.3: Networking\nIn Docker, each container has its unique IP, with linking allowing containers to communicate. Kubernetes uses a flat network model, allowing all pods to communicate with each other without needing to map ports.\n2.4: Data Storage\nDocker volumes can be used for persistent data storage. Kubernetes, while also supporting Docker volumes, provides its volume abstraction layer, making it compatible with numerous cloud storage options.\n2.5: Load Balancing\nDocker uses a simple DNS-based round-robin method for load balancing. Kubernetes, however, provides more advanced load balancing features, including the distribution of network traffic to maintain stable application performance.\n\nPart III: Docker and Kubernetes Working Together\nDocker and Kubernetes are not mutually exclusive; instead, they complement each other. Docker provides the containerization technology, and Kubernetes provides orchestration capabilities for managing large clusters of Docker containers.\n\nPart IV: When to Use Docker or Kubernetes\n4.1: Use Docker When:\n\nYou are containerising applications and ensuring consistent behavior across environments.\nYour application is relatively simple and does not require advanced orchestration or scaling features.\nYou want a simpler learning curve. Docker is generally easier to learn and implement.\n\n4.2: Use Kubernetes When:\n\nYou need to manage and scale large clusters of containers across multiple machines.\nYour application requires advanced orchestration features, such as load balancing, service discovery, and automated rollouts and rollbacks.\nYou are deploying microservices architecture at a large scale.\n\n\nConclusion:\nUnderstanding Docker and Kubernetes and their respective strengths can help you make informed decisions when deploying and managing your applications. While Docker provides an excellent platform for containerising applications, Kubernetes offers powerful tools for managing and scaling those containers. Choosing the right tool depends on your specific use case, the complexity and scale of your application, and your infrastructure needs."
  },
  {
    "objectID": "Notes/MLOPS0.html",
    "href": "Notes/MLOPS0.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "In today‚Äôs blog, I will discuss a pivotal concept in modern machine learning architectures: MLOps, short for Machine Learning Operations.\n\n\nMLOps is a set of best practices combining Machine Learning (ML), Data Engineering, and DevOps. The goal of MLOps is to streamline and standardize the machine learning lifecycle, from development and deployment to maintenance.\nMLOps aims to address various challenges in operationalizing ML models, such as managing data dependencies, maintaining code quality, ensuring model reproducibility, and monitoring model performance over time.\n\n\n\nLet‚Äôs dive deeper into the various components that constitute MLOps:\n\n\nVersion control is essential for tracking changes to code, data, parameters, and environment configuration. Git is often used for code, but tools like DVC (Data Version Control) can help manage data and model versions.\n\n\n\nTesting ensures the stability and reliability of the code, including model code and infrastructure code. Types of tests include unit tests, integration tests, and end-to-end tests. Additionally, data validation tests are crucial to ensure the quality of the data feeding into the models.\n\n\n\nCI is a practice where developers frequently merge their code changes into a central repository. After each merge, automated builds and tests are run. In the context of MLOps, this includes integration tests of ML code, infrastructure, and data pipelines.\n\n\n\nContinuous Deployment is the practice of automatically deploying the code to production after passing the build and test stages. For ML, this also includes model training, validation, and deployment.\n\n\n\nModel performance needs to be monitored over time to ensure it doesn‚Äôt degrade. Monitoring systems can track model performance metrics and provide alerts for any significant changes. Logging systems keep detailed records of data inputs and outputs, model predictions, and any errors or exceptions.\n\n\n\nEnsuring the same results can be achieved if a past version of the model is re-run with the same data and parameters is key. This includes versioning data, code, and environment, as Ill as tracking all experiments.\n\n\n\n\nSeveral tools and platforms help facilitate MLOps, including:\n\nMLflow: An open-source platform for managing the end-to-end ML lifecycle, including experimentation, reproducibility, and deployment.\nTFX (TensorFlow Extended): A Google-production-scale ML platform that provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor ML systems.\nKubeflow: An open-source project dedicated to making deployments of machine learning workflows on Kubernetes simple, portable, and scalable.\nSeldon: An open-source platform that enables businesses to deploy, scale, and optimize machine learning models in production.\n\n\n\n\nMLOps brings several benefits, including:\n\nEfficiency: By automating many steps in the ML lifecycle.\nReproducibility: By tracking every experiment with its data, code, and parameters.\nCollaboration: By enabling data scientists, data engineers, and DevOps to work together effectively.\nMonitoring: By continuously monitoring model performance and data quality.\nGovernance and regulatory compliance: By maintaining detailed logs of data, model predictions, and model changes.\n\nThe ultimate goal of MLOps is to accelerate the ML lifecycle and make ML models more valuable to the organization. Having a firm grasp of MLOps principles has been invaluable in my professional journey."
  },
  {
    "objectID": "Notes/MLOPS1.html",
    "href": "Notes/MLOPS1.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Following our introductory session on MLOps, today we will delve deeper into each component of the MLOps lifecycle and understand their practical applications.\n\n\nThe quality and consistency of data are critical for the performance of ML models. As such, versioning data is an important practice in MLOps. Tools like DVC can be used for this purpose. They keep track of changes in datasets and models, enabling you to reproduce any version of your experiment.\nAlso, don‚Äôt forget about data validation: tools like TensorFlow Data Validation (TFDV) can help you analyze and validate the consistency of your data over time.\n\n\n\nDuring this stage, data scientists build and train various machine learning models to solve the given problem. Key aspects of this stage are tracking experiments and ensuring reproducibility. An experiment involves a specific version of the code, a set of parameters, a dataset, and produces a model and its metrics. MLflow is a popular tool used to manage these experiments.\nTo ensure reproducibility, you must have version control in place for not just your code (using Git), but also your data and model (using tools like DVC or MLflow). Containerization technologies (like Docker) can also be used to maintain the consistency of the computing environment across different stages of the ML lifecycle.\n\n\n\nIn MLOps, testing isn‚Äôt limited to just the code; it also involves data testing and model validation.\n\nData Tests: These tests ensure that the data is in the correct format, within the expected range, and not corrupted.\nModel Tests: These tests confirm that the model is performing as expected and that its predictions make sense. Model validation techniques, like cross-validation or train/validation/test split, are used to assess the performance of the model.\n\n\n\n\nCI/CD are key DevOps practices that have been adapted for MLOps.\n\nContinuous Integration (CI): This involves regularly merging code changes into a central repository, after which automated builds and tests are run. CI ensures that the code remains in a deployable state and helps to catch bugs early.\nContinuous Deployment (CD): In CD, code changes are automatically deployed to production once they pass the necessary automated tests. In the context of MLOps, CD often involves deploying ML models to a serving infrastructure, which could be a server, a serverless platform, or an edge device.\n\nTools like Jenkins, Travis CI, or GitLab CI/CD are often used for implementing CI/CD pipelines.\n\n\n\nOnce the model is deployed, it‚Äôs important to continuously monitor its performance to ensure that it‚Äôs still providing valuable predictions as new data comes in. This can be achieved using monitoring tools like Prometheus or Grafana. Logging systems like Elasticsearch or Fluentd can help store detailed records of data inputs and outputs, model predictions, and any errors or exceptions.\n\n\n\nIn many industries, there are regulations requiring that models be explainable, fair, and unbiased. Thus, MLOps also involves applying techniques for model interpretability (like SHAP or LIME), and ensuring data privacy (like differential privacy or federated learning). Furthermore, maintaining detailed logs and implementing proper access control mechanisms can help meet regulatory compliance requirements.\nWe‚Äôll continue exploring these components in our hands-on session where we‚Äôll walk through the lifecycle of an ML project using popular MLOps tools."
  },
  {
    "objectID": "Notes/MLOPS2.html",
    "href": "Notes/MLOPS2.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Following our detailed discussion on MLOps components, it‚Äôs now time to put theory into practice. Today‚Äôs session will focus on implementing an end-to-end machine learning project using the MLOps principles we‚Äôve learned.\nWe will use a mix of tools for different stages of the project, including DVC for data versioning, MLflow for experiment tracking, Jenkins for CI/CD, and Prometheus and Grafana for monitoring.\n\n\nFirstly, we‚Äôll set up a collaborative environment with Git for version control. We will also set up DVC to track changes in our data and model files. All project members will clone the Git repository and set up DVC remotes.\n\n\n\nIn this step, we will prepare and explore the dataset. We‚Äôll create scripts for data cleaning and preprocessing, ensuring we annotate them with proper comments for clarity. Once we have a processed dataset ready for ML modeling, we‚Äôll use DVC to track the data file.\n\n\n\nNext, we will start building and training our models. It‚Äôs important to track the hyperparameters, metrics, and models for each experiment. We‚Äôll use MLflow for this purpose. MLflow will allow us to compare different experiments and choose the best model for our problem.\n\n\n\nNow, we will create unit tests for our code, data validation tests for our dataset, and model validation tests for our model. For each code commit, these tests will run automatically in our Jenkins CI/CD pipeline.\n\n\n\nOnce we have a model that performs well and passes all tests, it‚Äôs time to deploy it. We‚Äôll create a Docker image that includes our model and the necessary serving code, and we‚Äôll configure Jenkins to deploy this image automatically to our serving infrastructure.\n\n\n\nAfter deployment, we need to monitor the model‚Äôs performance in real-time. We will set up Prometheus to collect metrics from our model server, and we‚Äôll use Grafana to create a dashboard for viewing these metrics.\n\n\n\nMLOps is not a one-time setup; it‚Äôs a continuous process. We‚Äôll set up alerts to inform us when our model‚Äôs performance drops below a certain threshold. If that happens, or if we receive new data, we‚Äôll iterate on our models: we‚Äôll go back to the experimentation stage, make improvements, and push the changes through our CI/CD pipeline.\nRemember, MLOps is about managing the lifecycle of machine learning projects in a way that promotes collaboration, efficiency, and reliability. The tools and techniques we‚Äôre learning in this course will serve you well in your future data science endeavors.\nIn the next session, we‚Äôll get hands-on with these tools, and you‚Äôll have the opportunity to set up your own MLOps pipelines."
  },
  {
    "objectID": "Notes/MLOPS3.html",
    "href": "Notes/MLOPS3.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Sure, while a full end-to-end example with code would be quite lengthy for this format, I can provide simplified examples for each of the key steps we discussed in the previous tutorial. These examples will use Python and some popular libraries for each task."
  },
  {
    "objectID": "Notes/MLOPS3.html#setting-up-the-environment",
    "href": "Notes/MLOPS3.html#setting-up-the-environment",
    "title": "Dr Juan Beltran",
    "section": "1. Setting up the Environment",
    "text": "1. Setting up the Environment\nYou can initialize a new Git repository and DVC project using the command line:\n# Initialize a Git repository\ngit init\n\n# Initialize a DVC project\ndvc init"
  },
  {
    "objectID": "Notes/MLOPS3.html#data-preparation",
    "href": "Notes/MLOPS3.html#data-preparation",
    "title": "Dr Juan Beltran",
    "section": "2. Data Preparation",
    "text": "2. Data Preparation\nAssume you have a CSV file as your raw data. We‚Äôll use pandas to clean and preprocess the data:\nimport pandas as pd\n\n# Load raw data\ndf = pd.read_csv('raw_data.csv')\n\n# Preprocess the data\ndf_clean = df.dropna()  # Drop missing values\ndf_clean.to_csv('clean_data.csv', index=False)\n\n# Add to DVC\n!dvc add clean_data.csv"
  },
  {
    "objectID": "Notes/MLOPS3.html#experimentation-and-model-building",
    "href": "Notes/MLOPS3.html#experimentation-and-model-building",
    "title": "Dr Juan Beltran",
    "section": "3. Experimentation and Model Building",
    "text": "3. Experimentation and Model Building\nLet‚Äôs use sklearn for building a simple logistic regression model and mlflow for tracking the experiment:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport mlflow\n\n# Load clean data\ndf_clean = pd.read_csv('clean_data.csv')\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df_clean.drop('target', axis=1), df_clean['target'], test_size=0.2)\n\n# Start an MLflow experiment\nwith mlflow.start_run():\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n    \n    # Log metrics\n    mlflow.log_metric(\"accuracy\", model.score(X_test, y_test))"
  },
  {
    "objectID": "Notes/MLOPS3.html#testing",
    "href": "Notes/MLOPS3.html#testing",
    "title": "Dr Juan Beltran",
    "section": "4. Testing",
    "text": "4. Testing\nYou can use pytest to create tests for your data and model:\nimport pytest\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\ndef test_data():\n    df = pd.read_csv('clean_data.csv')\n    assert not df.isnull().any().any(), \"Data contains null values.\"\n\ndef test_model():\n    df = pd.read_csv('clean_data.csv')\n    model = LogisticRegression()\n    model.fit(df.drop('target', axis=1), df['target'])\n    assert model.score(df.drop('target', axis=1), df['target']) > 0.8, \"Model accuracy is too low.\""
  },
  {
    "objectID": "Notes/MLOPS3.html#model-deployment",
    "href": "Notes/MLOPS3.html#model-deployment",
    "title": "Dr Juan Beltran",
    "section": "5. Model Deployment",
    "text": "5. Model Deployment\nFor model deployment, you can save your model to a file and then load it in your serving code:\n# Save model to a file\nimport joblib\njoblib.dump(model, 'model.pkl')\n\n# Load model in serving code\nmodel = joblib.load('model.pkl')\nYou would then include this model file and the serving code in a Docker image."
  },
  {
    "objectID": "Notes/MLOPS3.html#monitoring",
    "href": "Notes/MLOPS3.html#monitoring",
    "title": "Dr Juan Beltran",
    "section": "6. Monitoring",
    "text": "6. Monitoring\nPrometheus and Grafana are often used for monitoring, but they are typically used outside of your Python code, so we‚Äôll skip the code example for this step."
  },
  {
    "objectID": "Notes/MLOPS3.html#maintenance-and-iteration",
    "href": "Notes/MLOPS3.html#maintenance-and-iteration",
    "title": "Dr Juan Beltran",
    "section": "7. Maintenance and Iteration",
    "text": "7. Maintenance and Iteration\nIf the model performance drops, you would go back to the experimentation stage, adjust your model or data, and then rerun your code.\nAlways remember, these are simplified examples and real-world scenarios would involve more complex data preprocessing, model training, testing, and deployment steps."
  },
  {
    "objectID": "Notes/NeuralNetworks.html",
    "href": "Notes/NeuralNetworks.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Neural Networks and Deep Learning Theory\nIntroduction:\nNeural networks and deep learning have revolutionized numerous fields, including image recognition, natural language processing, and autonomous vehicles. This series of tutorial notes will provide a foundational understanding of these topics.\n\nPart I: Introduction to Neural Networks\n1.1: What is a Neural Network?\nA neural network is a computing system inspired by the human brain‚Äôs neural network. It‚Äôs composed of interconnected processing nodes, called neurons or nodes. These nodes are organized in layers: an input layer, one or more hidden layers, and an output layer.\n1.2: The Neuron\nThe basic computational unit of a neural network is the neuron. It receives input from other neurons, processes it, and sends its output to other neurons in the network. Each input is weighted, and these weights get adjusted during the learning process.\n1.3: Activation Functions\nAn activation function defines the output of a neuron given a set of inputs. It transforms the weighted sum of the inputs into the output of the neuron. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh.\n\nPart II: The Learning Process in Neural Networks\n2.1: Forward Propagation\nDuring forward propagation, data flows from the input layer through the hidden layers to the output layer. Each neuron performs a weighted sum of inputs and passes the result through an activation function.\n2.2: Backward Propagation (Backpropagation)\nBackpropagation is a method to adjust the weights of the neurons based on the error of the network‚Äôs output. The network calculates the gradient of the loss function concerning each weight and adjusts the weights to minimize the loss.\n2.3: Gradient Descent and Optimizers\nGradient descent is an optimization algorithm that iteratively moves in the direction of steepest descent to minimize a function, in our case, the loss function. Optimizers like Stochastic Gradient Descent (SGD), Adam, RMSprop use different techniques to speed up this process.\n\nPart III: Introduction to Deep Learning\n3.1: What is Deep Learning?\nDeep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain‚Äôs neural networks. It uses multiple layers of neural networks to model complex patterns in data.\n3.2: Convolutional Neural Networks (CNNs)\nCNNs are a type of deep learning model particularly effective for image recognition tasks. They employ a mathematical operation called convolution to process data from the image pixels.\n3.3: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)\nRNNs are a class of neural networks for processing sequential data. LSTM is a type of RNN that can learn and remember over long sequences and is less likely to suffer from the vanishing gradient problem.\n\nPart IV: Regularization, Overfitting, and Underfitting\n4.1: Understanding Overfitting and Underfitting\nOverfitting occurs when a model learns the training data too well, including noise and outliers, leading to poor generalization to new data. Underfitting is when the model does not learn enough from the training data, resulting in poor performance on both training and test data.\n4.2: Regularization Techniques\nRegularization helps prevent overfitting by adding a penalty to the loss function based on the complexity of the model. Techniques include L1 and L2 regularization, dropout, and early stopping.\n\nPart V: Evaluation Metrics and Model Tuning\n5.1: Loss Functions\nLoss functions quantify how well the model‚Äôs predictions match the true values. Common loss functions include mean\nsquared error for regression tasks and cross-entropy for classification tasks.\n5.2: Evaluation Metrics\nEvaluation metrics help assess model performance. For classification tasks, these include accuracy, precision, recall, and F1-score. For regression tasks, common metrics include mean absolute error and root mean squared error.\n5.3: Hyperparameter Tuning\nHyperparameters are the parameters of the learning process, not learned from the data. Tuning them can significantly improve model performance. Techniques include grid search, random search, and Bayesian optimization.\n\nConclusion:\nNeural networks and deep learning represent a vast and rapidly evolving field. Understanding the foundational theory and principles equips you with the ability to effectively apply these tools, continue learning about new developments, and even contribute to pushing the boundaries of what‚Äôs possible.\nFor extra material go to the tutorial section"
  },
  {
    "objectID": "Notes/Newenvs.html",
    "href": "Notes/Newenvs.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "The Importance of Creating Environments in Python and Best Practices\nIntroduction:\nWhen working on multiple Python projects, it‚Äôs essential to create separate environments to keep dependencies, libraries, and even Python versions isolated from one another. This practice ensures that our projects remain clean, manageable, and free from package conflict. It‚Äôs an industry-standard best practice that all Python developers, including data scientists, should follow.\n\nPart I: Understanding Python Environments\n1.1: What is a Python Environment?\nA Python environment is a context in which you run Python code and includes everything that Python interacts with - the Python interpreter, libraries, and global settings.\n1.2: Why Do We Need Python Environments?\nCreating separate environments helps prevent conflicts between packages and Python versions when working on different projects. Each project can have its own dependencies, regardless of what dependencies other projects have.\n\nPart II: Importance of Creating Python Environments\n2.1: Isolation of Project Dependencies\nBy isolating your project environments, you avoid issues such as package version conflicts and the subsequent ‚Äúdependency hell.‚Äù It ensures that upgrading a package for one project doesn‚Äôt break another.\n2.2: Reproducibility\nEnvironments help ensure that your code runs consistently across different platforms. By specifying the versions of packages used in an environment, you can ensure that your code will run the same way on any machine.\n2.3: Ease of Sharing and Collaboration\nWhen sharing your code with others, it is easier if you also provide the environment used to run it. This way, collaborators can replicate your environment and run your code without having to resolve any dependency issues.\n\nPart III: Tools for Python Environment Management\n3.1: Virtualenv\nVirtualenv is a popular tool that creates isolated Python environments. It allows you to create an environment, install the necessary packages, and then activate and deactivate the environment as needed.\n3.2: Conda\nConda is a package manager that also manages environments. It‚Äôs particularly popular in the data science world because it makes it easy to install packages that are hard to compile from source code, like NumPy or SciPy.\n3.3: Pipenv\nPipenv combines the capabilities of pip and virtualenv into one tool, providing both package management and virtual environment support. It introduces a ‚Äúlock file‚Äù to lock the environment‚Äôs exact dependencies, improving reproducibility.\n3.4: Docker\nDocker isn‚Äôt a Python-specific tool, but it‚Äôs worth mentioning. It encapsulates your application and its environment into a container, ensuring consistency across multiple development and release cycles.\n\nPart IV: Best Practices for Creating Python Environments\n4.1: One Project, One Environment\nCreate a new environment for each project to isolate its dependencies. This practice is essential to avoid conflicts between different project dependencies.\n4.2: Document Dependencies\nAlways keep a record of your project‚Äôs dependencies. Tools like pip can generate a requirements.txt file, and conda can create an environment.yml file.\n4.3: Use Version Control\nVersion control systems like git can help keep track of changes, not only in your code but also in your environment setup. They can help you trace back what changes in your environment might have caused your code to break.\n4.4: Clean Up Regularly\nOld, unused environments can take up space and create clutter. It‚Äôs good practice to remove any environments you‚Äôre no longer using.\n\nConclusion:\nCreating and managing Python environments might seem like an extra step in the development process, but it is crucial for creating stable, reproducible, and conflict-free Python projects. By understanding the importance of Python environments and following the best practices,\nyou can improve your Python development workflow and make your life as a Python developer or data scientist significantly easier."
  },
  {
    "objectID": "Notes/PythonvsR.html",
    "href": "Notes/PythonvsR.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "The Power of R and Python for Data Science\nIntroduction:\nOne of the most common discussions in the data science world revolves around the choice between Python and R. Both languages have unique strengths and they are powerful tools when used effectively. A proficient data scientist can and often does, use both in their toolkit. These languages are not mutually exclusive, and each language can be the perfect tool for specific tasks. It‚Äôs all about understanding their unique strengths and leveraging them accordingly.\n\nPart I: Understanding Python and R\n1.1: Overview of Python\nPython is a general-purpose language noted for its simplicity and readability. This makes it a fantastic choice for beginners in programming and data science. Python‚Äôs robustness comes from its extensive libraries and packages, which cover almost every aspect of data science.\n1.2: Overview of R\nR is a statistical programming language that was specifically designed for data analysis, making it a go-to language for statisticians and researchers. It‚Äôs well-respected for its comprehensive statistical and graphical capabilities. R also has a wealth of packages for specialized scientific computation tasks.\n\nPart II: Strengths of Python in Data Science\n2.1: Machine Learning\nPython‚Äôs major strength lies in machine learning. Libraries like scikit-learn, TensorFlow, and PyTorch offer tools for predictive modeling, neural networks, natural language processing, and more.\n2.2: General Programming & Scripting\nPython shines in general-purpose programming tasks. This makes it perfect for building data pipelines, web scraping, automation, web development, and more.\n2.3: Community & Learning Resources\nPython boasts a larger user community than R, leading to more resources for learning and troubleshooting. Websites like Stack Overflow have a massive amount of content related to Python, making it easier for new data scientists to find help.\n\nPart III: Strengths of R in Data Science\n3.1: Statistical Analysis\nR is unparalleled in its statistical analysis capabilities. It has a wide range of in-built functions for testing statistical hypotheses and conducting complex data analyses.\n3.2: Data Visualization\nAlthough Python has Matplotlib, Seaborn, and Plotly, R‚Äôs ggplot2 package is considered one of the most sophisticated data visualization tools. It has a high level of flexibility and enables detailed layering and thematic customization.\n3.3: Reporting and Reproducible Research\nWith tools like R Markdown, Shiny, and Knitr, R excels at creating reports and interactive web applications, allowing others to reproduce your analysis with the original data and code.\n\nPart IV: Python vs R: A Comparative Summary\n\nEase of Learning: Python‚Äôs syntax is straightforward, making it easier for beginners to learn. However, R has a steeper learning curve but provides more statistical power.\nData Handling Capabilities: Python is preferred for large datasets and big data analysis due to its speed and efficiency, while R is better suited for dataset manipulation and statistical modeling.\nVisualization: Python has several good visualization libraries, but many data scientists agree that R‚Äôs ggplot2 offers superior control and complexity.\nMachine Learning: Python has a better machine learning ecosystem, which includes libraries like TensorFlow and PyTorch. R also has machine learning libraries like caret and mlr, but they are less developed compared to Python‚Äôs ecosystem.\nCommunity Support: Python has a wider community, resulting in faster package development and troubleshooting assistance. However, R has strong support in academia and research-oriented industries.\nJob Market: Python is generally more in demand in the industry. However, R is favored in specific sectors like biostatistics, bioinformatics, and academic research.\n\n\nPart V: The Convergence - Python and R in Data Science\nOne shouldn‚Äôt have to choose between Python and R; instead, the focus should be on learning to use both effectively. Many professionals use both languages in their work - Python for data manipulation and machine learning, and R for data analysis and visualization.\n5.1: Tools for Interoperability\nTools like Jupyter notebooks, Rpy2, and reticulate make it possible to use both languages interchangeably in the same project.\n5.2: Building a Polyglot Data Science Toolkit\nData scientists can and should develop a toolkit that takes advantage of the strengths of both languages. For example, you might use Python‚Äôs scikit-learn for machine learning, R‚Äôs ggplot2 for advanced visualizations, and Python‚Äôs pandas for data manipulation.\n\nConclusion:\nThe ‚ÄúPython vs.¬†R‚Äù debate is less about choosing one over the other and more about understanding the strengths of each language and using them to your advantage. Both languages have a significant role to play in the data science landscape and knowing when to use each one is a skill every data scientist should cultivate.\nRemember, the best tool for the job often depends on the specific task, the industry you‚Äôre in, and your team‚Äôs capabilities and preferences. Always choose the right tool for the task and keep learning and adapting. After all, data science is a field that‚Äôs always evolving."
  },
  {
    "objectID": "Notes/timeseries.html",
    "href": "Notes/timeseries.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Principles of Time Series Analysis ‚Äì From Traditional Methods to Neural Networks (with Examples in Python and R)\nIntroduction:\nTime series analysis is a statistical approach to analyze time-ordered data. It allows us to extract meaningful statistics and identify patterns to forecast future data points. We‚Äôll start with traditional methods and move towards advanced techniques like neural networks, providing examples in Python and R.\n\nPart I: Traditional Time Series Analysis Methods\n1.1: Autoregressive (AR) Models\nAutoregressive models forecast future values based on past values. It assumes that past values have a linear influence on future values.\n# Python example using statsmodels\nfrom statsmodels.tsa.ar_model import AutoReg\nmodel = AutoReg(data, lags=1)\nmodel_fit = model.fit()\n# R example using forecast package\nlibrary(forecast)\nfit <- auto.arima(data)\n1.2: Moving Average (MA) Models\nMA models use past forecast errors in a regression-like model. The dependence between an observation and an error in a previous time step is accounted for by this model.\n# Python example using statsmodels\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(data, order=(0, 0, 1))\nmodel_fit = model.fit()\n# R example using forecast package\nlibrary(forecast)\nfit <- auto.arima(data)\n1.3: Autoregressive Integrated Moving Average (ARIMA) Models\nARIMA combines autoregressive, differencing, and moving average elements. It‚Äôs a versatile model that can model a range of time series patterns.\n# Python example using statsmodels\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(data, order=(1, 1, 1))\nmodel_fit = model.fit()\n# R example using forecast package\nlibrary(forecast)\nfit <- auto.arima(data)\n\nPart II: Advanced Time Series Analysis Methods\n2.1: Recurrent Neural Networks (RNN)\nRNNs can use their internal state (memory) to process sequences of inputs, which makes them ideal for time series forecasting.\n# Python example using Keras\nfrom keras.models import Sequential\nfrom keras.layers import SimpleRNN\n\nmodel = Sequential()\nmodel.add(SimpleRNN(units=50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n2.2: Long Short-Term Memory (LSTM)\nLSTM is a type of RNN that can learn long-term dependencies. They‚Äôre widely used for sequence prediction problems and have proven to be particularly effective.\n# Python example using Keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nPart III: Evaluating Time Series Models\nRegardless of the type of model, it‚Äôs critical to evaluate its performance. Two common metrics for time series models are the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE).\n# Python example using sklearn\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\ny_pred = model.predict(test_data)\nMAE = mean_absolute_error(test_data, y_pred)\nRMSE = sqrt(mean_squared_error(test_data, y_pred))\n\n# R example using Metrics package\nlibrary(Metrics)\ny_pred <- forecast(fit, h=length(test_data))\nMAE = mae(test_data, y_pred$mean)\nRMSE = rmse(test_data, y_pred$mean)\n\nConclusion:\nTime series analysis is a complex field with a variety of methods ranging from traditional statistical models to advanced neural networks. The choice of method depends on the nature of the problem, the availability of data, and the level of accuracy required. Understanding the underlying principles of these methods is crucial in selecting the appropriate model and accurately interpreting its results."
  },
  {
    "objectID": "Notestrading.html",
    "href": "Notestrading.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Blog Post 1: Embracing the Power of Data Science in Understanding Physical Commodities\nPhysical commodities are the lifeblood of our industry, powering everything from vehicles to factories. But how can we leverage data science to better understand these commodities and increase our profitability?\nData science can help us analyze market trends, predict price movements, and optimize our trading strategies. For instance, machine learning algorithms can analyze historical price data to predict future price movements, helping us make more informed trading decisions. Additionally, natural language processing can be used to analyze news articles and social media posts to gauge market sentiment, which can also influence commodity prices.\nBlog Post 2: Leveraging Data Science in the Evolution of Commodity Trading\nThe commodity trading industry has evolved significantly over the centuries, from the ancient barter system to the modern electronic trading platforms. As we navigate this ever-changing landscape, data science has emerged as a powerful tool to help us stay ahead of the curve.\nBy analyzing historical trading data, we can identify patterns and trends that can inform our trading strategies. For example, we can use time series analysis to predict future price movements based on past trends. We can also use machine learning algorithms to create predictive models that can help us anticipate market changes and adjust our strategies accordingly.\nBlog Post 3: Optimizing the Global Supply Chain with Data Science\nManaging the global supply chain for commodities is a complex task, involving sourcing, transportation, storage, and delivery. Data science can help us optimize this process, reducing costs and increasing efficiency.\nFor instance, we can use predictive analytics to forecast demand and optimize our inventory levels, reducing storage costs. We can also use machine learning algorithms to optimize our transportation routes, reducing transportation costs and delivery times. Additionally, data science can help us identify potential supply chain disruptions and develop contingency plans, ensuring a steady supply of commodities.\nBlog Post 4: Enhancing the Role of Commodity Traders with Data Science\nCommodity traders play a crucial role in connecting producers and consumers. But how can data science enhance this role and increase our profitability?\nData science can help us better understand our customers‚Äô needs and preferences, allowing us to tailor our offerings and improve customer satisfaction. For instance, we can use customer segmentation algorithms to group our customers based on their buying behavior, and then tailor our marketing and sales strategies to each group. We can also use predictive analytics to forecast future demand, helping us plan our inventory and pricing strategies.\nBlog Post 5: Sourcing Commodities More Efficiently with Data Science\nSourcing commodities from producers is a critical part of our business. Data science can help us do this more efficiently, reducing costs and increasing profitability.\nFor instance, we can use predictive analytics to forecast production levels, helping us plan our sourcing strategies. We can also use machine learning algorithms to analyze production data and identify potential issues, allowing us to address them before they impact our supply.\nBlog Post 6: Transforming the Transportation of Commodities with Data Science\nTransporting commodities from the source to the market is a complex and costly process. But with data science, we can transform this process and increase our profitability.\nWe can use machine learning algorithms to optimize our transportation routes, reducing costs and delivery times. We can also use predictive analytics to forecast transportation demand and plan our capacity accordingly. Additionally, data science can help us monitor our transportation operations in real-time, allowing us to quickly address any issues and ensure a smooth delivery process.\nBlog Post 7: Storing Commodities More Efficiently with Data Science\nStoring commodities until they are needed is a crucial part of our business. But how can data science help us do this more efficiently and increase our profitability?\nWe can\nuse predictive analytics to forecast demand and optimize our inventory levels, reducing storage costs. We can also use machine learning algorithms to analyze storage data and identify potential issues, allowing us to address them before they impact our supply. Furthermore, data science can help us optimize the location of our storage facilities, reducing transportation costs and delivery times.\nBlog Post 8: Enhancing the Blending of Commodities with Data Science\nBlending commodities to meet customer specifications is a complex process that requires precision and expertise. Data science can enhance this process, improving product quality and increasing profitability.\nMachine learning algorithms can analyze blending data to optimize the blending process, improving product quality and reducing waste. Predictive analytics can also forecast the demand for different blends, helping us plan our blending operations and manage our inventory.\nBlog Post 9: Delivering Commodities More Efficiently with Data Science\nDelivering commodities to end-users is the final step in our supply chain. Data science can help us do this more efficiently, improving customer satisfaction and increasing profitability.\nWe can use machine learning algorithms to optimize our delivery routes, reducing delivery times and improving customer satisfaction. Predictive analytics can also forecast delivery demand, helping us plan our delivery capacity and manage our inventory. Furthermore, data science can help us monitor our delivery operations in real-time, allowing us to quickly address any issues and ensure a smooth delivery process.\nBlog Post 10: Managing Risk in Commodity Trading with Data Science\nRisk management is a crucial part of commodity trading. Data science can help us manage these risks more effectively, protecting our profitability.\nWe can use machine learning algorithms to analyze trading data and identify potential risks, allowing us to take proactive measures to mitigate these risks. Predictive analytics can also forecast market movements, helping us adjust our trading strategies to minimize risk. Furthermore, data science can help us monitor our risk exposure in real-time, allowing us to quickly respond to any changes in the market.\nBlog Post 11: Funding Commodity Trading More Effectively with Data Science\nFunding commodity trading is a complex process that requires careful planning and management. Data science can help us do this more effectively, improving our financial performance and increasing profitability.\nWe can use predictive analytics to forecast our funding needs, helping us plan our financial strategies and manage our cash flow. Machine learning algorithms can also analyze financial data to identify potential issues, allowing us to address them before they impact our financial performance. Furthermore, data science can help us monitor our financial performance in real-time, allowing us to quickly respond to any changes in the market."
  },
  {
    "objectID": "products.html#selected-projects-over-the-last-2-years",
    "href": "products.html#selected-projects-over-the-last-2-years",
    "title": "Projects",
    "section": "Selected projects over the last 2 years",
    "text": "Selected projects over the last 2 years\n\nPredicting Market Direction for Cocoa Futures\nBased on supply and demand data as well as key weather events I built time series models in order to predict the direction of price changes in futures contracts for cocoa in London and New York. I used prophet library and Keras, using recurrent neural networks. This model is used by traders in ECOM on a daily basis.\n\n\nPredicting Cocoa Production\nIn this project I used climatic variables to predict the yield of two crop seasons for cocoa in C√¥te d‚ÄôIvore. The model was a series of ensemble models that weighted the anomalies for each week/month and adjusted the forecast. The predictions have been tested over the last 2 years (4 seasons) with accurate results, even with models with early information.\n\n\nPredicting loss of suitability of coffee and cocoa due to climate change\nClimate change is having a major impact on the production of cocoa and coffee around the world. I used several models (Random Forest, Neural Networks and XGBoost) to model the current suitability of cocoa and coffee using several climatic variables. Once the model was calibrated on current conditions, the predictions were applied to future conditions to understand either the expansion or contraction of the areas of production that are suitable today. The results of these models has helped to develop the priorities to mitigate dramatic changes in the tropics.\n\n\nAmex Default Prediction\nI used this data to train junior peers to show them how to use different classification methods to extract value of large datasets. The data came from a Kaggle competition. I used gradient boosting decision trees. Specifically, I trained and evaluated the LGBMClassifier, XGBClassifier and the CatBoostClassifier models with gold standard results in the context of the competition to successfully identify whether a customer would default on their repayments of credit."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications and Awards",
    "section": "",
    "text": "2020: Beltr√°n JD and Puddifoot R: ECOM research into global warming and its effect on suitability for cocoa production. ‚ÄôAGEFI Commodities, special edition, Disruption in a Disrupted World. April: 22\n2017: Yang X, Hu R, Yin H, Jenkins J, Shu S, Tang H, Liu D, Weighill DA, Ha J, Heyduk K, Goodstein DM, Guo H, Moseley RC, Fitzek E, Jawdy S, Zhang Z, Xie M, Hartwell J, Grimwood J, Abraham PE, Mewalal R, Yim WC, Beltr√°n JD, Boxall SF, Dever LV, Palla KJ, Albion R, Garcia T, Mayer J, Lim SD, Wai CM, Van Buren R, De Paoli HC, Borland AM, Guo H, Chen J, Muchero W, Yin Y, Jacobson DA, Tschaplinski TJ, Hettich RL, Ming R , Winter K , Leebens-Mack JH, Smith JAC, Cushman J, Schmutz J, Tuskan GA.: Kalancho√´ genome reveals convergent evolution of crassulacean acid metabolism. Nature Communications 8:¬†1899\n2015: Yang X, Cushman JC, Borland AM, Edwards EJ, Wullschleger SD, Tuskan GA, Owen NA, Griffiths H, Smith JAC, De Paoli HC, Weston DJ, Cottingham R, Hartwell J, Davis SC, Silvera K, Ming R, Schlauch K, Abraham P, Stewart JR, Guo H, Albion R, Ha J, Lim SD, Wone BWM, Yim WC, Garcia T, Mayer JA, Petereit J, Nair SS, Casey E, Hettich RL, Ceusters J, Ranjan P, Palla KP, Yin H, Reyes-Garc√≠a C, Andrade JL, Freschi L, Beltr√°n JD, Dever LV, Boxall SF,Waller J, Davies J, Bupphada P, Kadu N, Winter K, Sage RF, Aguilar CN, Schmutz J, Jenkins J, Holtum JAM. A roadmap for research on crassulacean acid metabolism (CAM) to enhance sustainable food and bioenergy production in a hotter, drier world. New Phytologist 207:491‚Äì504.\n2014: Poster presentation, ‚ÄúEvolutionary origins and ecophysiology of CAM photosynthesis in the montane genus Puya (Bromeliaceae)‚Äù, 34 th New Phytologist Symposium ‚ÄúPlant Systems Biology and Ecology of CAM plants‚Äù, Lake Tahoe, Tahoe City, CA, USA, 15‚Äì18 July 2014.\n2013: Beltr√°n JD., Lasso E., Madri√±√°n S., Virgo A., Winter K.: Juvenile tank-bromeliads lacking tanks: Do they engage in CAM photosynthesis?, Photosynthetica 51: 55‚Äì62.\n2012: M.Sc. Thesis: CAM or not CAM: A study on juveniles of Guzmania lingulata, Guzmania monostachia and Werauhia sanguinolenta (Bromeliaceae), Universidad de los Andes.\n2010: B.Sc. Thesis: Evolution of xerophyte habit in the tribe Tillandsieae (Bromeliaceae): a phylogenetic approach, Universidad de los Andes."
  },
  {
    "objectID": "publications.html#awards",
    "href": "publications.html#awards",
    "title": "Publications and Awards",
    "section": "Awards",
    "text": "Awards\n‚Ä¢ Weidenfeld Scholarship and Leadership Program for D.Phil. Funding, University of Oxford (2013-2016) ‚Ä¢ Scholarship from Administrative Department of Science, Technology and Innovation (Colombia) for D.Phil. Funding, University of Oxford (2013-2017) ‚Ä¢ Winner of New Phytologist Poster Prize (1st place), 34th New Phytologist Symposium ‚ÄúPlant Systems Biology and Ecology of CAM plants‚Äù, California, U.S.A. (2014)"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "Programming Python\nAdvanced proficiency in Pandas, GeoPandas, SciKit-Learn, TensorFlow, Pytorch, keras, Matplotlib, NumPy, SciPy, SQLAlchemy, Seaborn, Bokeh, SciKit-Image\n\n\nProgramming R\nAdvanced proficiency in RStudio, Shiny, tidyverse, Parsnip, bayestestR, sparklyr, keras, dygraph, forecast\n\n\nModelling\nAdvanced experience in statistical modelling, time series analysis, supervised and unsupervised machine learning algorithms, currently implementing solutions in reinforcement learning (intermediate)\n\n\nGeospatial Analysis\nAdvanced skills in geospatial machine learning, professional proficiency in ArcGIS, QGIS and OpenGeos (Python)\n\n\nBig Data & Databases\nProficient in SQL, MongoDB (NoSQL), Spark (pyspark and sparklyr)\n\n\nExperimental Design\nStrong background in designing and implementing A/B tests, and Bayesian statistics\n\n\nLanguages\nFluent in English and Spanish (native)"
  },
  {
    "objectID": "Tutorials/advSQL.html",
    "href": "Tutorials/advSQL.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Advanced SQL\nIntroduction:\nBuilding on our SQL basics and intermediate knowledge, this tutorial, is the second part of SQL 101, here we will delve deeper into advanced SQL techniques, which can help in managing and manipulating complex relational databases.\nPart IV: SQL Constraints\n4.1 Understanding SQL Constraints\nSQL Constraints are used to specify rules for the data in a table. Constraints ensure the reliability and accuracy of the data in the table.\n4.2 Primary Key\nA primary key is used to uniquely identify each record in a table.\nCREATE TABLE Employees (\n    ID int PRIMARY KEY,\n    Name text,\n    Age int,\n    Salary real\n);\nThe PRIMARY KEY constraint uniquely identifies each record in the Employees table. This means that the ID field is unique for each record and cannot be NULL.\n4.3 Foreign Key\nA foreign key is used to link two tables together.\nCREATE TABLE Orders (\n    OrderID int PRIMARY KEY,\n    OrderNumber int,\n    EmployeeID int,\n    FOREIGN KEY (EmployeeID) REFERENCES Employees(ID)\n);\nHere, EmployeeID in the Orders table is a foreign key that refers to ID in the Employees table.\n4.4 Unique\nThe UNIQUE constraint ensures that all values in a column are different.\nCREATE TABLE Employees (\n    ID int PRIMARY KEY,\n    Name text UNIQUE,\n    Age int,\n    Salary real\n);\nThis ensures that the Name is unique for each record.\n\nPart V: Advanced SQL Queries\n5.1 Using Alias\nSQL aliases are used to give a table or a column a temporary name. They make column names more readable.\nSELECT e.Name AS Employee, e.Salary AS Income \nFROM Employees AS e;\nThis command selects all Name and Salary from the Employees table but results will be displayed as Employee and Income respectively.\n5.2 Conditional Selection\nThe CASE statement goes through conditions and returns a value when the first condition is met.\nSELECT Name, Salary,\nCASE \n    WHEN Salary > 50000 THEN 'Above Average'\n    ELSE 'Below Average' \nEND AS SalaryStatus\nFROM Employees;\nThis command adds a new column SalaryStatus that categorizes employees based on their salary.\n5.3 IN Operator\nThe IN operator allows you to specify multiple values in a WHERE clause.\nSELECT * FROM Employees\nWHERE Name IN ('John Doe', 'Jane Doe');\nThis command selects all fields from Employees where the Name is either ‚ÄòJohn Doe‚Äô or ‚ÄòJane Doe‚Äô.\n\nPart VI: Stored Procedures\n6.1 What is a Stored Procedure?\nA stored procedure is a prepared SQL code that you can save so the code can be reused over and over again.\n6.2 Creating a Stored Procedure\nLet‚Äôs assume we frequently need to fetch all employees with a salary above a certain value. Instead of writing this query every time, we can create a stored procedure.\nCREATE PROCEDURE SelectHighlyPaid @Salary real\nAS\nSELECT * FROM Employees WHERE Salary > @Salary;\nGO;\n6.3 Calling a Stored Procedure\nNow that we have our stored procedure, we can execute it.\nEXEC SelectHighlyPaid 50000;\nThis command calls the SelectHighlyPaid stored procedure and retrieves employees earning more than 50000.\n\nConclusion\nFrom basic commands to advanced procedures, SQL is a versatile tool that can handle complex queries with ease. By mastering these advanced concepts and continually practicing, you‚Äôre well on your way to becoming an SQL expert!"
  },
  {
    "objectID": "Tutorials/Forecasting.html",
    "href": "Tutorials/Forecasting.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "When it comes to predicting future trends, events, or quantities‚Äîknown as forecasting‚Äîtime series methods are some of the most commonly used approaches. By definition, a time series is a sequence of data points recorded at successive, equally spaced intervals. Given the unique traits of time series data, specialized analysis techniques have been developed over time, and these methods can be used to forecast future data points.\nIn this blog post, we will look at how to apply time series methods to forecast data using two popular programming languages: R and Python.\n\n\nBefore delving into the coding aspect, it‚Äôs important to understand the key components of a time series:\n\nTrend: The underlying pattern of growth or decline in a time series.\nSeasonality: Regular and predictable changes that recur every calendar year.\nCyclical components: Fluctuations happening due to economic cycles.\nIrregular (or residual) component: The random variation in the series.\n\n\n\n\nR provides robust support for time series analysis via several specialized packages like forecast, tseries, and tsibble.\nIn this example, we will use the forecast package to analyze the AirPassengers dataset.\n\n# Loading required library\n\nlibrary(forecast)\n\n# Load the dataset\n\ndata(AirPassengers)\n\n# Plotting the data\n\nplot(AirPassengers)\n\n# Decomposing the time series\n\ndecomposed_ap <- decompose(AirPassengers)\n\n# Plotting the decomposed time series\n\nplot(decomposed_ap)\n\n# Applying the ARIMA model\n\narima_model <- auto.arima(AirPassengers)\n\n# Forecasting for next 24 months\n\nforecast_arima <- forecast(arima_model, h=24)\n\n# Plotting the forecast\n\nplot(forecast_arima)\nIn this example, we first decomposed the time series to understand its underlying components. We then fitted an ARIMA model using the auto.arima() function, and used it to forecast the next 24 months.\n\n\n\nPython also has strong support for time series analysis, particularly through the pandas and statsmodels libraries.\nLet‚Äôs analyze the same AirPassengers dataset in Python.\n\n# Importing required libraries\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nimport matplotlib.pyplot as plt\n\n# Load the dataset\n\nair_passengers = pd.read_csv('AirPassengers.csv', parse_dates=['Month'], index_col='Month')\n\n# Plotting the data\n\nair_passengers.plot()\n\n# Decomposing the time series\n\ndecomposition = seasonal_decompose(air_passengers)\n\n# Plotting the decomposed time series\n\ndecomposition.plot()\n\nplt.show()\n\n# Applying the ARIMA model\n\nmodel = ARIMA(air_passengers, order=(5,1,0))\n\nmodel_fit = model.fit(disp=0)\n\n# Forecasting for next 24 months\n\nforecast, stderr, conf_int = model_fit.forecast(steps=24)\n\n# Plotting the forecast\n\nplt.plot(forecast)\n\nplt.fill_between(range(len(forecast)), forecast - stderr, forecast + stderr, color='r', alpha=.1)\n\nplt.show()\nHere, we used the seasonal_decompose() function from statsmodels to decompose the time series, fitted an ARIMA model, and made a forecast for the next 24 months.\n\n\n\nIn summary, time series forecasting is a powerful tool for\npredicting future trends and events. Both R and Python provide excellent support for time series analysis and can be chosen based on the specific requirements of your project and your familiarity with the language. Happy forecasting!"
  },
  {
    "objectID": "Tutorials/GoogleEarth.html",
    "href": "Tutorials/GoogleEarth.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Introduction to Google Earth Engine\nIntroduction:\nGoogle Earth Engine is a cloud-based platform for planetary-scale environmental data analysis. It combines a multi-petabyte catalog of satellite imagery and geospatial datasets with computational capabilities to analyze and extract insights. In this blog, we will explore how to use Google Earth Engine, access its data, what types of analysis are possible, and how to produce composite images and measure deforestation.\n\nPart I: Getting Started with Google Earth Engine\n1.1: Setting Up\nTo begin, you will need to sign up for a Google Earth Engine account. Once approved, you can access the Code Editor at https://code.earthengine.google.com/.\n1.2: Navigating the Interface\nThe Google Earth Engine Code Editor is divided into several panels:\n\nScripts panel: where you can create, save, and manage your scripts.\nCode Editor panel: where you enter JavaScript code.\nMap panel: where results are displayed.\nInspector panel: to query map results.\n\n\nPart II: Accessing Data\n2.1: Image and ImageCollection\nIn Google Earth Engine, raster data are represented as Image objects. A collection of images is an ImageCollection. You can access satellite imagery and other raster datasets from the Earth Engine Data Catalog.\n2.2: Example - Accessing Landsat Imagery\nvar image = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20140318');\nMap.addLayer(image, {bands: ['B4', 'B3', 'B2'], max: 0.3}, 'true color image');\n\nPart III: Types of Analysis\n3.1: Basic Operations\nYou can perform arithmetic operations on images, compute spectral indices, and extract statistics.\n3.2: Example - NDVI Calculation\nvar ndvi = image.normalizedDifference(['B5', 'B4']);\nMap.addLayer(ndvi, {min: 0, max: 1, palette: ['white', 'green']}, 'NDVI');\n\nPart IV: Creating Composite Images\nCreating a composite image involves combining several images into one. In the context of Google Earth Engine, you can create a composite image from an ImageCollection.\n4.1: Example - Median Composite\nvar collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_TOA')\n    .filterDate('2019-01-01', '2019-12-31');\n\nvar composite = collection.median();\n\nMap.addLayer(composite, {bands: ['B4', 'B3', 'B2'], max: 0.3}, 'composite image');\n\nPart V: Measuring Deforestation with Google Earth Engine\nGoogle Earth Engine can be used to monitor and measure deforestation. You can compute the difference between two images or track changes over time.\n5.1: Example - Difference in NDVI\nvar image1 = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20140101');\nvar image2 = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20141231');\n\nvar ndvi1 = image1.normalizedDifference(['B5', 'B4']);\nvar ndvi2 = image2.normalizedDifference(['B5', 'B4']);\n\nvar difference = ndvi2.subtract(ndvi1);\nMap.addLayer(difference, {min: -1, max: 1, palette: ['blue', 'white', 'green\n\n']}, 'NDVI difference');\n\nConclusion:\nGoogle Earth Engine is a powerful platform for large-scale geospatial data analysis. It provides an accessible interface for both accessing vast amounts of data and applying complex analyses to generate meaningful insights. As we‚Äôve seen in the blog, it can be used to monitor and measure environmental phenomena such as deforestation. While we‚Äôve only scratched the surface, I encourage you to explore the platform further, as the range of applications is vast."
  },
  {
    "objectID": "Tutorials/ImageClassification.html",
    "href": "Tutorials/ImageClassification.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Image Classification with Deep Learning\nIntroduction:\nImage Classification is a key application of Deep Learning, with uses ranging from medical imaging to autonomous driving. In this tutorial, we‚Äôll explore Convolutional Neural Networks (CNNs), a class of models that has proven to be highly effective for image classification tasks. We will also discuss how to implement them in Python and explore ways to optimize them.\n\nPart I: Understanding Image Classification with Deep Learning\n1.1: What is Image Classification?\nImage Classification is a process of assigning a class label to an image from a predefined set. This is often achieved by training a model on a large number of images with known class labels, then applying this model to classify new images.\n1.2: Convolutional Neural Networks (CNNs)\nCNNs are specifically designed to process grid-like data, where there‚Äôs a spatial relationship between the pixels. This makes them ideal for image processing tasks.\n\nPart II: Implementing CNNs in Python for Image Classification\nWe will use the Keras library to build a simple CNN for image classification. Here‚Äôs an example of a simple CNN architecture:\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Initialize the CNN\nmodel = Sequential()\n\n# Add a Convolutional layer\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n\n# Add a Pooling layer\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten\nmodel.add(Flatten())\n\n# Full connection\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32)\nCode Explanation:\n\nWe first import the necessary libraries and initialize a Sequential model.\nWe then add a Conv2D layer. This is the convolutional layer that will extract features from the input images by sliding a convolution filter over the input to produce a feature map. Here we choose 32 filters and each filter is of size 3 by 3.\nNext, we add a MaxPooling2D layer. This is a pooling layer with a 2x2 filter that reduces the dimensions of the feature maps and helps to make the model invariant to small translations.\nWe then flatten the 2D arrays into a 1D array to feed into a Dense layer (also called a fully connected layer).\nTwo Dense layers are added. The first has 128 units, and the second has 1 unit, which is the output layer (as this is a binary classification problem).\nThe model is then compiled with the ‚Äòadam‚Äô optimizer, ‚Äòbinary_crossentropy‚Äô as the loss function, and ‚Äòaccuracy‚Äô as the evaluation metric.\nFinally, we fit the model to our training data.\n\n\nPart III: Optimizing CNNs\nOptimizing a CNN can involve adjusting the architecture (number and types of layers, number of nodes in layers), tuning the hyperparameters, or using techniques such as data augmentation or transfer learning.\n3.1: Adjusting the Architecture\nFor example, we can add more convolutional layers or fully connected layers, change the size of the filters, or add Dropout layers to prevent overfitting.\n3.2: Hyperparameter Tuning\nThis could involve adjusting the learning rate, batch size, number of epochs, or parameters related to specific layers (like the number\nof filters in a Conv2D layer).\n3.3: Data Augmentation and Transfer Learning\nData Augmentation involves creating new training examples by applying random jitters and transformations to the images, which can improve performance and reduce overfitting.\nTransfer Learning involves taking a pre-trained model (usually trained on a large dataset like ImageNet) and fine-tuning it on your specific task. This can often yield excellent results with less data and compute resources.\n\nConclusion:\nImage Classification with CNNs is a key aspect of Deep Learning, with a wide range of applications. Understanding how these models work, how to implement them in Python, and how to optimize them are crucial skills for anyone working with image data. Always consider the complexity of your specific problem and the resources at your disposal when selecting a model and optimization strategy."
  },
  {
    "objectID": "Tutorials/NeuralNetworks.html",
    "href": "Tutorials/NeuralNetworks.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Deep Learning for Time Series Analysis\nIntroduction:\nDeep Learning provides powerful tools for time series analysis. Models like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Convolutional Neural Networks (CNNs) have proven to be effective for tasks such as time series forecasting. In this tutorial, we‚Äôll explore these models, how they can be implemented in Python, and some ways to optimize them.\n\nPart I: Understanding Time Series Analysis with Deep Learning\n1.1: What is Time Series Analysis?\nTime Series Analysis involves studying ordered data points collected over time to identify patterns, trends, cycles, and forecast future data points.\n1.2: Deep Learning for Time Series Analysis\nDeep Learning models have shown significant success in Time Series Analysis due to their ability to learn complex temporal dependencies, handle large amounts of data, and automatically extract features.\n\nPart II: Deep Learning Models for Time Series Analysis\n2.1: Recurrent Neural Networks (RNNs)\nRNNs are designed to use their internal state (memory) to process sequences of inputs, which makes them ideal for time series forecasting.\n2.2: Long Short-Term Memory (LSTM)\nLSTMs are an extension of RNNs and were designed to remember long-term dependencies in sequence data.\n2.3: Convolutional Neural Networks (CNNs)\nThough traditionally used for image processing, CNNs can also be used for time series forecasting. They can learn to automatically extract spatial and temporal features from raw input data.\n\nPart III: Implementing Deep Learning Models in Python\nUsing the Keras library, we can easily implement these models. Here‚Äôs an example of an LSTM:\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\n# Define model architecture\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Fit the model\nmodel.fit(X, y, epochs=200, verbose=0)\n\nPart IV: Optimizing Deep Learning Models\nOptimization of Deep Learning models for time series analysis is typically done via hyperparameter tuning and architecture design.\n4.1: Hyperparameter Tuning\nJust like in previous algorithms we have discussed, we can adjust parameters such as the learning rate, number of hidden units, number of layers, and many others to optimize the model‚Äôs performance.\n4.2: Architecture Design\nThe design of the model‚Äôs architecture can also have a significant impact on performance. For instance, one could decide to stack multiple LSTM layers or use a combination of CNN and LSTM layers.\n\nConclusion:\nDeep Learning provides a suite of powerful tools for time series analysis. Understanding these models, their application to time series data, and how to implement and optimize them in Python are key skills in the modern data scientist‚Äôs toolbox. Always remember, though, that these are complex models, and simpler methods may often provide equally effective solutions. Always consider the complexity and requirements of your specific problem when selecting a model."
  },
  {
    "objectID": "Tutorials/Regression.html",
    "href": "Tutorials/Regression.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Tutorial: Regression Models in Python ‚Äì From Basics to Advanced\nIntroduction\nRegression analysis is a powerful statistical method that allows you to examine the relationship between two or more variables. In this tutorial, we‚Äôll explore several types of regression models using Python, including Linear Regression, Lasso, Ridge, Elastic Net, Bayesian Regression, and Decision Trees.\nWe‚Äôll be using real-world data from the Our World in Data repository.\n\nPart I: Setting Up Your Environment\nTo get started, we first need to install some necessary libraries. If you don‚Äôt have them installed already, you can do so by running the following commands:\n!pip install pandas numpy sklearn matplotlib seaborn\npandas and numpy will help us handle the data, sklearn will provide the regression models, and matplotlib and seaborn will help us visualize the data.\n\nPart II: Importing the Data\nWe‚Äôre going to use the Our World in Data‚Äôs CO2 and GDP dataset. This dataset includes information about countries‚Äô CO2 emissions and their GDP.\nFirst, let‚Äôs import our libraries and load our data.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/owid/co2-data/master/owid-co2-data.csv\"\ndata = pd.read_csv(url)\n\n# Check the data\ndata.head()\nWe use pandas‚Äô read_csv function to load our data. data.head() prints the first 5 rows of our dataframe.\n\nPart III: Preprocessing the Data\nBefore we start modeling, we need to preprocess our data. We will deal with missing values and scale our data.\n# Drop missing values\ndata = data[['year', 'co2', 'gdp']].dropna()\n\n# Define our inputs and target\nX = data[['year', 'gdp']]\ny = data['co2']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nWe use dropna() to drop missing values, and train_test_split() to split our data into a training set and a test set. The StandardScaler() standardizes our data to have a mean of 0 and a standard deviation of 1.\n\nPart IV: Linear Regression\nLinear regression is a basic and commonly used type of predictive analysis.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Initialize the model\nlr = LinearRegression()\n\n# Fit the model\nlr.fit(X_train, y_train)\n\n# Predict the test set\ny_pred = lr.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint('MSE: ', mse)\nWe initialize our model with LinearRegression(), fit it to our training data with lr.fit(), and make predictions with lr.predict(). Finally, we evaluate our model by calculating the Mean Squared Error.\n\nPart V: Lasso, Ridge and Elastic Net Regression\nThese are extensions of linear regression that have regularizations.\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\n\n# Lasso\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_pred = lasso.predict(X_test)\nprint\n\n('Lasso MSE: ', mean_squared_error(y_test, y_pred))\n\n# Ridge\nridge = Ridge(alpha=0.1)\nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_test)\nprint('Ridge MSE: ', mean_squared_error(y_test, y_pred))\n\n# Elastic Net\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic.fit(X_train, y_train)\ny_pred = elastic.predict(X_test)\nprint('ElasticNet MSE: ', mean_squared_error(y_test, y_pred))\nFor each model, we initialize it (with the appropriate hyperparameters), fit it, make predictions, and evaluate it. The alpha hyperparameter controls the strength of the penalty, and l1_ratio controls the mixture of L1 and L2 penalties for Elastic Net.\n\nPart VI: Bayesian Regression and Decision Trees\nBayesian regression uses Bayes‚Äô theorem to update the probability of a hypothesis as more evidence or information becomes available. Decision Trees split the data into branches to make predictions.\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import BayesianRidge\n\n# Bayesian Ridge Regression\nbayesian = BayesianRidge()\nbayesian.fit(X_train, y_train)\ny_pred = bayesian.predict(X_test)\nprint('BayesianRidge MSE: ', mean_squared_error(y_test, y_pred))\n\n# Decision Tree Regression\ntree = DecisionTreeRegressor(max_depth=5)\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\nprint('DecisionTree MSE: ', mean_squared_error(y_test, y_pred))\nThe procedure for Bayesian Ridge and Decision Tree Regression is the same as before. For Decision Trees, the max_depth hyperparameter controls the maximum depth of the tree.\n\nConclusion\nIn this tutorial, we have covered various types of regression models in Python, including linear regression, lasso, ridge, elastic net, Bayesian regression, and decision trees. We have also demonstrated how to import and preprocess real-world data, fit models, make predictions, and evaluate model performance. By understanding these fundamental concepts, you‚Äôre now well equipped to explore more complex predictive modeling techniques."
  },
  {
    "objectID": "Tutorials/Regression2.html",
    "href": "Tutorials/Regression2.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Hyperparameter Optimization in Regression Analysis - Part 1\nIntroduction:\nIn the previous tutorial, we discussed Lasso, Ridge, and Elastic Net regressions. In this tutorial, we will focus on the basics of how to optimize these models through hyperparameter tuning.\n\nPart I: Understanding Hyperparameters\n1.1: What are Hyperparameters?\nHyperparameters are parameters that are not learned from data. They are set prior to the commencement of the learning process and help to control the behavior of the model. For instance, the ‚Äòalpha‚Äô in our previous tutorial is a hyperparameter.\n\nPart II: Methods for Hyperparameter Tuning\n2.1: Grid Search\nGrid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on the specific parameter values of a model.\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the grid of hyperparameters 'params'\nparams = {'alpha': [0.02, 0.024, 0.025, 0.026, 0.03]}\n\n# Instantiate a 10-fold CV grid search object 'grid'\ngrid = GridSearchCV(estimator=lasso, param_grid=params, cv=10)\n\n# Fit the grid search\ngrid.fit(X_train, y_train)\n2.2: Random Search\nRandom search differs from grid search in that it does not try every parameter value. Instead, it randomly selects values within the ranges specified.\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Define the grid of hyperparameters 'params'\nparams = {'alpha': [0.02, 0.024, 0.025, 0.026, 0.03]}\n\n# Instantiate a 10-fold CV randomized search object 'random'\nrandom = RandomizedSearchCV(estimator=ridge, param_distributions=params, cv=10)\n\n# Fit the random search\nrandom.fit(X_train, y_train)\n\nPart III: Hyperparameter Tuning for Regression Models\n3.1: Tuning Lasso Regression\nWhen tuning Lasso Regression, the main hyperparameter to adjust is ‚Äòalpha‚Äô, which controls the amount of shrinkage: the larger the value of alpha, the greater the amount of shrinkage.\n3.2: Tuning Ridge Regression\nIn Ridge Regression, ‚Äòalpha‚Äô is also the primary hyperparameter. However, the impact is slightly different: alpha reduces the complexity of the model by coefficient shrinkage.\n3.3: Tuning Elastic Net Regression\nElastic Net has two key hyperparameters: ‚Äòalpha‚Äô and ‚Äòl1_ratio‚Äô. ‚ÄòAlpha‚Äô is the penalty term and ‚Äòl1_ratio‚Äô is the mix between L1 and L2 regularization.\n\nConclusion:\nHyperparameter tuning is crucial in building optimal machine learning models. Although it can be a time-consuming process, Python, with libraries such as Scikit-Learn, provides efficient tools for carrying out this process effectively. Tuning your models can lead to significant improvements in model performance."
  },
  {
    "objectID": "Tutorials/Regression3.html",
    "href": "Tutorials/Regression3.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Hyperparameter Optimization in Python ‚Äì Mastering Hyperopt and Sklearn\n\nIntroduction\nIn the previous two tutorials we have cover the basics of linear models and some of the hyperparameters.\nAs you know, hyperparameters are parameters that are not learned from the data, and they need to be set before the learning process begins. In this tutorial, we will focus on hyperparameter optimization using Hyperopt and Sklearn in Python.\nWe will continue with the data and models from the first part of the tutorial.\n\nPart I: Setting Up Your Environment\nFirstly, we need to install hyperopt if it‚Äôs not already installed. You can install it by running the following command:\n!pip install hyperopt\nNow, let‚Äôs import all the libraries we are going to use.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\nPart II: Defining the Objective Function\nWe will start by defining our objective function. This function should take in the hyperparameters and return the value that we want to minimize, which in our case is the cross-validation mean squared error of our model.\ndef objective(hyperparameters):\n    model = Ridge(**hyperparameters)\n    mse = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error').mean()\n    return {'loss': -mse, 'status': STATUS_OK}\nHere, we‚Äôre creating a Ridge regression model with the given hyperparameters, computing the cross-validation mean squared error, and returning it as the loss to be minimized.\n\nPart III: Setting Up the Search Space\nNow we will define the search space for our hyperparameters. We will search for the optimal alpha value, which is a regularization strength parameter for Ridge regression.\nspace = {\n    'alpha': hp.uniform('alpha', 0.0, 2.0)\n}\nWe‚Äôre telling Hyperopt that we want to find an alpha between 0.0 and 2.0. hp.uniform specifies that any value within the range is equally likely to be selected as a candidate.\n\nPart IV: Running the Optimization\nWe are now ready to start our hyperparameter optimization!\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trials)\nThe fmin function performs the optimization. We pass in our objective function, the search space, the optimization algorithm (Tree-structured Parzen Estimator), and the number of evaluations to perform.\n\nPart V: Visualizing the Results\nFinally, let‚Äôs visualize the process of optimization.\nloss = [x['result']['loss'] for x in trials.trials]\n\nplt.figure(figsize=(8, 6))\nplt.plot(loss)\nplt.title('Hyperopt Optimization Process')\nplt.xlabel('Iteration')\nplt.ylabel('Mean Squared Error')\nplt.show()\nThis script creates a plot of mean squared error over each iteration, which can give you an idea of how the optimization progressed.\n\nConclusion\nHyperparameter optimization is a crucial step in tuning machine learning models, and libraries like Hyperopt provide powerful tools for this purpose. By understanding these techniques, you can train models that perform better and utilize resources more efficiently. Happy modeling!"
  },
  {
    "objectID": "Tutorials/RF.html",
    "href": "Tutorials/RF.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Random Forests - Understanding, Implementing, and Optimizing\nIntroduction:\nRandom Forests are a popular machine learning method that are highly flexible and capable of performing both regression and classification tasks. In this tutorial, we will explore Random Forests in depth, including when they are a good fit, their parameters and hyperparameters, and how to implement and optimize them in Python.\n\nPart I: Understanding Random Forests\n1.1: What is a Random Forest?\nA Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mean/ mode of the individual trees for regression/ classification tasks, respectively.\n1.2: When to Use Random Forests\nRandom Forests are particularly effective in situations where you have a large dataset with many features and complex relationships. They can handle both categorical and numerical data and are robust to outliers and missing data. They also prevent overfitting by averaging the output of many trees.\n\nPart II: Parameters and Hyperparameters of Random Forests\n2.1: Main Parameters\n\nn_estimators: The number of trees in the forest.\nmax_features: The maximum number of features considered when splitting a node.\nmax_depth: The maximum depth of the tree.\n\n2.2: Hyperparameters\nThese are parameters that cannot be learned from the data and are set prior to training. In Random Forests, these include all parameters like n_estimators, max_features, and max_depth.\n\nPart III: Implementing Random Forests in Python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instantiate the random forest model\nrf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\nPart IV: Optimizing Random Forests\nOptimizing a Random Forest involves tuning its hyperparameters to find the best combination for your specific dataset. This can be achieved using methods such as Grid Search and Randomized Search, as we discussed in the previous tutorial.\n4.1: Grid Search Example\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8]\n}\n\n# Create a based model\nrf = RandomForestClassifier()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\nConclusion:\nRandom Forests are a powerful and flexible machine learning algorithm that can provide excellent results on a wide range of problems. Understanding the underlying mechanics, knowing how to implement them in Python, and being able to optimize their hyperparameters are all vital skills for any machine learning practitioner. Remember, as with all machine learning algorithms, it‚Äôs important to thoroughly understand your data and the problem at hand to make informed decisions on algorithm selection and tuning."
  },
  {
    "objectID": "Tutorials/RL.html",
    "href": "Tutorials/RL.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning that focuses on how an agent ought to take actions in an environment to maximize some notion of cumulative reward. In this tutorial, we will go over the fundamental principles of RL and how to implement them in Python and R. Absolutely! Let‚Äôs delve deeper into the concepts and code.\n\n\nReinforcement Learning is a dynamic process that uses rewards and punishments to train an agent. It consists of several elements:\n\nAgent: This is the learner or decision-maker. It takes actions based on the state it‚Äôs in and the policy it has learned.\nEnvironment: This is the context in which the agent operates. It could be a maze for a robotic agent or a board game for a game-playing agent.\nState: This is the current situation the agent is in. The state will be presented to the agent as an input, and it is the agent‚Äôs representation of the environment at a given time.\nAction: These are the possible steps or decisions that the agent can make.\nReward: These are the feedback signals that guide the agent to the goal. A reward can be positive (for good actions) or negative (for bad actions).\n\nThe agent learns from the consequences of its actions, rather than from being taught explicitly. It selects actions based on its past experiences (exploitation) and also by new choices (exploration), which is essentially the dilemma between choosing a known best action and trying out a new action to see if it‚Äôs better.\n\n\n\nQ-Learning is a values iteration algorithm in RL. In Q-Learning, we define a Q-function (Q stands for quality) that the agent learns. The Q-function Q(s,a) gives the expected future reward if action a is taken in state s.\nThe Q-Learning algorithm uses a table to store the Q-values for each (state-action) pair. This table guides the agent to the best action at each state.\n\n\n\nWe will use the OpenAI Gym library, which is a popular Python library for developing and comparing RL algorithms. It provides several pre-defined environments where we can test our agents.\nOne such environment is the FrozenLake-v0 game. In the FrozenLake game, the agent controls the movement of a character in a grid world. The agent‚Äôs goal is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). The catch is that the ice is slippery, so the agent won‚Äôt always move in the direction it intends to.\nHere‚Äôs an example of a FrozenLake environment:\nSFFF       (S: starting point, safe)\nFHFH       (F: frozen surface, safe)\nFFFH       (H: hole, fall to your doom)\nHFFG       (G: goal, where the frisbee is located)\nThis Python code implements the Q-Learning algorithm:\nTo start, we should install the necessary dependencies, including TensorFlow and Keras for building neural networks, and Gym for the reinforcement learning environment.\npip install gym tensorflow keras numpy matplotlib\nThe implementation is the following:\nimport numpy as np\nimport gym\nimport random\nimport time\nfrom IPython.display import clear_output\n\n# Create the environment\nenv = gym.make(\"FrozenLake-v0\")\nHere we import the necessary libraries and create the FrozenLake-v0 environment.\n# Initialize Q-table with zero\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\nThe Q-table is initialized with zeros and has a size of (number_of_states x number_of_actions).\n# Hyperparameters\nnum_episodes = 10000\nmax_steps_per_episode = 100\n\nlearning_rate = 0.1\ndiscount_rate = 0.99\n\nexploration_rate = 1\nmax_exploration_rate = 1\nmin_exploration_rate = 0.01\nexploration_decay_rate = \n\n0.001\n\nrewards_all_episodes = []\nThese are the hyperparameters. The number of episodes is the number of games we want the agent to play during the training. In each episode, the agent can make a fixed number of steps (max_steps_per_episode). The learning_rate is the rate at which the AI agent should learn, the discount_rate is the importance of future rewards. The exploration rate is the rate at which the agent should explore the environment randomly, instead of using the already computed Q-values.\n# Q-learning algorithm\nfor episode in range(num_episodes):\n    state = env.reset()\n\n    done = False\n    rewards_current_episode = 0\n\n    for step in range(max_steps_per_episode): \nHere we start to play the episodes. At the start of each episode, we reset the environment to its initial state. done is a flag used to indicate if the episode has ended. rewards_current_episode is used to sum up all the rewards the agent has received in the current episode.\n        exploration_rate_threshold = random.uniform(0, 1)\n        if exploration_rate_threshold > exploration_rate:\n            action = np.argmax(q_table[state,:]) \n        else:\n            action = env.action_space.sample()\nHere the agent decides whether to explore or exploit. A random number is chosen. If it is greater than the exploration rate, the agent exploits the environment and chooses the action with the highest Q-value in the current state (exploit). Otherwise, it explores the environment, and the action is chosen randomly (explore).\n        new_state, reward, done, info = env.step(action)\n\n        # Update Q-table for Q(s,a)\n        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n\n        state = new_state\n        rewards_current_episode += reward \n\n        if done == True: \n            break\nThe agent performs the action and moves to the new state, and receives the reward. The Q-value of the (state-action) pair is updated using the Q-learning update rule.\n    # Exploration rate decay\n    exploration_rate = min_exploration_rate + \\\n        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n\n    rewards_all_episodes.append(rewards_current_episode)\nAfter each episode, we decrease the exploration rate. We want our agent to explore less as it learns more about the environment. The rewards_all_episodes list keeps track of the total rewards in each episode.\n\n\n\nNow let‚Äôs implement the same Q-Learning in R. We‚Äôre going to make a simple grid world with four cells, where the agent needs to find the terminal state to get a reward.\nlibrary(MDPtoolbox)\n\n# Define transition and reward matrices\nS <- 4\nA <- 2\nT <- array(0, c(S, A, S))\nR <- matrix(0, S, A)\n\n# Define transitions\nT[1, 1, 2] <- 1\nT[2, 1, 3] <- 1\nT[3, 1, 4] <- 1\nT[4, 1, 4] <- 1\n\nT[1, 2, 1] <- 1\nT[2, 2, 1] <- 1\nT[3, 2, 2] <- 1\nT[4, 2, 3] <- 1\n\n# Define rewards\nR[3, 1] <- 1\nR[4, 1] <- 1\n\n# Run Q-Learning\nresult <- mdp_example_qlearning(T, R, 0.9, 10000)\nThe code above creates a simple environment using the MDPtoolbox library. The environment is a simple four-state system where the agent learns to navigate to the terminal state using Q-Learning.\nLet‚Äôs now dive deeper into more advanced concepts of reinforcement learning with Python, including Policy Gradients and Deep Q-Networks (DQNs).\n\n\n\nPolicy Gradients are a type of reinforcement learning algorithms that are more advanced than Q-Learning. They learn a parameterized policy that can choose actions without consulting a value function. One popular algorithm in this family is the REINFORCE algorithm.\n\n\nInstead of learning a value function that tells us what is the expected sum of rewards given a state and an action (as in Q-Learning), we‚Äôre going to learn directly a policy function. The policy is a distribution over actions given states.\nIn Policy Gradient methods, we typically use a simple function approximator such as a neural network to approximate the policy function.\nThe REINFORCE algorithm works as follows:\n\nInitialize the policy parameters Œ∏ at random.\nGenerate an episode following œÄ(‚ãÖ|‚ãÖ;Œ∏), and let R_t be the return from time step t.\nFor each time step t of the episode, compute the gradient ‚àá_Œ∏ log œÄ(A_t|S_t;Œ∏) and update the parameters using stochastic gradient ascent: Œ∏ ‚Üê Œ∏ + Œ±R_t ‚àá_Œ∏ log œÄ(A_t|S_t;Œ∏)\n\n\n\n\nLet‚Äôs implement a simple version of the REINFORCE algorithm using the CartPole environment from Gym. This environment consists of a pole connected to a cart moving along a frictionless track. The pole starts upright, and the goal is to prevent it from falling over by applying a force of +1 or -1 to the cart.\nimport gym\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Reshape, Flatten\nfrom keras.optimizers import Adam\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(24, input_dim=4, activation='relu')) # input is the state\n    model.add(Dense(24, activation='relu'))\n    model.add(Dense(2, activation='softmax')) # output is the probability distribution over actions\n    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n    return model\nThis function creates our policy network. It takes a state as input and outputs a probability distribution over actions.\ndef generate_episode(policy, env):\n    states, actions, rewards = [], [], []\n    state = env.reset()\n    done = False\n    while not done:\n        state = state.reshape([1, state.size])\n        prob = policy.predict(state)[0]\n        action = np.random.choice(env.action_space.n, p=prob)\n        next_state, reward, done, _ = env.step(action)\n\n        states.append(state)\n        actions.append(action)\n        rewards.append(reward)\n\n        state = next_state\n\n    return states, actions, rewards\nThis function generates an episode by running the policy in the environment. It returns the list of states, actions, and rewards encountered during the episode.\ndef compute_returns(rewards, gamma=0.99):\n    R = 0\n    returns = []\n    for r in reversed(rewards):\n        R = r + gamma * R\n        returns.insert(0, R)\n    return np.array(returns)\nThis function computes the returns for each time step in an episode, using the rewards and a discount factor gamma.\ndef train_policy(policy, episodes):\n   \n\n env = gym.make('CartPole-v0')\n    for episode in range(episodes):\n        states, actions, rewards = generate_episode(policy, env)\n        returns = compute_returns(rewards)\n        X = np.squeeze(np.vstack(states))\n        y = np.zeros([len(actions), env.action_space.n])\n        for i, action in enumerate(actions):\n            y[i, action] = returns[i]\n        policy.train_on_batch(X, y)\n    env.close()\nThis function trains the policy for a given number of episodes. For each episode, it generates an episode, computes the returns, and updates the policy parameters using the returns as weight for the actions taken.\nNow we can create our policy and train it:\npolicy = create_model()\ntrain_policy(policy, 1000)\n\n\n\n\nDeep Q-Networks (DQNs) are a type of Q-Learning algorithm that uses a neural network as a function approximator for the Q-function. It was introduced by DeepMind in their groundbreaking paper ‚ÄúPlaying Atari with Deep Reinforcement Learning‚Äù in 2013.\n\n\nDQN is essentially Q-Learning with function approximation, but with a few key differences:\n\nExperience Replay: To break the correlation between consecutive samples, DQN stores the agent‚Äôs experiences in a replay buffer. During training, it samples mini-batches of experiences from the replay buffer to update the Q-network.\nTarget Network: To stabilize the training, DQN uses two networks: a Q-network for selecting actions, and a target network (a clone of the Q-network) for generating the Q-learning targets. The weights of the target network are updated less frequently (usually, every few thousand steps) to keep the targets stable.\n\n\n\n\nNow let‚Äôs implement a DQN for the CartPole environment.\nimport gym\nimport random\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom collections import deque\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95  # discount rate\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.model = self._build_model()\nThis class defines our DQN agent. It has a memory for storing experiences, parameters for the epsilon-greedy policy (epsilon is the probability of choosing a random action), and a neural network model for approximating the Q-function.\ndef _build_model(self):\n    model = Sequential()\n    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n    model.add(Dense(24, activation='relu'))\n    model.add(Dense(self.action_size, activation='linear'))\n    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n    return model\nThis function builds a simple 2-layer fully connected neural network. The input size is the size of the state space, and the output size is the size of the action space. The model uses mean squared error (MSE) for loss as we‚Äôre dealing with a regression problem (predicting the Q-values), and the Adam optimizer for training the model.\nNext, we define a method for the agent to remember the experiences.\ndef remember(self, state, action, reward, next_state, done):\n    self.memory.append((state, action, reward, next_state, done))\nThis function simply adds an experience to the agent‚Äôs memory. The experience is a tuple of state, action, reward, next state, and done (which tells whether the episode ended after this step).\nNow, let‚Äôs implement the action selection policy.\ndef act(self, state):\n    if np.random.rand() <= self.epsilon:\n        return random.randrange(self.action_size)\n    act_values = self.model.predict(state)\n    return np.argmax(act_values[0])  # returns action\nThis function implements an epsilon-greedy policy. With probability epsilon, it chooses a random action (for exploration), and with probability 1 - epsilon, it chooses the best action according to the current Q-value estimates (for exploitation).\nThe core of the DQN algorithm is in the learning phase, where it updates the Q-value estimates.\ndef replay(self, batch_size):\n    minibatch = random.sample(self.memory, batch_size)\n    for state, action, reward, next_state, done in minibatch:\n        target = self.model.predict(state)\n        if done:\n            target[0][action] = reward\n        else:\n            Q_future = max(self.model.predict(next_state)[0])\n            target[0][action] = reward + self.gamma * Q_future\n        self.model.fit(state, target, epochs=1, verbose=0)\n    if self.epsilon > self.epsilon_min:\n        self.epsilon *= self.epsilon_decay\nIn this function, a mini-batch of experiences is sampled from the memory, and for each experience, the Q-value target for the action taken is updated, and the network‚Äôs weights are updated to fit this target value. If the episode ended (done is True), the target is simply the reward, else it‚Äôs the reward plus the discounted estimated maximum future Q-value.\nFinally, we create an instance of the DQNAgent and train it to balance the pole in the CartPole environment.\nenv = gym.make('CartPole-v1')\nagent = DQNAgent(env.observation_space.shape[0], env.action_space.n)\nepisodes = 500\nbatch_size = 32\n\nfor e in range(episodes):\n    state = env.reset()\n    state = np.reshape(state, [1, agent.state_size])\n    for time in range(500):\n        action = agent.act(state)\n        next_state, reward, done, _ = env.step(action)\n        reward = reward if not done else -10\n        next_state = np.reshape(next_state, [1, agent.state_size])\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        if done:\n            print(\"episode: {}/{}, score: {}\".format(e, episodes, time))\n            break\n\n\n        if len(agent.memory) > batch_size:\n            agent.replay(batch_size)\nThe agent plays a number of episodes of the game, choosing actions via its act method, remembering the resulting experiences, and learning from its memory by replaying experiences. The game resets when the pole falls over, giving the agent negative rewards as a penalty.\nIn this tutorial, we went over a more advanced concept in reinforcement learning, including policy gradient methods like REINFORCE and value-based methods like DQN. These methods use neural networks to learn directly from high-dimensional sensory inputs, and have achieved superhuman performance in complex tasks like playing video games and controlling robots.\n\n\n\n\nReinforcement learning is a powerful approach for tasks that involve sequential decision-making. This tutorial presented the fundamental concepts of RL and walked through an example of how to implement Q-Learning, a simple but powerful RL algorithm, in Python and R. Remember that the RL field is vast and complex, this is just the tip of the iceberg!\nThis tutorial is intended as a starting point. I encourage you to continue exploring more complex environments, policies, and algorithms as you continue your journey in reinforcement learning. Happy learning!"
  },
  {
    "objectID": "Tutorials/RLStocks.html",
    "href": "Tutorials/RLStocks.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "In this tutorial we will focus on using machine learning techniques, particularly reinforcement learning, for trading in financial markets. The goal will be to build a trading agent that learns to maximize total profit over a given period.\nWe will be using the OpenAI Gym‚Äôs environment model, so if you are not already familiar with it, you can check the previous tutorials.\n\n\nBefore we can train an agent, we need to set up the trading environment. We‚Äôll use pandas_datareader to download historical stock price data from Yahoo Finance.\nFirst, let‚Äôs install the required libraries.\npip install gym pandas_datareader yfinance\nNext, we can import the required libraries:\nimport gym\nfrom gym import spaces\nimport numpy as np\nimport pandas as pd\nfrom pandas_datareader import data as pdr\nimport yfinance as yf\nyf.pdr_override()\nNext, let‚Äôs define our custom trading environment. The environment will have a continuous action space representing the proportion of our portfolio to invest in the stock. A positive action value indicates a long position, while a negative action value indicates a short position. The state will be the latest stock price and the current portfolio value.\nclass TradingEnv(gym.Env):\n    def __init__(self, stock_symbol, start_date, end_date, initial_balance=10000):\n        super(TradingEnv, self).__init__()\n\n        # Load data\n        self.stock_data = pdr.get_data_yahoo(stock_symbol, start=start_date, end=end_date)\n        self.stock_prices = self.stock_data['Adj Close'].values\n        self.initial_balance = initial_balance\n        self.n_steps = len(self.stock_prices)\n\n        # Define action and state space\n        # They must be gym.spaces objects\n        # Example when using discrete actions:\n        # self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n        self.action_space = spaces.Box(low=-1, high=1, shape=(1,))\n        self.observation_space = spaces.Box(low=0, shape=(2,))\n\n    def step(self, action):\n        # Execute one time step within the environment\n        pass\n\n    def reset(self):\n        # Reset the state of the environment to an initial state\n        pass\n\n    def render(self, mode='human'):\n        # Render the environment to the screen\n        pass\n\n\n\nThe trading agent will be implemented using a DQN (Deep Q-Network) similar to the one we used for playing the CartPole game. The agent will be trained to maximize the total rewards, which are defined as the change in portfolio value.\nimport random\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom collections import deque\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95  # discount rate\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n\n    def _build_model(self):\n        # Neural network for Deep-Q learning Model\n        model = Sequential()\n        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n        model.add(Dense(24, activation='relu'))\n        model.add(Dense(self.action_size, activation='linear'))\n        model\n\n.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n        return model\n\n    # Other methods like `remember`, `act`, `replay` would be similar to what we defined earlier in the CartPole example\n\n\n\nFinally, we can create an instance of our trading environment and our DQN agent, and train the agent using historical data. The agent learns by observing the outcome of its actions, remembering them, and learning from the experiences.\n# Specify the stock symbol and date range\nstock_symbol = 'AAPL'\nstart_date = '2015-01-01'\nend_date = '2020-12-31'\n\n# Create the trading environment\nenv = TradingEnv(stock_symbol, start_date, end_date)\n\n# Create the DQN agent\nagent = DQNAgent(env.observation_space.shape[0], env.action_space.shape[0])\n\n# Training the agent\nepisodes = 100\nfor e in range(episodes):\n    state = env.reset()\n    state = np.reshape(state, [1, agent.state_size])\n    for time in range(env.n_steps):\n        action = agent.act(state)\n        next_state, reward, done, _ = env.step(action)\n        reward = reward if not done else -10\n        next_state = np.reshape(next_state, [1, agent.state_size])\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        if done:\n            print(\"episode: {}/{}, score: {}\".format(e, episodes, time))\n            break\n        if len(agent.memory) > batch_size:\n            agent.replay(batch_size)\nNote that the trading environment and DQN agent are not fully implemented in this tutorial. The step, reset, and render methods in the TradingEnv class and the remember, act, and replay methods in the DQNAgent class need to be implemented similar to the CartPole example.\nAlso, note that this is a very simplified model that doesn‚Äôt take into account transaction costs, uses only the price data to make decisions, and assumes that the agent can trade any fractional amount of the stock, which might not be possible in real-world trading scenarios.\nFinally, remember that this model is trained entirely on historical data, and as the common saying in finance goes, ‚Äúpast performance is not indicative of future results‚Äù. Therefore, caution should be taken when using reinforcement learning models for actual trading."
  },
  {
    "objectID": "Tutorials/SQL.html",
    "href": "Tutorials/SQL.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "SQL 101\nIntroduction\nStructured Query Language (SQL) is a standardized programming language used for managing relational databases and performing various operations on the data in them. This tutorial will guide you through the basics to advanced SQL commands.\n\nPart I: SQL Basics\n1.1 Setting Up\nFirst, we need to set up a database. For this tutorial, we‚Äôll be using SQLite, a software library that provides a relational database management system.\nYou can install SQLite from its official website. After installation, you can interact with SQLite via the command-line shell.\n1.2 Creating a Database\nTo create a database in SQLite, you use the following command:\nsqlite3 TestDB.db\nThis command creates a new database named TestDB.db. If the database exists, it opens the database.\n1.3 Creating a Table\nTo create a table in SQL, we use the CREATE TABLE statement.\nCREATE TABLE Employees (\n    ID int,\n    Name text,\n    Age int,\n    Salary real\n);\nThis creates a table named Employees with four columns: ID, Name, Age, and Salary.\n1.4 Inserting Data\nWe can insert data into the table using the INSERT INTO statement.\nINSERT INTO Employees (ID, Name, Age, Salary) \nVALUES (1, 'John Doe', 30, 50000.00 );\nThis command inserts a new row into the Employees table.\n1.5 Querying Data\nYou can retrieve data using the SELECT statement.\nSELECT * FROM Employees;\nThis command selects all records from the Employees table.\n\nPart II: Intermediate SQL\n2.1 Updating Data\nYou can update existing records using the UPDATE statement.\nUPDATE Employees\nSET Salary = 55000.00\nWHERE ID = 1;\nThis command updates the salary of the employee with ID 1.\n2.2 Deleting Data\nYou can delete records using the DELETE statement.\nDELETE FROM Employees WHERE ID = 1;\nThis command deletes the employee with ID 1.\n2.3 Ordering Data\nYou can sort the result set using the ORDER BY keyword.\nSELECT * FROM Employees ORDER BY Salary DESC;\nThis command selects all employees and orders them by Salary in descending order.\n\nPart III: Advanced SQL\n3.1 Joins\nSQL joins are used to combine rows from two or more tables.\nSELECT Employees.Name, Orders.OrderID\nFROM Employees\nINNER JOIN Orders ON Employees.ID = Orders.EmployeeID;\nThis INNER JOIN keyword selects records that have matching values in both tables.\n3.2 Aggregate Functions\nSQL aggregate functions return a single value, calculated from values in a column.\nSELECT AVG(Salary) AS AverageSalary FROM Employees;\nThis command calculates the average salary of all employees.\n3.3 Group By\nThe GROUP BY statement groups rows that have the same values in specified columns into aggregated data.\nSELECT COUNT(ID), Age\nFROM Employees\nGROUP BY Age;\nThis command counts the number of employees of each age.\n3.4 Subquery\nA Subquery or Inner query or Nested query is a query within another SQL query.\nSELECT Name \nFROM Employees\nWHERE Salary > (SELECT AVG(Salary) FROM Employees);\nThis command finds employees who earn more than the average\nsalary.\n\nConclusion\nSQL is a powerful language with vast capabilities in managing and manipulating data in relational databases. With a solid understanding of SQL, from basics to advanced topics, you‚Äôll be well-equipped to handle any data thrown your way!"
  },
  {
    "objectID": "Tutorials/xgboost.html",
    "href": "Tutorials/xgboost.html",
    "title": "Dr Juan Beltran",
    "section": "",
    "text": "XGBoost - Understanding, Implementing, and Optimizing\nIntroduction:\nXGBoost, short for eXtreme Gradient Boosting, is an optimized and scalable machine learning system for tree boosting. In this tutorial, we‚Äôll explore the workings of XGBoost, when it‚Äôs a good choice, its parameters and hyperparameters, and how to implement and optimize it in Python.\n\nPart I: Understanding XGBoost\n1.1: What is XGBoost?\nXGBoost is an ensemble learning method that applies the principle of gradient boosting to decision trees. It‚Äôs recognized for its execution speed and model performance, which have made it a favorite tool among Kaggle competition winners.\n1.2: When to Use XGBoost\nXGBoost is a great choice when dealing with numerical or mixed data types, and when prediction accuracy is a major concern. It can handle both regression and classification problems, but it might be overkill for simple problems where a simpler model could work better.\n\nPart II: Parameters and Hyperparameters of XGBoost\n2.1: Main Parameters\n\nn_estimators: The number of gradient boosted trees. Equivalent to number of boosting rounds.\nmax_depth: Maximum tree depth for base learners.\nlearning_rate: Boosting learning rate (also known as eta).\ngamma: Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\n2.2: Hyperparameters\nJust like with Random Forests, the parameters listed above are hyperparameters, meaning they are set before the learning process begins.\n\nPart III: Implementing XGBoost in Python\nimport xgboost as xgb\n\n# Instantiate the XGBClassifier\nxgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n\n# Fit the classifier to the training set\nxgb_clf.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = xgb_clf.predict(X_test)\n\nPart IV: Optimizing XGBoost\nOptimizing XGBoost involves tuning its hyperparameters to get the best performance for a specific dataset. We can again use techniques like Grid Search and Randomized Search.\n4.1: Grid Search Example\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300, 500],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth' : [4,5,6,7,8],\n    'gamma': [0, 0.1, 0.2]\n}\n\n# Create a based model\nxgb_clf = xgb.XGBClassifier()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = xgb_clf, param_grid = param_grid, cv = 3)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\nConclusion:\nXGBoost is a powerful machine learning algorithm that excels in many contexts, particularly where accuracy is paramount. Understanding the ins and outs of XGBoost, knowing how to implement it in Python, and being able to optimize its hyperparameters are essential skills for any data scientist or machine learning engineer."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning that focuses on how an agent ought to take actions in an environment to maximize some notion of cumulative reward. In this tutorial, we will go over the fundamental principles of RL and how to implement them in Python and R.\n\n\nRL revolves around these main concepts:\n\nAgent: The RL algorithm that learns from trial and error.\nEnvironment: The context where the agent operates.\nState: The current situation of the agent.\nAction: All possible moves the agent can make.\nReward: Feedback from the environment.\n\nThe agent learns by interacting with its environment, it takes actions, the environment returns the new state and gives a reward. The goal of the agent is to maximize the sum of rewards.\n\n\n\nOne of the simplest ways to understand reinforcement learning is through Q-Learning. The objective of Q-Learning is to find a policy that is optimal in the sense that the expected value of the total reward over all successive steps is maximized.\nThe Q-Learning algorithm uses a table (Q-table) where we have a row for each state (s) and a column for each action (a). The cell at the intersection of state (s) and action (a) represents the expected future reward that the agent will get if it takes action (a) while in state (s).\nLet‚Äôs start with the Python implementation of Q-learning.\n\n\n\nFor our Python implementation, we will use the OpenAI Gym environment, which provides different game scenarios for training reinforcement learning agents. We‚Äôll work with the FrozenLake-v0 game.\nimport numpy as np\nimport gym\nimport random\nimport time\nfrom IPython.display import clear_output\n\n# Create the environment\nenv = gym.make(\"FrozenLake-v0\")\n\n# Initialize Q-table with zero\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n# Hyperparameters\nnum_episodes = 10000\nmax_steps_per_episode = 100\n\nlearning_rate = 0.1\ndiscount_rate = 0.99\n\nexploration_rate = 1\nmax_exploration_rate = 1\nmin_exploration_rate = 0.01\nexploration_decay_rate = 0.001\n\nrewards_all_episodes = []\n\n# Q-learning algorithm\nfor episode in range(num_episodes):\n    state = env.reset()\n\n    done = False\n    rewards_current_episode = 0\n\n    for step in range(max_steps_per_episode): \n        exploration_rate_threshold = random.uniform(0, 1)\n        if exploration_rate_threshold > exploration_rate:\n            action = np.argmax(q_table[state,:]) \n        else:\n            action = env.action_space.sample()\n\n        new_state, reward, done, info = env.step(action)\n\n        # Update Q-table for Q(s,a)\n        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n\n        state = new_state\n        rewards_current_episode += reward \n\n        if done == True: \n            break\n\n    # Exploration rate decay\n    exploration_rate = min_exploration_rate + \\\n        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n\n    rewards_all_episodes.append(rewards_current_episode)\nThis code creates an agent that learns to play\nthe FrozenLake-v0 game. The agent uses Q-Learning to learn how to play.\n\n\n\nNow let‚Äôs implement the same Q-Learning in R. We‚Äôre going to make a simple grid world with four cells, where the agent needs to find the terminal state to get a reward.\nlibrary(MDPtoolbox)\n\n# Define transition and reward matrices\nS <- 4\nA <- 2\nT <- array(0, c(S, A, S))\nR <- matrix(0, S, A)\n\n# Define transitions\nT[1, 1, 2] <- 1\nT[2, 1, 3] <- 1\nT[3, 1, 4] <- 1\nT[4, 1, 4] <- 1\n\nT[1, 2, 1] <- 1\nT[2, 2, 1] <- 1\nT[3, 2, 2] <- 1\nT[4, 2, 3] <- 1\n\n# Define rewards\nR[3, 1] <- 1\nR[4, 1] <- 1\n\n# Run Q-Learning\nresult <- mdp_example_qlearning(T, R, 0.9, 10000)\nThe code above creates a simple environment using the MDPtoolbox library. The environment is a simple four-state system where the agent learns to navigate to the terminal state using Q-Learning.\n\n\n\nReinforcement learning is a powerful approach for tasks that involve sequential decision-making. This tutorial presented the fundamental concepts of RL and walked through an example of how to implement Q-Learning, a simple but powerful RL algorithm, in Python and R. Remember that the RL field is vast and complex, this is just the tip of the iceberg!\nThis tutorial is intended as a starting point. I encourage you to continue exploring more complex environments, policies, and algorithms as you continue your journey in reinforcement learning. Happy learning!"
  },
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Work Experience",
    "section": "",
    "text": "Lead Data Scientist | ECOM Trading | Nov 2022 - Present\n\nCurrently leading the data science team at the Research department in ECOM Trading, one of the world‚Äôs largest traders of coffee, cotton and cocoa. I started at ECOM Trading in 2019 as a Senior Data Scientist, I was promoted to the role of Lead Data Scientist in November 2022.\nMy career at ECOM has been marked by pivotal contributions to our core product offerings. These contributions span the realms of data infrastructure and data centralisation as well as the creation, deployment and maintenance of machine learning models, which I describe in further detail below.\nI‚Äôve played a significant role in the development and maintenance of advanced trading signal models that leverage Supply & Demand dynamics, weather data, and historical price trends. This unique blend of information allows us to predict market behaviour with high precision, creating a strategic advantage in our trading operations.\nRecognizing the critical importance of sustainability in today‚Äôs market, I‚Äôve been instrumental in developing a new framework for measuring deforestation on global farms. This initiative highlights our commitment to responsible trading and provides invaluable insights into environmental impact.\nTo further streamline our processes and ensure data integrity, I‚Äôve implemented an automated alert system that identifies potential anomalies in cotton, coffee and cocoa price data. This system proactively sends email alerts when irregularities are detected, allowing us to swiftly mitigate potential risks and maintain market-leading accuracy in our data-driven decisions.\n\nSenior Data Scientist | ECOM Trading | Dec 2019 - Nov 2022\n\nLed the development of a comprehensive suite of data products, encompassing dashboards, automatic reports, and web applications in the research department as a senior data scientist.\nDeveloped an array of machine learning models to tackle key industry questions, from predicting the supply and demand of commodities like coffee, cotton, and cocoa to estimating crop yields using satellite imagery.\nUtilized my predictive models to forecast a range of pivotal factors, including the productivity of West African cocoa farms, the medium-term climate change effects on global plantations, and future price dynamics based on climatic variables.\nConceived an innovative web application that fused historical deforestation data with georeferenced farm locations, forming a unique deforestation risk model for global farms.\n\nProduct Developer - Data Science | Decoded | Oct 2018 - Nov 2019\n\nPioneered and executed over 40% of the curriculum for Decoded‚Äôs flagship offering, the Data Academy, fostering data science integration among blue-chip clients, including Societe General, UBS, Unilever, Nike, and M&S.\nCrafted comprehensive course material covering fundamental coding in Python and R, advanced statistics, and a wide array of machine learning techniques.\nAuthored specialized R and Python modules for an extensive range of topics such as regression analysis, classification methods, neural networks, Big Data handling, time series analysis, and SQL.\nRegularly hosted workshops to impart these modules to client employees, both in-person and via webinars, and trained new facilitators to carry the torch forward.\n\nResearch Associate - Data Science | Institute for Risk and Disaster Reduction, University College London | Jan 2018 - Oct 2018\n\nDeveloped a machine learning model capable of real-time prediction of potential mosquito breeding points across four cities in Brazil, employing Python, TensorFlow, and Keras to create recurrent neural networks that geospatially modelled virus and mosquito occurrences.\nThe results of my work were manifested in a user-friendly bokeh web application, which I built and was actively used by health professionals in Brazil.\nComplimenting this, I also created a mobile application to streamline data collection from health professionals on the ground."
  }
]