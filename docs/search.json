[
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Work Experience",
    "section": "",
    "text": "Currently leading the data science team at the Research department in ECOM, my journey has been marked by pivotal contributions to our core product offerings.\nI’ve played a significant role in the development and maintenance of advanced trading signal models that leverage Supply & Demand dynamics, weather data, and historical price trends. This unique blend of information allows us to predict market behaviour with high precision, creating a strategic advantage in our trading operations.\nRecognizing the critical importance of sustainability in today’s market, I’ve been instrumental in developing a new framework for measuring deforestation on global farms. This initiative highlights our commitment to responsible trading and provides invaluable insights into environmental impact.\nTo further streamline our processes and ensure data integrity, I’ve implemented an automated alert system that identifies potential anomalies in cotton, coffee and cocoa price data. This system proactively sends email alerts when irregularities are detected, allowing us to swiftly mitigate potential risks and maintain market-leading accuracy in our data-driven decisions.\n\nSenior Data Scientist | ECOM Trading | Dec 2019 - Nov 2022\n\nLed the developments of a comprehensive suite of data products, encompassing dashboards, automatic reports, and web applications in the research department as a senior data scientist.\nDeveloped an array of machine learning models to tackle key industry questions, from predicting the supply and demand of commodities like coffee, cotton, and cocoa to estimating crop yields using satellite imagery.\nUtilized my predictive models to forecast a range of pivotal factors, including the productivity of West African cocoa farms, the medium-term climate change effects on global plantations, and future price dynamics based on climatic variables.\nConceived an innovative web application that fused historical deforestation data with georeferenced farm locations, forming a unique deforestation risk model for global farms.\n\nProduct Developer - Data Science | Decoded | Oct 2018 - Nov 2019\n\nPioneered and executed over 40% of the curriculum for Decoded’s flagship offering, the Data Academy, fostering data science integration among blue-chip clients, including Societe General, UBS, Unilever, Nike, and M&S.\nCrafted comprehensive course material covering fundamental coding in Python and R, advanced statistics, and a wide array of machine learning techniques.\nAuthored specialized R and Python modules for an extensive range of topics such as regression analysis, classification methods, neural networks, Big Data handling, time series analysis, and SQL.\nRegularly hosted workshops to impart these modules to client employees, both in-person and via webinars, and trained new facilitators to carry the torch forward.\n\nResearch Associate - Data Science | Institute for Risk and Disaster Reduction, University College London | Jan 2018 - Oct 2018\n\nDeveloped a machine learning model capable of real-time prediction of potential mosquito breeding points across four cities in Brazil, employing Python, TensorFlow, and Keras to create recurrent neural networks that geospatially modelled virus and mosquito occurrences.\nThe results of my work were manifested in a user-friendly bokeh web application, which I built and was actively used by health professionals in Brazil.\nComplimenting this, I also created a mobile application to streamline data collection from health professionals on the ground."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr Juan D. Beltran",
    "section": "",
    "text": "I have been coding in R for the last 15 years and in Python for the last 9 years. I have designed, developed and maintained a wide range of data products which combine machine learning models and GIS analysis in the shape of stand alone models, dashboards, advanced visualisation, automatic reports, and web apps. I have extensive knowledge in statistical modelling, forecasting time series and geospatial analysis."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2017: Yang X, Hu R, Yin H, Jenkins J, Shu S, Tang H, Liu D, Weighill DA, Ha J, Heyduk K, Goodstein DM, Guo H, Moseley RC, Fitzek E, Jawdy S, Zhang Z, Xie M, Hartwell J, Grimwood J, Abraham PE, Mewalal R, Yim WC, Beltrán JD, Boxall SF, Dever LV, Palla KJ, Albion R, Garcia T, Mayer J, Lim SD, Wai CM, Van Buren R, De Paoli HC, Borland AM, Guo H, Chen J, Muchero W, Yin Y, Jacobson DA, Tschaplinski TJ, Hettich RL, Ming R , Winter K , Leebens-Mack JH, Smith JAC, Cushman J, Schmutz J, Tuskan GA.: Kalanchoë genome reveals convergent evolution of crassulacean acid metabolism. Nature Communications 8: 1899\n2015: Yang X, Cushman JC, Borland AM, Edwards EJ, Wullschleger SD, Tuskan GA, Owen NA, Griffiths H, Smith JAC, De Paoli HC, Weston DJ, Cottingham R, Hartwell J, Davis SC, Silvera K, Ming R, Schlauch K, Abraham P, Stewart JR, Guo H, Albion R, Ha J, Lim SD, Wone BWM, Yim WC, Garcia T, Mayer JA, Petereit J, Nair SS, Casey E, Hettich RL, Ceusters J, Ranjan P, Palla KP, Yin H, Reyes-García C, Andrade JL, Freschi L, Beltrán JD, Dever LV, Boxall SF,Waller J, Davies J, Bupphada P, Kadu N, Winter K, Sage RF, Aguilar CN, Schmutz J, Jenkins J, Holtum JAM. A roadmap for research on crassulacean acid metabolism (CAM) to enhance sustainable food and bioenergy production in a hotter, drier world. New Phytologist 207:491–504.\n2014: Poster presentation, “Evolutionary origins and ecophysiology of CAM photosynthesis in the montane genus Puya (Bromeliaceae)”, 34 th New Phytologist Symposium “Plant Systems Biology and Ecology of CAM plants”, Lake Tahoe, Tahoe City, CA, USA, 15–18 July 2014.\n2013: Beltrán JD., Lasso E., Madriñán S., Virgo A., Winter K.: Juvenile tank-bromeliads lacking tanks: Do they engage in CAM photosynthesis?, Photosynthetica 51: 55–62.\n2012: M.Sc. Thesis: CAM or not CAM: A study on juveniles of Guzmania lingulata, Guzmania monostachia and Werauhia sanguinolenta (Bromeliaceae), Universidad de los Andes.\n2010: B.Sc. Thesis: Evolution of xerophyte habit in the tribe Tillandsieae (Bromeliaceae): a phylogenetic approach, Universidad de los Andes."
  },
  {
    "objectID": "juan.html",
    "href": "juan.html",
    "title": "Summary",
    "section": "",
    "text": "I have several years of experience working as a data scientist in different industries. Advanced knowledge of machine learning, time series analysis, and GIS. Significant experience programming in Python and R."
  },
  {
    "objectID": "juan.html#project-management",
    "href": "juan.html#project-management",
    "title": "Summary",
    "section": "Project Management",
    "text": "Project Management\nI have experience establishing and working as a part of cross-functional teams. I have worked with Agile methodology and continuous integration/continuous delivery principles."
  },
  {
    "objectID": "juan.html#presentation-data-visualisation",
    "href": "juan.html#presentation-data-visualisation",
    "title": "Summary",
    "section": "Presentation & Data Visualisation",
    "text": "Presentation & Data Visualisation\nI have strong experience building dashboards,reports and presentations for wider audiences using RMarkdown, Shiny and Bokeh. Recent experience using Quarto (which I used to build my CV). I have presented my results in academic and commercial journals. I run a journal club in the research department."
  },
  {
    "objectID": "juan.html#leadership",
    "href": "juan.html#leadership",
    "title": "Summary",
    "section": "Leadership",
    "text": "Leadership\nI have been mentoring juniors peers and acting as a data advocate across multiple teams in my current and previous roles. I have researched difficult problems with curiosity and rigour. I have been designing technical interviews and explaining complex concepts to stakeholders with clarity and empathy."
  },
  {
    "objectID": "Forecasting.html",
    "href": "Forecasting.html",
    "title": "Forecasting with Time Series Methods: A Comparative Approach using R and Python",
    "section": "",
    "text": "When it comes to predicting future trends, events, or quantities—known as forecasting—time series methods are some of the most commonly used approaches. By definition, a time series is a sequence of data points recorded at successive, equally spaced intervals. Given the unique traits of time series data, specialized analysis techniques have been developed over time, and these methods can be used to forecast future data points.\nIn this blog post, we will look at how to apply time series methods to forecast data using two popular programming languages: R and Python.\n\n\nBefore delving into the coding aspect, it’s important to understand the key components of a time series:\n\nTrend: The underlying pattern of growth or decline in a time series.\nSeasonality: Regular and predictable changes that recur every calendar year.\nCyclical components: Fluctuations happening due to economic cycles.\nIrregular (or residual) component: The random variation in the series.\n\n\n\n\nR provides robust support for time series analysis via several specialized packages like forecast, tseries, and tsibble.\nIn this example, we will use the forecast package to analyze the AirPassengers dataset.\n\n# Loading required library\n\nlibrary(forecast)\n\n# Load the dataset\n\ndata(AirPassengers)\n\n# Plotting the data\n\nplot(AirPassengers)\n\n# Decomposing the time series\n\ndecomposed_ap <- decompose(AirPassengers)\n\n# Plotting the decomposed time series\n\nplot(decomposed_ap)\n\n# Applying the ARIMA model\n\narima_model <- auto.arima(AirPassengers)\n\n# Forecasting for next 24 months\n\nforecast_arima <- forecast(arima_model, h=24)\n\n# Plotting the forecast\n\nplot(forecast_arima)\nIn this example, we first decomposed the time series to understand its underlying components. We then fitted an ARIMA model using the auto.arima() function, and used it to forecast the next 24 months.\n\n\n\nPython also has strong support for time series analysis, particularly through the pandas and statsmodels libraries.\nLet’s analyze the same AirPassengers dataset in Python.\n\n# Importing required libraries\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nimport matplotlib.pyplot as plt\n\n# Load the dataset\n\nair_passengers = pd.read_csv('AirPassengers.csv', parse_dates=['Month'], index_col='Month')\n\n# Plotting the data\n\nair_passengers.plot()\n\n# Decomposing the time series\n\ndecomposition = seasonal_decompose(air_passengers)\n\n# Plotting the decomposed time series\n\ndecomposition.plot()\n\nplt.show()\n\n# Applying the ARIMA model\n\nmodel = ARIMA(air_passengers, order=(5,1,0))\n\nmodel_fit = model.fit(disp=0)\n\n# Forecasting for next 24 months\n\nforecast, stderr, conf_int = model_fit.forecast(steps=24)\n\n# Plotting the forecast\n\nplt.plot(forecast)\n\nplt.fill_between(range(len(forecast)), forecast - stderr, forecast + stderr, color='r', alpha=.1)\n\nplt.show()\nHere, we used the seasonal_decompose() function from statsmodels to decompose the time series, fitted an ARIMA model, and made a forecast for the next 24 months.\n\n\n\nIn summary, time series forecasting is a powerful tool for\npredicting future trends and events. Both R and Python provide excellent support for time series analysis and can be chosen based on the specific requirements of your project and your familiarity with the language. Happy forecasting!"
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "Education",
    "section": "",
    "text": "2013-2017: University of Oxford, DPhil (PhD) Plant Sciences\nDuring my PhD I used machine learning, extensive computational modelling and advanced statistics to understand how plants save water under drought stress. I also studied the relationship between climatic variables and the distribution of species under existing climatic scenarios and under climate change scenarios. I used different approaches to test the differences between the “climatic niche” of water-saving and non-water-saving plants including Neural networks, XGBoost, Random Forest, and Clustering analysis. I also conducted A/B tests using bootstrapping and Bayesian statistics. These analyses were conducted using R and Python.\n\n2012: MSc Biology (1st place), Universidad de los Andes (Colombia)\nI used R to test different hypotheses concerning changes in the biochemistry of juvenile plants when they experienced drought.\n2010: BSc Biology, Universidad de los Andes (Colombia)\nI learned R in 2008 as part of my biostatistics course, used extensively in my dissertation and since then I have not stopped using it."
  },
  {
    "objectID": "education.html#awards",
    "href": "education.html#awards",
    "title": "Education",
    "section": "Awards",
    "text": "Awards\n2013-2016: Weidenfeld Scholarship and Leadership Program for D.Phil. Funding at University of Oxford.\n2013-2017: Scholarship from Administrative Department of Science, Technology and Innovation (Colombia) for D.Phil. Funding at University of Oxford .\n2014: Winner of New Phytologist Poster Prize (1 st place), 34 th New Phytologist Symposium “Plant Systems Biology and Ecology of CAM plants”, California, U.S.A."
  },
  {
    "objectID": "introMLOps.html",
    "href": "introMLOps.html",
    "title": "Introduction to MLOps",
    "section": "",
    "text": "In today’s blog, I will discuss a pivotal concept in modern machine learning architectures: **MLOps**, short for Machine Learning Operations.\n## 1. What is MLOps?\nMLOps is a set of best practices combining Machine Learning (ML), Data Engineering, and DevOps. The goal of MLOps is to streamline and standardize the machine learning lifecycle, from development and deployment to maintenance.\nMLOps aims to address various challenges in operationalizing ML models, such as managing data dependencies, maintaining code quality, ensuring model reproducibility, and monitoring model performance over time.\n## 2. Key Components of MLOps\nLet’s dive deeper into the various components that constitute MLOps:\n### 2.1 Version Control\nVersion control is essential for tracking changes to code, data, parameters, and environment configuration. Git is often used for code, but tools like DVC (Data Version Control) can help manage data and model versions.\n### 2.2 Automated Testing\nTesting ensures the stability and reliability of the code, including model code and infrastructure code. Types of tests include unit tests, integration tests, and end-to-end tests. Additionally, data validation tests are crucial to ensure the quality of the data feeding into the models.\n### 2.3 Continuous Integration (CI)\nCI is a practice where developers frequently merge their code changes into a central repository. After each merge, automated builds and tests are run. In the context of MLOps, this includes integration tests of ML code, infrastructure, and data pipelines.\n### 2.4 Continuous Deployment (CD)\nContinuous Deployment is the practice of automatically deploying the code to production after passing the build and test stages. For ML, this also includes model training, validation, and deployment.\n### 2.5 Monitoring & Logging\nModel performance needs to be monitored over time to ensure it doesn’t degrade. Monitoring systems can track model performance metrics and provide alerts for any significant changes. Logging systems keep detailed records of data inputs and outputs, model predictions, and any errors or exceptions.\n### 2.6 Reproducibility\nEnsuring the same results can be achieved if a past version of the model is re-run with the same data and parameters is key. This includes versioning data, code, and environment, as Ill as tracking all experiments.\n## 3. MLOps Tools\nSeveral tools and platforms help facilitate MLOps, including:\n- **MLflow**: An open-source platform for managing the end-to-end ML lifecycle, including experimentation, reproducibility, and deployment.\n- **TFX (TensorFlow Extended)**: A Google-production-scale ML platform that provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor ML systems.\n- **Kubeflow**: An open-source project dedicated to making deployments of machine learning workflows on Kubernetes simple, portable, and scalable.\n- **Seldon**: An open-source platform that enables businesses to deploy, scale, and optimize machine learning models in production.\n## 4. Benefits of MLOps\nMLOps brings several benefits, including:\n- **Efficiency**: By automating many steps in the ML lifecycle.\n- **Reproducibility**: By tracking every experiment with its data, code, and parameters.\n- **Collaboration**: By enabling data scientists, data engineers, and DevOps to work together effectively.\n- **Monitoring**: By continuously monitoring model performance and data quality.\n- **Governance and regulatory compliance**: By maintaining detailed logs of data, model predictions, and model changes.\nThe ultimate goal of MLOps is to accelerate the ML lifecycle and make ML models more valuable to the organization. Having a firm grasp of MLOps principles has been invaluable in my professional journey."
  },
  {
    "objectID": "products.html#selected-projects-over-the-last-2-years",
    "href": "products.html#selected-projects-over-the-last-2-years",
    "title": "Projects",
    "section": "Selected projects over the last 2 years",
    "text": "Selected projects over the last 2 years\n\nPredicting Market Direction for Cocoa Futures\nBased on supply and demand data as well as key weather events I built time series models in order to predict the direction of price changes in futures contracts for cocoa in London and New York. I used prophet library and Keras, using recurrent neural networks. This model is used by traders in ECOM on a daily basis.\n\n\nPredicting Cocoa Production\nIn this project I used climatic variables to predict the yield of two crop seasons for cocoa in Côte d’Ivore. The model was a series of ensemble models that weighted the anomalies for each week/month and adjusted the forecast. The predictions have been tested over the last 2 years (4 seasons) with accurate results, even with models with early information.\n\n\nPredicting loss of suitability of coffee and cocoa due to climate change\nClimate change is having a major impact on the production of cocoa and coffee around the world. I used several models (Random Forest, Neural Networks and XGBoost) to model the current suitability of cocoa and coffee using several climatic variables. Once the model was calibrated on current conditions, the predictions were applied to future conditions to understand either the expansion or contraction of the areas of production that are suitable today. The results of these models has helped to develop the priorities to mitigate dramatic changes in the tropics.\n\n\nAmex Default Prediction\nI used this data to train junior peers to show them how to use different classification methods to extract value of large datasets. The data came from a Kaggle competition. I used gradient boosting decision trees. Specifically, I trained and evaluated the LGBMClassifier, XGBClassifier and the CatBoostClassifier models with gold standard results in the context of the competition to successfully identify whether a customer would default on their repayments of credit."
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "Python (most used libraries: Pandas, GeoPandas, prophet, SciKit-Learn, TensorFlow, Pytorch, keras, Matplotlib, NumPy, SciPy, SQLAlchemy, Seaborn, Bokeh, SciKit-Images)\nR (most used packages: RStudio, Shiny, tidyverse, Parsnip, bayestestR, sparklyr, keras, dygraph, forecast)"
  },
  {
    "objectID": "skills.html#geospatial-analysis",
    "href": "skills.html#geospatial-analysis",
    "title": "Skills",
    "section": "Geospatial analysis",
    "text": "Geospatial analysis\nAdvanced skills in ArcGIS and QGIS"
  },
  {
    "objectID": "skills.html#big-data-databases",
    "href": "skills.html#big-data-databases",
    "title": "Skills",
    "section": "Big Data & Databases",
    "text": "Big Data & Databases\nProficient in SQL, MongoDB (NoSQL), Spark (pyspark and sparklyr)"
  },
  {
    "objectID": "skills.html#experimental-design",
    "href": "skills.html#experimental-design",
    "title": "Skills",
    "section": "Experimental design",
    "text": "Experimental design\nStrong background designing and implementing A/B tests, machine learning algorithms (supervised and unsupervised) and Bayesian statistics"
  },
  {
    "objectID": "skills.html#languages",
    "href": "skills.html#languages",
    "title": "Skills",
    "section": "Languages",
    "text": "Languages\nFluent in English and Spanish (native)"
  }
]