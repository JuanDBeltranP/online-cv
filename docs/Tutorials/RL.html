<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.38">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Dr Juan Beltran – rl</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PWP5F6C91J"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PWP5F6C91J', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Dr Juan Beltran</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-sections" role="button" data-bs-toggle="dropdown" aria-expanded="false">Sections</a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-sections">    
        <li class="dropdown-header">CV</li>
        <li>
    <a class="dropdown-item" href="../juan.html">
 <span class="dropdown-text">Brief Description</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../skills.html">
 <span class="dropdown-text">Skills</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../work.html">
 <span class="dropdown-text">Work Experience</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../education.html">
 <span class="dropdown-text">Education</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../products.html">
 <span class="dropdown-text">Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../publications.html">
 <span class="dropdown-text">Publications and Awards</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../CV.html">
 <span class="dropdown-text">Full CV</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Notes</li>
        <li>
    <a class="dropdown-item" href="../Notes/PythonvsR.html">
 <span class="dropdown-text">Python or R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/Newenvs.html">
 <span class="dropdown-text">The importance of environments</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/commenting.html">
 <span class="dropdown-text">Why do we need to comment the code?</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/MLOPS0.html">
 <span class="dropdown-text">MLOps Notes 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/MLOPS1.html">
 <span class="dropdown-text">MLOps Notes 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/MLOPS2.html">
 <span class="dropdown-text">MLOps Notes 3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/MLOPS3.html">
 <span class="dropdown-text">MLOps Notes 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/timeseries.html">
 <span class="dropdown-text">Time series analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Notes/NeuralNetworks.html">
 <span class="dropdown-text">Neural Networks</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Tutorials</li>
        <li>
    <a class="dropdown-item" href="../Tutorials/SQL.html">
 <span class="dropdown-text">SQL 101</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/advSQL.html">
 <span class="dropdown-text">Advance SQL</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/Regression.html">
 <span class="dropdown-text">Regression analysis 101</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/Regression2.html">
 <span class="dropdown-text">Hyperparameters in regression P1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/Regression3.html">
 <span class="dropdown-text">Hyperparameters in regression P2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/RF.html">
 <span class="dropdown-text">Random Forest 101</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/xgboost.html">
 <span class="dropdown-text">XGboost 101</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/NeuralNetworks.html">
 <span class="dropdown-text">Deep Learning for Time Series Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/ImageClassification.html">
 <span class="dropdown-text">Image Classification with Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/RL.html">
 <span class="dropdown-text">Reinforcement Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/RLStocks.html">
 <span class="dropdown-text">RL and trading in financial markets 101</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../Tutorials/GoogleEarth.html">
 <span class="dropdown-text">Google Earth</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JuanDBeltranP"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/jdbeltran/"><i class="bi bi-linkedin" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">(Untitled)</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">CV</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../juan.html" class="sidebar-item-text sidebar-link">Brief Description</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../skills.html" class="sidebar-item-text sidebar-link">Skills</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../work.html" class="sidebar-item-text sidebar-link">Work Experience</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../education.html" class="sidebar-item-text sidebar-link">Education</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../products.html" class="sidebar-item-text sidebar-link">Projects</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../publications.html" class="sidebar-item-text sidebar-link">Publications and Awards</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CV.html" class="sidebar-item-text sidebar-link">Full CV</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Notes</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/PythonvsR.html" class="sidebar-item-text sidebar-link">Python or R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/Newenvs.html" class="sidebar-item-text sidebar-link">The importance of environments</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/commenting.html" class="sidebar-item-text sidebar-link">Why do we need to comment the code?</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/MLOPS0.html" class="sidebar-item-text sidebar-link">MLOps Notes 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/MLOPS1.html" class="sidebar-item-text sidebar-link">MLOps Notes 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/MLOPS2.html" class="sidebar-item-text sidebar-link">MLOps Notes 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/MLOPS3.html" class="sidebar-item-text sidebar-link">MLOps Notes 4</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/timeseries.html" class="sidebar-item-text sidebar-link">Time series analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Notes/NeuralNetworks.html" class="sidebar-item-text sidebar-link">Neural Networks</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Tutorials</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/SQL.html" class="sidebar-item-text sidebar-link">SQL 101</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/advSQL.html" class="sidebar-item-text sidebar-link">Advance SQL</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/Regression.html" class="sidebar-item-text sidebar-link">Regression analysis 101</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/Regression2.html" class="sidebar-item-text sidebar-link">Hyperparameters in regression P1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/Regression3.html" class="sidebar-item-text sidebar-link">Hyperparameters in regression P2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/RF.html" class="sidebar-item-text sidebar-link">Random Forest 101</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/xgboost.html" class="sidebar-item-text sidebar-link">XGboost 101</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/NeuralNetworks.html" class="sidebar-item-text sidebar-link">Deep Learning for Time Series Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/ImageClassification.html" class="sidebar-item-text sidebar-link">Image Classification with Deep Learning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/RL.html" class="sidebar-item-text sidebar-link active">Reinforcement Learning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/RLStocks.html" class="sidebar-item-text sidebar-link">RL and trading in financial markets 101</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tutorials/GoogleEarth.html" class="sidebar-item-text sidebar-link">Google Earth</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#reinforcement-learning-in-python-and-r-a-comprehensive-guide" id="toc-reinforcement-learning-in-python-and-r-a-comprehensive-guide" class="nav-link active" data-scroll-target="#reinforcement-learning-in-python-and-r-a-comprehensive-guide">Reinforcement Learning in Python and R: A Comprehensive Guide</a>
  <ul class="collapse">
  <li><a href="#understanding-the-basics-of-reinforcement-learning" id="toc-understanding-the-basics-of-reinforcement-learning" class="nav-link" data-scroll-target="#understanding-the-basics-of-reinforcement-learning">1. Understanding the Basics of Reinforcement Learning</a></li>
  <li><a href="#q-learning-introduction-to-a-simple-reinforcement-learning-algorithm" id="toc-q-learning-introduction-to-a-simple-reinforcement-learning-algorithm" class="nav-link" data-scroll-target="#q-learning-introduction-to-a-simple-reinforcement-learning-algorithm">2. Q-Learning: Introduction to a Simple Reinforcement Learning Algorithm</a></li>
  <li><a href="#implementing-q-learning-in-python-using-openais-gym" id="toc-implementing-q-learning-in-python-using-openais-gym" class="nav-link" data-scroll-target="#implementing-q-learning-in-python-using-openais-gym">3. Implementing Q-Learning in Python using OpenAI’s Gym</a></li>
  <li><a href="#implementing-q-learning-in-r" id="toc-implementing-q-learning-in-r" class="nav-link" data-scroll-target="#implementing-q-learning-in-r">4. Implementing Q-Learning in R</a></li>
  <li><a href="#policy-gradients" id="toc-policy-gradients" class="nav-link" data-scroll-target="#policy-gradients">5. Policy Gradients</a>
  <ul class="collapse">
  <li><a href="#understanding-policy-gradient" id="toc-understanding-policy-gradient" class="nav-link" data-scroll-target="#understanding-policy-gradient">Understanding Policy Gradient</a></li>
  <li><a href="#implementing-policy-gradients" id="toc-implementing-policy-gradients" class="nav-link" data-scroll-target="#implementing-policy-gradients">Implementing Policy Gradients</a></li>
  </ul></li>
  <li><a href="#deep-q-networks-dqns" id="toc-deep-q-networks-dqns" class="nav-link" data-scroll-target="#deep-q-networks-dqns">6. Deep Q-Networks (DQNs)</a>
  <ul class="collapse">
  <li><a href="#understanding-dqn" id="toc-understanding-dqn" class="nav-link" data-scroll-target="#understanding-dqn">Understanding DQN</a></li>
  <li><a href="#implementing-dqns" id="toc-implementing-dqns" class="nav-link" data-scroll-target="#implementing-dqns">Implementing DQNs</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">7. Conclusions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="reinforcement-learning-in-python-and-r-a-comprehensive-guide" class="level1">
<h1>Reinforcement Learning in Python and R: A Comprehensive Guide</h1>
<p>Reinforcement Learning (RL) is a subfield of machine learning that focuses on how an agent ought to take actions in an environment to maximize some notion of cumulative reward. In this tutorial, we will go over the fundamental principles of RL and how to implement them in Python and R. Absolutely! Let’s delve deeper into the concepts and code.</p>
<section id="understanding-the-basics-of-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-basics-of-reinforcement-learning">1. Understanding the Basics of Reinforcement Learning</h2>
<p>Reinforcement Learning is a dynamic process that uses rewards and punishments to train an agent. It consists of several elements:</p>
<ul>
<li><strong>Agent</strong>: This is the learner or decision-maker. It takes actions based on the state it’s in and the policy it has learned.</li>
<li><strong>Environment</strong>: This is the context in which the agent operates. It could be a maze for a robotic agent or a board game for a game-playing agent.</li>
<li><strong>State</strong>: This is the current situation the agent is in. The state will be presented to the agent as an input, and it is the agent’s representation of the environment at a given time.</li>
<li><strong>Action</strong>: These are the possible steps or decisions that the agent can make.</li>
<li><strong>Reward</strong>: These are the feedback signals that guide the agent to the goal. A reward can be positive (for good actions) or negative (for bad actions).</li>
</ul>
<p>The agent learns from the consequences of its actions, rather than from being taught explicitly. It selects actions based on its past experiences (exploitation) and also by new choices (exploration), which is essentially the dilemma between choosing a known best action and trying out a new action to see if it’s better.</p>
</section>
<section id="q-learning-introduction-to-a-simple-reinforcement-learning-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="q-learning-introduction-to-a-simple-reinforcement-learning-algorithm">2. Q-Learning: Introduction to a Simple Reinforcement Learning Algorithm</h2>
<p>Q-Learning is a values iteration algorithm in RL. In Q-Learning, we define a Q-function (Q stands for quality) that the agent learns. The Q-function Q(s,a) gives the expected future reward if action a is taken in state s.</p>
<p>The Q-Learning algorithm uses a table to store the Q-values for each (state-action) pair. This table guides the agent to the best action at each state.</p>
</section>
<section id="implementing-q-learning-in-python-using-openais-gym" class="level2">
<h2 class="anchored" data-anchor-id="implementing-q-learning-in-python-using-openais-gym">3. Implementing Q-Learning in Python using OpenAI’s Gym</h2>
<p>We will use the OpenAI Gym library, which is a popular Python library for developing and comparing RL algorithms. It provides several pre-defined environments where we can test our agents.</p>
<p>One such environment is the FrozenLake-v0 game. In the FrozenLake game, the agent controls the movement of a character in a grid world. The agent’s goal is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). The catch is that the ice is slippery, so the agent won’t always move in the direction it intends to.</p>
<p>Here’s an example of a FrozenLake environment:</p>
<pre><code>SFFF       (S: starting point, safe)
FHFH       (F: frozen surface, safe)
FFFH       (H: hole, fall to your doom)
HFFG       (G: goal, where the frisbee is located)</code></pre>
<p>This Python code implements the Q-Learning algorithm:</p>
<p>To start, we should install the necessary dependencies, including TensorFlow and Keras for building neural networks, and Gym for the reinforcement learning environment.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pip install gym tensorflow keras numpy matplotlib</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The implementation is the following:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> clear_output</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"FrozenLake-v0"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here we import the necessary libraries and create the FrozenLake-v0 environment.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Q-table with zero</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>q_table <span class="op">=</span> np.zeros([env.observation_space.n, env.action_space.n])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The Q-table is initialized with zeros and has a size of (number_of_states x number_of_actions).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>num_episodes <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>max_steps_per_episode <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>discount_rate <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>exploration_rate <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>max_exploration_rate <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>min_exploration_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>exploration_decay_rate <span class="op">=</span> </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="fl">0.001</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>rewards_all_episodes <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These are the hyperparameters. The number of episodes is the number of games we want the agent to play during the training. In each episode, the agent can make a fixed number of steps (max_steps_per_episode). The learning_rate is the rate at which the AI agent should learn, the discount_rate is the importance of future rewards. The exploration rate is the rate at which the agent should explore the environment randomly, instead of using the already computed Q-values.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Q-learning algorithm</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(num_episodes):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> env.reset()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    rewards_current_episode <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(max_steps_per_episode): </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here we start to play the episodes. At the start of each episode, we reset the environment to its initial state. <code>done</code> is a flag used to indicate if the episode has ended. <code>rewards_current_episode</code> is used to sum up all the rewards the agent has received in the current episode.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>        exploration_rate_threshold <span class="op">=</span> random.uniform(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exploration_rate_threshold <span class="op">&gt;</span> exploration_rate:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> np.argmax(q_table[state,:]) </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> env.action_space.sample()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here the agent decides whether to explore or exploit. A random number is chosen. If it is greater than the exploration rate, the agent exploits the environment and chooses the action with the highest Q-value in the current state (exploit). Otherwise, it explores the environment, and the action is chosen randomly (explore).</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>        new_state, reward, done, info <span class="op">=</span> env.step(action)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update Q-table for Q(s,a)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        q_table[state, action] <span class="op">=</span> q_table[state, action] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> learning_rate) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>            learning_rate <span class="op">*</span> (reward <span class="op">+</span> discount_rate <span class="op">*</span> np.<span class="bu">max</span>(q_table[new_state, :]))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> new_state</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        rewards_current_episode <span class="op">+=</span> reward </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> done <span class="op">==</span> <span class="va">True</span>: </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The agent performs the action and moves to the new state, and receives the reward. The Q-value of the (state-action) pair is updated using the Q-learning update rule.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Exploration rate decay</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    exploration_rate <span class="op">=</span> min_exploration_rate <span class="op">+</span> <span class="op">\</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        (max_exploration_rate <span class="op">-</span> min_exploration_rate) <span class="op">*</span> np.exp(<span class="op">-</span>exploration_decay_rate<span class="op">*</span>episode)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    rewards_all_episodes.append(rewards_current_episode)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>After each episode, we decrease the exploration rate. We want our agent to explore less as it learns more about the environment. The <code>rewards_all_episodes</code> list keeps track of the total rewards in each episode.</p>
</section>
<section id="implementing-q-learning-in-r" class="level2">
<h2 class="anchored" data-anchor-id="implementing-q-learning-in-r">4. Implementing Q-Learning in R</h2>
<p>Now let’s implement the same Q-Learning in R. We’re going to make a simple grid world with four cells, where the agent needs to find the terminal state to get a reward.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MDPtoolbox)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define transition and reward matrices</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="fu">c</span>(S, A, S))</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, S, A)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define transitions</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Define rewards</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">3</span>, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">4</span>, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Run Q-Learning</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">mdp_example_qlearning</span>(T, R, <span class="fl">0.9</span>, <span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The code above creates a simple environment using the <code>MDPtoolbox</code> library. The environment is a simple four-state system where the agent learns to navigate to the terminal state using Q-Learning.</p>
<p>Let’s now dive deeper into more advanced concepts of reinforcement learning with Python, including Policy Gradients and Deep Q-Networks (DQNs).</p>
</section>
<section id="policy-gradients" class="level2">
<h2 class="anchored" data-anchor-id="policy-gradients">5. Policy Gradients</h2>
<p>Policy Gradients are a type of reinforcement learning algorithms that are more advanced than Q-Learning. They learn a parameterized policy that can choose actions without consulting a value function. One popular algorithm in this family is the REINFORCE algorithm.</p>
<section id="understanding-policy-gradient" class="level3">
<h3 class="anchored" data-anchor-id="understanding-policy-gradient">Understanding Policy Gradient</h3>
<p>Instead of learning a value function that tells us what is the expected sum of rewards given a state and an action (as in Q-Learning), we’re going to learn directly a policy function. The policy is a distribution over actions given states.</p>
<p>In Policy Gradient methods, we typically use a simple function approximator such as a neural network to approximate the policy function.</p>
<p>The REINFORCE algorithm works as follows:</p>
<ol type="1">
<li>Initialize the policy parameters θ at random.</li>
<li>Generate an episode following π(⋅|⋅;θ), and let R_t be the return from time step t.</li>
<li>For each time step t of the episode, compute the gradient ∇_θ log π(A_t|S_t;θ) and update the parameters using stochastic gradient ascent: θ ← θ + αR_t ∇_θ log π(A_t|S_t;θ)</li>
</ol>
</section>
<section id="implementing-policy-gradients" class="level3">
<h3 class="anchored" data-anchor-id="implementing-policy-gradients">Implementing Policy Gradients</h3>
<p>Let’s implement a simple version of the REINFORCE algorithm using the CartPole environment from Gym. This environment consists of a pole connected to a cart moving along a frictionless track. The pole starts upright, and the goal is to prevent it from falling over by applying a force of +1 or -1 to the cart.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Reshape, Flatten</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model():</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(<span class="dv">24</span>, input_dim<span class="op">=</span><span class="dv">4</span>, activation<span class="op">=</span><span class="st">'relu'</span>)) <span class="co"># input is the state</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(<span class="dv">24</span>, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)) <span class="co"># output is the probability distribution over actions</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>, optimizer<span class="op">=</span>Adam())</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function creates our policy network. It takes a state as input and outputs a probability distribution over actions.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_episode(policy, env):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    states, actions, rewards <span class="op">=</span> [], [], []</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> env.reset()</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> state.reshape([<span class="dv">1</span>, state.size])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        prob <span class="op">=</span> policy.predict(state)[<span class="dv">0</span>]</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> np.random.choice(env.action_space.n, p<span class="op">=</span>prob)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        next_state, reward, done, _ <span class="op">=</span> env.step(action)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        states.append(state)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        actions.append(action)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        rewards.append(reward)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> states, actions, rewards</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function generates an episode by running the policy in the environment. It returns the list of states, actions, and rewards encountered during the episode.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_returns(rewards, gamma<span class="op">=</span><span class="fl">0.99</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    returns <span class="op">=</span> []</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">reversed</span>(rewards):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        R <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> R</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        returns.insert(<span class="dv">0</span>, R)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(returns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function computes the returns for each time step in an episode, using the rewards and a discount factor gamma.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_policy(policy, episodes):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a> env <span class="op">=</span> gym.make(<span class="st">'CartPole-v0'</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        states, actions, rewards <span class="op">=</span> generate_episode(policy, env)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> compute_returns(rewards)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> np.squeeze(np.vstack(states))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> np.zeros([<span class="bu">len</span>(actions), env.action_space.n])</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, action <span class="kw">in</span> <span class="bu">enumerate</span>(actions):</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>            y[i, action] <span class="op">=</span> returns[i]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        policy.train_on_batch(X, y)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    env.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function trains the policy for a given number of episodes. For each episode, it generates an episode, computes the returns, and updates the policy parameters using the returns as weight for the actions taken.</p>
<p>Now we can create our policy and train it:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>policy <span class="op">=</span> create_model()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>train_policy(policy, <span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="deep-q-networks-dqns" class="level2">
<h2 class="anchored" data-anchor-id="deep-q-networks-dqns">6. Deep Q-Networks (DQNs)</h2>
<p>Deep Q-Networks (DQNs) are a type of Q-Learning algorithm that uses a neural network as a function approximator for the Q-function. It was introduced by DeepMind in their groundbreaking paper “Playing Atari with Deep Reinforcement Learning” in 2013.</p>
<section id="understanding-dqn" class="level3">
<h3 class="anchored" data-anchor-id="understanding-dqn">Understanding DQN</h3>
<p>DQN is essentially Q-Learning with function approximation, but with a few key differences:</p>
<ol type="1">
<li><p><strong>Experience Replay</strong>: To break the correlation between consecutive samples, DQN stores the agent’s experiences in a replay buffer. During training, it samples mini-batches of experiences from the replay buffer to update the Q-network.</p></li>
<li><p><strong>Target Network</strong>: To stabilize the training, DQN uses two networks: a Q-network for selecting actions, and a target network (a clone of the Q-network) for generating the Q-learning targets. The weights of the target network are updated less frequently (usually, every few thousand steps) to keep the targets stable.</p></li>
</ol>
</section>
<section id="implementing-dqns" class="level3">
<h3 class="anchored" data-anchor-id="implementing-dqns">Implementing DQNs</h3>
<p>Now let’s implement a DQN for the CartPole environment.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQNAgent:</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_size, action_size):</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state_size <span class="op">=</span> state_size</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_size <span class="op">=</span> action_size</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.memory <span class="op">=</span> deque(maxlen<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> <span class="fl">0.95</span>  <span class="co"># discount rate</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># exploration rate</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_min <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon_decay <span class="op">=</span> <span class="fl">0.995</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> <span class="va">self</span>._build_model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This class defines our DQN agent. It has a memory for storing experiences, parameters for the epsilon-greedy policy (epsilon is the probability of choosing a random action), and a neural network model for approximating the Q-function.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _build_model(<span class="va">self</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(<span class="dv">24</span>, input_dim<span class="op">=</span><span class="va">self</span>.state_size, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(<span class="dv">24</span>, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(<span class="va">self</span>.action_size, activation<span class="op">=</span><span class="st">'linear'</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mse'</span>, optimizer<span class="op">=</span>Adam(lr<span class="op">=</span><span class="va">self</span>.learning_rate))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function builds a simple 2-layer fully connected neural network. The input size is the size of the state space, and the output size is the size of the action space. The model uses mean squared error (MSE) for loss as we’re dealing with a regression problem (predicting the Q-values), and the Adam optimizer for training the model.</p>
<p>Next, we define a method for the agent to remember the experiences.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remember(<span class="va">self</span>, state, action, reward, next_state, done):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.memory.append((state, action, reward, next_state, done))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function simply adds an experience to the agent’s memory. The experience is a tuple of state, action, reward, next state, and done (which tells whether the episode ended after this step).</p>
<p>Now, let’s implement the action selection policy.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> act(<span class="va">self</span>, state):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;=</span> <span class="va">self</span>.epsilon:</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.randrange(<span class="va">self</span>.action_size)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    act_values <span class="op">=</span> <span class="va">self</span>.model.predict(state)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.argmax(act_values[<span class="dv">0</span>])  <span class="co"># returns action</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function implements an epsilon-greedy policy. With probability epsilon, it chooses a random action (for exploration), and with probability 1 - epsilon, it chooses the best action according to the current Q-value estimates (for exploitation).</p>
<p>The core of the DQN algorithm is in the learning phase, where it updates the Q-value estimates.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> replay(<span class="va">self</span>, batch_size):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    minibatch <span class="op">=</span> random.sample(<span class="va">self</span>.memory, batch_size)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> state, action, reward, next_state, done <span class="kw">in</span> minibatch:</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> <span class="va">self</span>.model.predict(state)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> done:</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>            target[<span class="dv">0</span>][action] <span class="op">=</span> reward</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            Q_future <span class="op">=</span> <span class="bu">max</span>(<span class="va">self</span>.model.predict(next_state)[<span class="dv">0</span>])</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            target[<span class="dv">0</span>][action] <span class="op">=</span> reward <span class="op">+</span> <span class="va">self</span>.gamma <span class="op">*</span> Q_future</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.fit(state, target, epochs<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.epsilon <span class="op">&gt;</span> <span class="va">self</span>.epsilon_min:</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">*=</span> <span class="va">self</span>.epsilon_decay</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this function, a mini-batch of experiences is sampled from the memory, and for each experience, the Q-value target for the action taken is updated, and the network’s weights are updated to fit this target value. If the episode ended (<code>done</code> is True), the target is simply the reward, else it’s the reward plus the discounted estimated maximum future Q-value.</p>
<p>Finally, we create an instance of the DQNAgent and train it to balance the pole in the CartPole environment.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">'CartPole-v1'</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> DQNAgent(env.observation_space.shape[<span class="dv">0</span>], env.action_space.n)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> env.reset()</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> np.reshape(state, [<span class="dv">1</span>, agent.state_size])</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> time <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> agent.act(state)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        next_state, reward, done, _ <span class="op">=</span> env.step(action)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> reward <span class="cf">if</span> <span class="kw">not</span> done <span class="cf">else</span> <span class="op">-</span><span class="dv">10</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> np.reshape(next_state, [<span class="dv">1</span>, agent.state_size])</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        agent.remember(state, action, reward, next_state, done)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> done:</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"episode: </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">, score: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(e, episodes, time))</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(agent.memory) <span class="op">&gt;</span> batch_size:</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            agent.replay(batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The agent plays a number of episodes of the game, choosing actions via its <code>act</code> method, remembering the resulting experiences, and learning from its memory by replaying experiences. The game resets when the pole falls over, giving the agent negative rewards as a penalty.</p>
<p>In this tutorial, we went over a more advanced concept in reinforcement learning, including policy gradient methods like REINFORCE and value-based methods like DQN. These methods use neural networks to learn directly from high-dimensional sensory inputs, and have achieved superhuman performance in complex tasks like playing video games and controlling robots.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">7. Conclusions</h2>
<p>Reinforcement learning is a powerful approach for tasks that involve sequential decision-making. This tutorial presented the fundamental concepts of RL and walked through an example of how to implement Q-Learning, a simple but powerful RL algorithm, in Python and R. Remember that the RL field is vast and complex, this is just the tip of the iceberg!</p>
<p>This tutorial is intended as a starting point. I encourage you to continue exploring more complex environments, policies, and algorithms as you continue your journey in reinforcement learning. Happy learning!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>